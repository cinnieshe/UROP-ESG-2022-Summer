{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f5d7493e",
      "metadata": {
        "id": "f5d7493e"
      },
      "source": [
        "# Fine-Tune GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "009958f8",
      "metadata": {
        "id": "009958f8"
      },
      "source": [
        "## Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "58f8830d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58f8830d",
        "outputId": "a3784de3-4313-432f-f9e7-ccb8dac97848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import os # store and load weight\n",
        "import pandas as pd # load data\n",
        "import nltk # text processing\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer # text processing\n",
        "from nltk.corpus import stopwords #text processing\n",
        "import numpy as np # one-hot vector\n",
        "import matplotlib.pyplot as plt # model analysis\n",
        "from itertools import chain # feature construction\n",
        "from collections import Counter # build feats-dict\n",
        "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import keras\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Input, Add, Concatenate,\\\n",
        "    Bidirectional, SimpleRNN, LSTM, GRU\n",
        "\n",
        "\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e6bf78",
      "metadata": {
        "id": "08e6bf78"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cc4574c0",
      "metadata": {
        "id": "cc4574c0"
      },
      "outputs": [],
      "source": [
        "def load_data(filename):\n",
        "    \"\"\"\n",
        "    Input: string filename\n",
        "    Output: a pandas dataframe for the whole dataset after droping missing values\n",
        "    Support google colab or local environments\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # local environment\n",
        "        df = pd.read_csv(filename)\n",
        "        df = df.dropna(subset=['sentence', 'label']) ## drop missing values\n",
        "        return df\n",
        "    except:\n",
        "        # google colab environment\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        \n",
        "        df = pd.read_csv('/content/drive/MyDrive/' + filename)\n",
        "        df = df.dropna(subset=['sentence', 'label']) ## drop missing values\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2b8210f7",
      "metadata": {
        "id": "2b8210f7"
      },
      "outputs": [],
      "source": [
        "def split_data(df):\n",
        "    \"\"\"\n",
        "    Input: pandas dataframe\n",
        "    Output: training dataframe (81%), validation dataframe (9%), test dataframe (10%)\n",
        "    \"\"\"\n",
        "    df_train, df_val = train_test_split(df, stratify=df['label'],test_size=0.1, random_state=42)\n",
        "    \n",
        "    return df_train, df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b1e2f6ea",
      "metadata": {
        "id": "b1e2f6ea"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    :param text: a doc with multiple sentences, type: str\n",
        "    return a word list, type: list\n",
        "    e.g.\n",
        "    Input: 'Text mining is to identify useful information.'\n",
        "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "def stem(tokens):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    return a list of stemmed words, type: list\n",
        "    e.g.\n",
        "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
        "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
        "    \"\"\"\n",
        "\n",
        "    return [ps.stem(token) for token in tokens]\n",
        "\n",
        "def n_gram(tokens, n=1):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    :param n: the corresponding n-gram, type: int\n",
        "    return a list of n-gram tokens, type: list\n",
        "    e.g.\n",
        "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
        "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
        "    \"\"\"\n",
        "    if n == 1:\n",
        "        return tokens\n",
        "    else:\n",
        "        results = list()\n",
        "        for i in range(len(tokens)-n+1):\n",
        "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
        "            results.append(\" \".join(tokens[i:i+n]))\n",
        "        return results\n",
        "    \n",
        "def filter_stopwords(tokens):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    return a list of filtered tokens, type: list\n",
        "    e.g.\n",
        "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
        "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
        "    \"\"\"\n",
        "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
        "\n",
        "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
        "    \"\"\"\n",
        "    :param data: a list of features, type: list(list)\n",
        "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n",
        "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n",
        "    :param max_size: the max size of feature dict, type: int\n",
        "    return a feature dict that maps features to indices, sorted by frequencies\n",
        "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
        "    \"\"\"\n",
        "    # count all features\n",
        "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
        "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
        "        valid_feats = [f for f, cnt in feat_cnt.most_common(max_size)]\n",
        "    else:\n",
        "        valid_feats = list()\n",
        "        for f, cnt in feat_cnt.most_common():\n",
        "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
        "                (max_freq == -1 or cnt <= max_freq):\n",
        "                valid_feats.append(f)\n",
        "    if max_size > 0 and len(valid_feats) > max_size:\n",
        "        valid_feats = valid_feats[:max_size]        \n",
        "    print(\"Size of features:\", len(valid_feats))\n",
        "    \n",
        "    # build a mapping from features to indices\n",
        "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
        "    return feats_dict\n",
        "\n",
        "def get_onehot_vector(feats, feats_dict):\n",
        "    \"\"\"\n",
        "    :param feats: a list of features, type: list\n",
        "    :param feats_dict: a dict from features to indices, type: dict\n",
        "    return a feature vector,\n",
        "    \"\"\"\n",
        "    # initialize the vector as all zeros\n",
        "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
        "    for f in feats:\n",
        "        # get the feature index, return -1 if the feature is not existed\n",
        "        f_idx = feats_dict.get(f, -1)\n",
        "        if f_idx != -1:\n",
        "            # set the corresponding element as 1\n",
        "            vector[f_idx] = 1\n",
        "    return vector\n",
        "\n",
        "# Get index vector\n",
        "def get_index_vector(feats, feats_dict, max_len):\n",
        "    \"\"\"\n",
        "    :param feats: a list of features, type: list\n",
        "    :param feats_dict: a dict from features to indices, type: dict\n",
        "    :param feats: a list of features, type: list\n",
        "    return a feature vector,\n",
        "    \"\"\"\n",
        "    # initialize the vector as all zeros\n",
        "    vector = np.zeros(max_len, dtype=np.int64)\n",
        "    for i, f in enumerate(feats):\n",
        "        if i == max_len:\n",
        "            break\n",
        "        # get the feature index, return 1 (<unk>) if the feature is not existed\n",
        "        try:\n",
        "            vector[i] = feats_dict[f]\n",
        "        except KeyError:\n",
        "            vector[i] = 1\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ba2e5264",
      "metadata": {
        "id": "ba2e5264"
      },
      "outputs": [],
      "source": [
        "def build_RNN(input_length, vocab_size, embedding_size,\n",
        "              hidden_size, output_size,\n",
        "              num_rnn_layers, num_mlp_layers,\n",
        "              rnn_type=\"lstm\",\n",
        "              bidirectional=False,\n",
        "              embedding_matrix=None,\n",
        "              activation=\"tanh\",\n",
        "              dropout_rate=0.0,\n",
        "              batch_norm=False,\n",
        "              l2_reg=0.0,\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              optimizer=\"Adam\",\n",
        "              learning_rate=0.001,\n",
        "              metric=\"accuracy\"):\n",
        "\n",
        "    x = Input(shape=(input_length,))\n",
        "    \n",
        "    ################################\n",
        "    ###### Word Representation #####\n",
        "    ################################\n",
        "    # word representation layer\n",
        "    if embedding_matrix is not None:\n",
        "        emb = Embedding(input_dim=vocab_size,\n",
        "                        output_dim=embedding_size,\n",
        "                        input_length=input_length,\n",
        "                        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                        trainable=False)(x)\n",
        "    else:\n",
        "        emb = Embedding(input_dim=vocab_size,\n",
        "                        output_dim=embedding_size,\n",
        "                        input_length=input_length,\n",
        "                        embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
        "    \n",
        "    ################################\n",
        "    ####### Recurrent Layers #######\n",
        "    ################################\n",
        "    # recurrent layers\n",
        "    if rnn_type == \"rnn\":\n",
        "        fn = SimpleRNN\n",
        "    elif rnn_type == \"lstm\":\n",
        "        fn = LSTM\n",
        "    elif rnn_type == \"gru\":\n",
        "        fn = GRU\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    h = emb\n",
        "    for i in range(num_rnn_layers):\n",
        "        is_last = (i == num_rnn_layers-1)\n",
        "        if bidirectional:\n",
        "            h = Bidirectional(fn(hidden_size,\n",
        "                                 kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
        "                                 recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
        "                                 return_sequences=not is_last))(h)\n",
        "        else:\n",
        "            h = fn(hidden_size,\n",
        "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
        "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
        "                   return_sequences=not is_last)(h)\n",
        "        h = Dropout(dropout_rate, seed=0)(h)\n",
        "    \n",
        "    ################################\n",
        "    #### Fully Connected Layers ####\n",
        "    ################################\n",
        "    # multi-layer perceptron\n",
        "    for i in range(num_mlp_layers-1):\n",
        "        new_h = Dense(hidden_size,\n",
        "                      kernel_initializer=keras.initializers.he_normal(seed=0),\n",
        "                      bias_initializer=\"zeros\",\n",
        "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
        "        # add batch normalization layer\n",
        "        if batch_norm:\n",
        "            new_h = BatchNormalization()(new_h)\n",
        "        # add residual connection\n",
        "        if i == 0:\n",
        "            h = new_h\n",
        "        else:\n",
        "            h = Add()([h, new_h])\n",
        "        # add activation\n",
        "        h = Activation(activation)(h)\n",
        "    y = Dense(output_size,\n",
        "              activation=\"softmax\",\n",
        "              kernel_initializer=keras.initializers.he_normal(seed=0),\n",
        "              bias_initializer=\"zeros\")(h)\n",
        "    \n",
        "    # set the loss, the optimizer, and the metric\n",
        "    if optimizer == \"SGD\":\n",
        "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    elif optimizer == \"RMSprop\":\n",
        "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "    elif optimizer == \"Adam\":\n",
        "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    model = Model(x, y)\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e808299",
      "metadata": {
        "id": "6e808299"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a800533e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a800533e",
        "outputId": "d091e404-7ad6-4b66-f3a6-98c7a9609bc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "FILENAME = 'final_dataset_formatted.csv'\n",
        "TEST_FILENAME = 'final_dataset_formatted_test.csv'\n",
        "\n",
        "# load data\n",
        "df = load_data(FILENAME)\n",
        "df_test = load_data(TEST_FILENAME)\n",
        "\n",
        "# labels\n",
        "labels = ['CC', 'NC', 'PW', 'HC', 'PL', 'CR', 'CG', 'BE', 'N']\n",
        "num_labels = 9\n",
        "\n",
        "# split data\n",
        "df_train, df_val = split_data(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3ef1538",
      "metadata": {
        "id": "e3ef1538"
      },
      "source": [
        "## Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9c1e5c86",
      "metadata": {
        "id": "9c1e5c86"
      },
      "outputs": [],
      "source": [
        "# split text and labels\n",
        "train_texts = df_train.iloc[:, 0]\n",
        "train_labels = df_train.iloc[:, 1]\n",
        "valid_texts = df_val.iloc[:, 0]\n",
        "valid_labels = df_val.iloc[:, 1]\n",
        "test_texts = df_test.iloc[:, 0]\n",
        "test_labels = df_test.iloc[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "06f680cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06f680cb",
        "outputId": "5b02d408-ebe5-48d3-afc9-bde45fdfb9c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train size: 3645\n",
            "valid size: 405\n",
            "test size: 450\n"
          ]
        }
      ],
      "source": [
        "# get train, validation, and test dataset size\n",
        "train_size = len(train_texts)\n",
        "valid_size = len(valid_texts)\n",
        "test_size = len(test_texts)\n",
        "\n",
        "print(f'train size: {train_size}')\n",
        "print(f'valid size: {valid_size}')\n",
        "print(f'test size: {test_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "22a28c0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22a28c0b",
        "outputId": "79ed84a8-1266-4b2e-ae9f-4f02d96db19a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of features: 2465\n"
          ]
        }
      ],
      "source": [
        "# extract features\n",
        "min_freq = 3\n",
        "\n",
        "train_tokens = [tokenize(text) for text in train_texts]\n",
        "valid_tokens = [tokenize(text) for text in valid_texts]\n",
        "test_tokens = [tokenize(text) for text in test_texts]\n",
        "\n",
        "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
        "valid_stemmed = [stem(tokens) for tokens in valid_tokens]\n",
        "test_stemmed = [stem(tokens) for tokens in test_tokens]\n",
        "\n",
        "train_feats = [filter_stopwords(tokens) for tokens in train_stemmed]\n",
        "valid_feats = [filter_stopwords(tokens) for tokens in valid_stemmed]\n",
        "test_feats = [filter_stopwords(tokens) for tokens in test_stemmed]\n",
        "\n",
        "# build a mapping from features to indices\n",
        "feats_dict = get_feats_dict(\n",
        "    chain.from_iterable(train_feats),\n",
        "    min_freq=min_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "14716d41",
      "metadata": {
        "id": "14716d41"
      },
      "outputs": [],
      "source": [
        "max_len = 75 # from EDA\n",
        "\n",
        "# build the feats_matrix\n",
        "# convert each example to a index vector, and then stack vectors as a matrix\n",
        "train_feats_matrix = np.vstack(\n",
        "    [get_index_vector(f, feats_dict, max_len) for f in train_feats])\n",
        "valid_feats_matrix = np.vstack(\n",
        "    [get_index_vector(f, feats_dict, max_len) for f in valid_feats])\n",
        "test_feats_matrix = np.vstack(\n",
        "    [get_index_vector(f, feats_dict, max_len) for f in test_feats])\n",
        "\n",
        "# convert each label to a ont-hot vector, and then stack vectors as a matrix\n",
        "train_label_matrix = keras.utils.to_categorical(train_labels, num_classes=num_labels)\n",
        "valid_label_matrix = keras.utils.to_categorical(valid_labels, num_classes=num_labels)\n",
        "test_label_matrix = tf.keras.utils.to_categorical(test_labels, num_classes=num_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14b1fd1d",
      "metadata": {
        "id": "14b1fd1d"
      },
      "source": [
        "## Fine-Tune Model  \n",
        "    \n",
        "embedding_size  \n",
        "hidden_size  \n",
        "num_rnn_layers  \n",
        "num_mlp_layers  \n",
        "activation  \n",
        "dropout_rate  \n",
        "batch_norm  \n",
        "l2_reg  \n",
        "optimizer  \n",
        "learning_rate  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "63ef2920",
      "metadata": {
        "id": "63ef2920"
      },
      "outputs": [],
      "source": [
        "# set seed\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "71105160",
      "metadata": {
        "id": "71105160"
      },
      "outputs": [],
      "source": [
        "# indicator for saving models' weights\n",
        "count = 0\n",
        "\n",
        "# lstm\n",
        "rnn_type=\"gru\"\n",
        "bidirectional=False\n",
        "\n",
        "# initial settings\n",
        "epoch = 80\n",
        "batch_size = 100\n",
        "\n",
        "embedding_size=100\n",
        "hidden_size=100\n",
        "num_rnn_layers=1\n",
        "num_mlp_layers=2\n",
        "activation='tanh'\n",
        "dropout_rate=0.5\n",
        "batch_norm=True\n",
        "l2_reg=0.005\n",
        "optimizer=\"Adam\"\n",
        "learning_rate=0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d734c154",
      "metadata": {
        "id": "d734c154"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "cd926c58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd926c58",
        "outputId": "3a3474ad-7811-41f1-9f06-a0fc2b0c8b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 1\n",
            "optimizer: SGD\n",
            "accuracy: 0.1136\n",
            "macro_f1: 0.0270\n",
            "----------------------------------------------------------------------------\n",
            "count: 2\n",
            "optimizer: RMSprop\n",
            "accuracy: 0.6568\n",
            "macro_f1: 0.6571\n",
            "----------------------------------------------------------------------------\n",
            "count: 3\n",
            "optimizer: Adam\n",
            "accuracy: 0.6617\n",
            "macro_f1: 0.6668\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "optimizer: Adam, accuracy: 0.6617283950617284, macro_f1: 0.6668432375852194\n"
          ]
        }
      ],
      "source": [
        "optimizer_list = ['SGD', 'RMSprop', 'Adam']\n",
        "best_optimizer = ''\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for optimizer in optimizer_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'optimizer: {optimizer}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_optimizer = optimizer\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'optimizer: {best_optimizer}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "fae2666e",
      "metadata": {
        "id": "fae2666e"
      },
      "outputs": [],
      "source": [
        "optimizer = best_optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a67de14",
      "metadata": {
        "id": "1a67de14"
      },
      "source": [
        "### activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3026ae0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3026ae0b",
        "outputId": "02bd3045-cef0-4111-c53d-61336b3acd65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 4\n",
            "activation: relu\n",
            "accuracy: 0.7630\n",
            "macro_f1: 0.7625\n",
            "----------------------------------------------------------------------------\n",
            "count: 5\n",
            "activation: tanh\n",
            "accuracy: 0.7210\n",
            "macro_f1: 0.7255\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "activation: relu, accuracy: 0.762962962962963, macro_f1: 0.7624693915008827\n"
          ]
        }
      ],
      "source": [
        "activation_list = ['relu', 'tanh']\n",
        "best_activation = ''\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for activation in activation_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'activation: {activation}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_activation = activation\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'activation: {best_activation}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "346af949",
      "metadata": {
        "id": "346af949"
      },
      "outputs": [],
      "source": [
        "activation = best_activation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dede6a89",
      "metadata": {
        "id": "dede6a89"
      },
      "source": [
        "### embedding_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9b74b026",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b74b026",
        "outputId": "9cdb4762-78da-4d76-d5fa-6947694ea231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 6\n",
            "embedding_size: 50\n",
            "accuracy: 0.6691\n",
            "macro_f1: 0.6730\n",
            "----------------------------------------------------------------------------\n",
            "count: 7\n",
            "embedding_size: 75\n",
            "accuracy: 0.7309\n",
            "macro_f1: 0.7328\n",
            "----------------------------------------------------------------------------\n",
            "count: 8\n",
            "embedding_size: 100\n",
            "accuracy: 0.7259\n",
            "macro_f1: 0.7252\n",
            "----------------------------------------------------------------------------\n",
            "count: 9\n",
            "embedding_size: 128\n",
            "accuracy: 0.7605\n",
            "macro_f1: 0.7623\n",
            "----------------------------------------------------------------------------\n",
            "count: 10\n",
            "embedding_size: 256\n",
            "accuracy: 0.7457\n",
            "macro_f1: 0.7462\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "embedding_size: 128, accuracy: 0.7604938271604939, macro_f1: 0.7622715383006778\n"
          ]
        }
      ],
      "source": [
        "embedding_size_list = [50, 75, 100, 128, 256]\n",
        "best_embedding_size = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for embedding_size in embedding_size_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'embedding_size: {embedding_size}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_embedding_size = embedding_size\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'embedding_size: {best_embedding_size}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9bdfdc26",
      "metadata": {
        "id": "9bdfdc26"
      },
      "outputs": [],
      "source": [
        "embedding_size = best_embedding_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c091490",
      "metadata": {
        "id": "9c091490"
      },
      "source": [
        "### hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e1a2e439",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1a2e439",
        "outputId": "c8eb2d64-3694-475f-8b44-75d0d9447d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 11\n",
            "hidden_size: 50\n",
            "accuracy: 0.6864\n",
            "macro_f1: 0.6966\n",
            "----------------------------------------------------------------------------\n",
            "count: 12\n",
            "hidden_size: 75\n",
            "accuracy: 0.7062\n",
            "macro_f1: 0.7091\n",
            "----------------------------------------------------------------------------\n",
            "count: 13\n",
            "hidden_size: 100\n",
            "accuracy: 0.6691\n",
            "macro_f1: 0.6707\n",
            "----------------------------------------------------------------------------\n",
            "count: 14\n",
            "hidden_size: 128\n",
            "accuracy: 0.7679\n",
            "macro_f1: 0.7681\n",
            "----------------------------------------------------------------------------\n",
            "count: 15\n",
            "hidden_size: 256\n",
            "accuracy: 0.7383\n",
            "macro_f1: 0.7402\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "hidden_size: 128, accuracy: 0.7679012345679013, macro_f1: 0.7681442412489273\n"
          ]
        }
      ],
      "source": [
        "hidden_size_list = [50, 75, 100, 128, 256]\n",
        "best_hidden_size = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for hidden_size in hidden_size_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'hidden_size: {hidden_size}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_hidden_size = hidden_size\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'hidden_size: {best_hidden_size}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b5d13981",
      "metadata": {
        "id": "b5d13981"
      },
      "outputs": [],
      "source": [
        "hidden_size = best_hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78b04d57",
      "metadata": {
        "id": "78b04d57"
      },
      "source": [
        "### num_rnn_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "8533e918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8533e918",
        "outputId": "63735ac1-a181-4a1b-d1f3-399df3824e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 16\n",
            "num_rnn_layers: 1\n",
            "accuracy: 0.7556\n",
            "macro_f1: 0.7550\n",
            "----------------------------------------------------------------------------\n",
            "count: 17\n",
            "num_rnn_layers: 2\n",
            "accuracy: 0.7235\n",
            "macro_f1: 0.7258\n",
            "----------------------------------------------------------------------------\n",
            "count: 18\n",
            "num_rnn_layers: 3\n",
            "accuracy: 0.7432\n",
            "macro_f1: 0.7410\n",
            "----------------------------------------------------------------------------\n",
            "count: 19\n",
            "num_rnn_layers: 4\n",
            "accuracy: 0.7309\n",
            "macro_f1: 0.7295\n",
            "----------------------------------------------------------------------------\n",
            "count: 20\n",
            "num_rnn_layers: 5\n",
            "accuracy: 0.6864\n",
            "macro_f1: 0.6891\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "num_rnn_layers: 1, accuracy: 0.7555555555555555, macro_f1: 0.7549916795088715\n"
          ]
        }
      ],
      "source": [
        "num_rnn_layers_list = [1, 2, 3, 4, 5]\n",
        "best_num_rnn_layers = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for num_rnn_layers in num_rnn_layers_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'num_rnn_layers: {num_rnn_layers}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_num_rnn_layers = num_rnn_layers\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'num_rnn_layers: {best_num_rnn_layers}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0dd3e868",
      "metadata": {
        "id": "0dd3e868"
      },
      "outputs": [],
      "source": [
        "num_rnn_layers = best_num_rnn_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1592cf65",
      "metadata": {
        "id": "1592cf65"
      },
      "source": [
        "### num_mlp_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7506f58a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7506f58a",
        "outputId": "fee343c1-f79f-46dd-9663-3b077ef1b1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 21\n",
            "num_mlp_layers: 1\n",
            "accuracy: 0.7481\n",
            "macro_f1: 0.7460\n",
            "----------------------------------------------------------------------------\n",
            "count: 22\n",
            "num_mlp_layers: 2\n",
            "accuracy: 0.7383\n",
            "macro_f1: 0.7361\n",
            "----------------------------------------------------------------------------\n",
            "count: 23\n",
            "num_mlp_layers: 3\n",
            "accuracy: 0.7111\n",
            "macro_f1: 0.7123\n",
            "----------------------------------------------------------------------------\n",
            "count: 24\n",
            "num_mlp_layers: 4\n",
            "accuracy: 0.6519\n",
            "macro_f1: 0.6572\n",
            "----------------------------------------------------------------------------\n",
            "count: 25\n",
            "num_mlp_layers: 5\n",
            "accuracy: 0.1160\n",
            "macro_f1: 0.0315\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "num_mlp_layers: 1, accuracy: 0.7481481481481481, macro_f1: 0.7459761579986077\n"
          ]
        }
      ],
      "source": [
        "num_mlp_layers_list = [1, 2, 3, 4, 5]\n",
        "best_num_mlp_layers = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for num_mlp_layers in num_mlp_layers_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'num_mlp_layers: {num_mlp_layers}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_num_mlp_layers = num_mlp_layers\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'num_mlp_layers: {best_num_mlp_layers}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "dc770393",
      "metadata": {
        "id": "dc770393"
      },
      "outputs": [],
      "source": [
        "num_mlp_layers = best_num_mlp_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b303efe",
      "metadata": {
        "id": "7b303efe"
      },
      "source": [
        "### dropout_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "e14a1e2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e14a1e2b",
        "outputId": "af0342c4-27bb-4de7-e959-cc80607fc1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 26\n",
            "dropout_rate: 0.1\n",
            "accuracy: 0.7580\n",
            "macro_f1: 0.7559\n",
            "----------------------------------------------------------------------------\n",
            "count: 27\n",
            "dropout_rate: 0.3\n",
            "accuracy: 0.7210\n",
            "macro_f1: 0.7230\n",
            "----------------------------------------------------------------------------\n",
            "count: 28\n",
            "dropout_rate: 0.5\n",
            "accuracy: 0.7531\n",
            "macro_f1: 0.7545\n",
            "----------------------------------------------------------------------------\n",
            "count: 29\n",
            "dropout_rate: 0.6\n",
            "accuracy: 0.7333\n",
            "macro_f1: 0.7428\n",
            "----------------------------------------------------------------------------\n",
            "count: 30\n",
            "dropout_rate: 0.7\n",
            "accuracy: 0.6988\n",
            "macro_f1: 0.6975\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "dropout_rate: 0.1, accuracy: 0.7580246913580246, macro_f1: 0.7558710315978633\n"
          ]
        }
      ],
      "source": [
        "dropout_rate_list = [0.1, 0.3, 0.5, 0.6, 0.7]\n",
        "best_dropout_rate = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for dropout_rate in dropout_rate_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'dropout_rate: {dropout_rate}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_dropout_rate = dropout_rate\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'dropout_rate: {best_dropout_rate}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ba777e48",
      "metadata": {
        "id": "ba777e48"
      },
      "outputs": [],
      "source": [
        "dropout_rate = best_dropout_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f464f5e",
      "metadata": {
        "id": "5f464f5e"
      },
      "source": [
        "### batch_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "98773c3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98773c3c",
        "outputId": "1fee75b6-cde7-404c-9406-413915508a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 31\n",
            "batch_norm: True\n",
            "accuracy: 0.7407\n",
            "macro_f1: 0.7395\n",
            "----------------------------------------------------------------------------\n",
            "count: 32\n",
            "batch_norm: False\n",
            "accuracy: 0.7630\n",
            "macro_f1: 0.7656\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "batch_norm: False, accuracy: 0.762962962962963, macro_f1: 0.7655563998865346\n"
          ]
        }
      ],
      "source": [
        "batch_norm_list = [True, False]\n",
        "best_batch_norm = False\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for batch_norm in batch_norm_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'batch_norm: {batch_norm}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_batch_norm = batch_norm\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'batch_norm: {best_batch_norm}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "6df6104a",
      "metadata": {
        "id": "6df6104a"
      },
      "outputs": [],
      "source": [
        "batch_norm = best_batch_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c91175",
      "metadata": {
        "id": "58c91175"
      },
      "source": [
        "### l2_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "54f3758c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54f3758c",
        "outputId": "2f477b37-014c-4e43-8275-7c8de946a022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 33\n",
            "l2_reg: 0.001\n",
            "accuracy: 0.7704\n",
            "macro_f1: 0.7689\n",
            "----------------------------------------------------------------------------\n",
            "count: 34\n",
            "l2_reg: 0.005\n",
            "accuracy: 0.7012\n",
            "macro_f1: 0.7030\n",
            "----------------------------------------------------------------------------\n",
            "count: 35\n",
            "l2_reg: 0.01\n",
            "accuracy: 0.7728\n",
            "macro_f1: 0.7754\n",
            "----------------------------------------------------------------------------\n",
            "count: 36\n",
            "l2_reg: 0.1\n",
            "accuracy: 0.7259\n",
            "macro_f1: 0.7242\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "l2_reg: 0.01, accuracy: 0.7728395061728395, macro_f1: 0.7753505548328936\n"
          ]
        }
      ],
      "source": [
        "l2_reg_list = [0.001, 0.005, 0.01, 0.1]\n",
        "best_l2_reg = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for l2_reg in l2_reg_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'l2_reg: {l2_reg}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_l2_reg = l2_reg\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'l2_reg: {best_l2_reg}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "5333252d",
      "metadata": {
        "id": "5333252d"
      },
      "outputs": [],
      "source": [
        "l2_reg = best_l2_reg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6904d64d",
      "metadata": {
        "id": "6904d64d"
      },
      "source": [
        "### learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "9a8893a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a8893a0",
        "outputId": "b924a7ef-e92b-4a12-a258-4c0c064edc69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 37\n",
            "learning_rate: 0.0001\n",
            "accuracy: 0.7481\n",
            "macro_f1: 0.7513\n",
            "----------------------------------------------------------------------------\n",
            "count: 38\n",
            "learning_rate: 0.001\n",
            "accuracy: 0.7728\n",
            "macro_f1: 0.7728\n",
            "----------------------------------------------------------------------------\n",
            "count: 39\n",
            "learning_rate: 0.01\n",
            "accuracy: 0.7926\n",
            "macro_f1: 0.7935\n",
            "----------------------------------------------------------------------------\n",
            "count: 40\n",
            "learning_rate: 0.1\n",
            "accuracy: 0.6938\n",
            "macro_f1: 0.6939\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "learning_rate: 0.01, accuracy: 0.7925925925925926, macro_f1: 0.7934674385799617\n"
          ]
        }
      ],
      "source": [
        "learning_rate_list = [0.0001, 0.001, 0.01, 0.1]\n",
        "best_learning_rate = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for learning_rate in learning_rate_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'learning_rate: {learning_rate}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_learning_rate = learning_rate\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'learning_rate: {best_learning_rate}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "c17054f7",
      "metadata": {
        "id": "c17054f7"
      },
      "outputs": [],
      "source": [
        "learning_rate = best_learning_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31a48114",
      "metadata": {
        "id": "31a48114"
      },
      "source": [
        "## Final GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 200"
      ],
      "metadata": {
        "id": "G4td3OP2ezAx"
      },
      "id": "G4td3OP2ezAx",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "41a2d96e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41a2d96e",
        "outputId": "bcd6dc38-083c-43e9-f9d1-69ab0d5ae674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "37/37 [==============================] - 3s 30ms/step - loss: 2.2040 - accuracy: 0.1062 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1990 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 2/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2009 - accuracy: 0.1089 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1992 - val_accuracy: 0.1111 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 3/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.2013 - accuracy: 0.1048 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1994 - val_accuracy: 0.1111 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 4/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.2012 - accuracy: 0.1023 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1981 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 5/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.1988 - accuracy: 0.1163 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1986 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 6/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.1989 - accuracy: 0.1051 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1977 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 7/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.1989 - accuracy: 0.1018 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1980 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 8/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.2005 - accuracy: 0.1114 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1979 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 9/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1985 - accuracy: 0.1188 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1986 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 10/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.1996 - accuracy: 0.1070 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1979 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 11/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1978 - accuracy: 0.1075 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1975 - val_accuracy: 0.1111 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 12/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1969 - accuracy: 0.1114 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1975 - val_accuracy: 0.1111 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 13/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1974 - accuracy: 0.1026 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1975 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 14/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1974 - accuracy: 0.1021 - precision_53: 0.0000e+00 - recall_53: 0.0000e+00 - val_loss: 2.1978 - val_accuracy: 0.1111 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 15/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.1969 - accuracy: 0.1125 - precision_53: 1.0000 - recall_53: 2.7435e-04 - val_loss: 2.1975 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 16/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1960 - accuracy: 0.1045 - precision_53: 1.0000 - recall_53: 8.2305e-04 - val_loss: 2.1975 - val_accuracy: 0.1111 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 17/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.1965 - accuracy: 0.0993 - precision_53: 1.0000 - recall_53: 0.0011 - val_loss: 2.1975 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 18/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1962 - accuracy: 0.1051 - precision_53: 1.0000 - recall_53: 0.0016 - val_loss: 2.1974 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 19/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.1958 - accuracy: 0.1089 - precision_53: 1.0000 - recall_53: 0.0016 - val_loss: 2.1980 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 20/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1950 - accuracy: 0.1064 - precision_53: 1.0000 - recall_53: 0.0019 - val_loss: 2.1974 - val_accuracy: 0.1111 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 21/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1948 - accuracy: 0.1089 - precision_53: 1.0000 - recall_53: 0.0022 - val_loss: 2.1973 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 22/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1948 - accuracy: 0.1084 - precision_53: 1.0000 - recall_53: 0.0022 - val_loss: 2.1973 - val_accuracy: 0.1086 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 23/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1943 - accuracy: 0.1086 - precision_53: 1.0000 - recall_53: 0.0022 - val_loss: 2.1974 - val_accuracy: 0.1111 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 24/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1947 - accuracy: 0.1122 - precision_53: 1.0000 - recall_53: 0.0022 - val_loss: 2.1975 - val_accuracy: 0.1086 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 25/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1938 - accuracy: 0.1048 - precision_53: 1.0000 - recall_53: 0.0022 - val_loss: 2.1975 - val_accuracy: 0.1111 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 26/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.1941 - accuracy: 0.1075 - precision_53: 1.0000 - recall_53: 0.0022 - val_loss: 2.1974 - val_accuracy: 0.1136 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 27/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1940 - accuracy: 0.1106 - precision_53: 1.0000 - recall_53: 0.0022 - val_loss: 2.1974 - val_accuracy: 0.1111 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 28/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1932 - accuracy: 0.1064 - precision_53: 1.0000 - recall_53: 0.0022 - val_loss: 2.1953 - val_accuracy: 0.1111 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 29/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.1921 - accuracy: 0.1163 - precision_53: 1.0000 - recall_53: 0.0022 - val_loss: 2.1935 - val_accuracy: 0.1160 - val_precision_53: 0.0000e+00 - val_recall_53: 0.0000e+00\n",
            "Epoch 30/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0343 - accuracy: 0.1868 - precision_53: 0.4714 - recall_53: 0.0181 - val_loss: 1.8665 - val_accuracy: 0.2494 - val_precision_53: 1.0000 - val_recall_53: 0.0296\n",
            "Epoch 31/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.7612 - accuracy: 0.2554 - precision_53: 0.7253 - recall_53: 0.0464 - val_loss: 1.6887 - val_accuracy: 0.2765 - val_precision_53: 0.8611 - val_recall_53: 0.0765\n",
            "Epoch 32/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.4992 - accuracy: 0.3748 - precision_53: 0.7223 - recall_53: 0.1092 - val_loss: 1.4228 - val_accuracy: 0.4025 - val_precision_53: 0.7742 - val_recall_53: 0.1185\n",
            "Epoch 33/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.1591 - accuracy: 0.5040 - precision_53: 0.6925 - recall_53: 0.2422 - val_loss: 1.3131 - val_accuracy: 0.5309 - val_precision_53: 0.7834 - val_recall_53: 0.3037\n",
            "Epoch 34/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.8611 - accuracy: 0.6804 - precision_53: 0.7886 - recall_53: 0.5221 - val_loss: 1.1452 - val_accuracy: 0.6765 - val_precision_53: 0.7431 - val_recall_53: 0.6000\n",
            "Epoch 35/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.5332 - accuracy: 0.8381 - precision_53: 0.8637 - recall_53: 0.7995 - val_loss: 1.0897 - val_accuracy: 0.7111 - val_precision_53: 0.7534 - val_recall_53: 0.6938\n",
            "Epoch 36/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.3400 - accuracy: 0.9056 - precision_53: 0.9146 - recall_53: 0.8960 - val_loss: 1.1807 - val_accuracy: 0.7358 - val_precision_53: 0.7526 - val_recall_53: 0.7284\n",
            "Epoch 37/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.2614 - accuracy: 0.9314 - precision_53: 0.9361 - recall_53: 0.9248 - val_loss: 1.1862 - val_accuracy: 0.7160 - val_precision_53: 0.7347 - val_recall_53: 0.7111\n",
            "Epoch 38/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.1933 - accuracy: 0.9512 - precision_53: 0.9557 - recall_53: 0.9476 - val_loss: 1.2100 - val_accuracy: 0.7457 - val_precision_53: 0.7519 - val_recall_53: 0.7333\n",
            "Epoch 39/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 0.1391 - accuracy: 0.9695 - precision_53: 0.9713 - recall_53: 0.9660 - val_loss: 1.2477 - val_accuracy: 0.7383 - val_precision_53: 0.7468 - val_recall_53: 0.7284\n",
            "Epoch 40/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.1085 - accuracy: 0.9756 - precision_53: 0.9758 - recall_53: 0.9739 - val_loss: 1.2069 - val_accuracy: 0.7407 - val_precision_53: 0.7456 - val_recall_53: 0.7383\n",
            "Epoch 41/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 0.1043 - accuracy: 0.9761 - precision_53: 0.9772 - recall_53: 0.9753 - val_loss: 1.1953 - val_accuracy: 0.7679 - val_precision_53: 0.7711 - val_recall_53: 0.7654\n",
            "Epoch 42/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0767 - accuracy: 0.9830 - precision_53: 0.9835 - recall_53: 0.9813 - val_loss: 1.2381 - val_accuracy: 0.7556 - val_precision_53: 0.7652 - val_recall_53: 0.7481\n",
            "Epoch 43/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0603 - accuracy: 0.9874 - precision_53: 0.9885 - recall_53: 0.9868 - val_loss: 1.2728 - val_accuracy: 0.7580 - val_precision_53: 0.7741 - val_recall_53: 0.7531\n",
            "Epoch 44/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0471 - accuracy: 0.9915 - precision_53: 0.9920 - recall_53: 0.9912 - val_loss: 1.4426 - val_accuracy: 0.7432 - val_precision_53: 0.7500 - val_recall_53: 0.7407\n",
            "Epoch 45/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0480 - accuracy: 0.9909 - precision_53: 0.9912 - recall_53: 0.9901 - val_loss: 1.3565 - val_accuracy: 0.7654 - val_precision_53: 0.7758 - val_recall_53: 0.7605\n",
            "Epoch 46/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0372 - accuracy: 0.9934 - precision_53: 0.9940 - recall_53: 0.9934 - val_loss: 1.4399 - val_accuracy: 0.7481 - val_precision_53: 0.7544 - val_recall_53: 0.7432\n",
            "Epoch 47/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0380 - accuracy: 0.9920 - precision_53: 0.9926 - recall_53: 0.9920 - val_loss: 1.4772 - val_accuracy: 0.7383 - val_precision_53: 0.7431 - val_recall_53: 0.7358\n",
            "Epoch 48/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0464 - accuracy: 0.9882 - precision_53: 0.9893 - recall_53: 0.9877 - val_loss: 1.3389 - val_accuracy: 0.7432 - val_precision_53: 0.7519 - val_recall_53: 0.7407\n",
            "Epoch 49/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0408 - accuracy: 0.9904 - precision_53: 0.9909 - recall_53: 0.9896 - val_loss: 1.4907 - val_accuracy: 0.7235 - val_precision_53: 0.7289 - val_recall_53: 0.7235\n",
            "Epoch 50/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0299 - accuracy: 0.9940 - precision_53: 0.9948 - recall_53: 0.9940 - val_loss: 1.5103 - val_accuracy: 0.7531 - val_precision_53: 0.7587 - val_recall_53: 0.7531\n",
            "Epoch 51/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0248 - accuracy: 0.9956 - precision_53: 0.9956 - recall_53: 0.9956 - val_loss: 1.5496 - val_accuracy: 0.7481 - val_precision_53: 0.7506 - val_recall_53: 0.7432\n",
            "Epoch 52/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 0.0416 - accuracy: 0.9896 - precision_53: 0.9904 - recall_53: 0.9893 - val_loss: 1.5536 - val_accuracy: 0.7383 - val_precision_53: 0.7388 - val_recall_53: 0.7333\n",
            "Epoch 53/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0325 - accuracy: 0.9915 - precision_53: 0.9929 - recall_53: 0.9909 - val_loss: 1.4016 - val_accuracy: 0.7605 - val_precision_53: 0.7702 - val_recall_53: 0.7531\n",
            "Epoch 54/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0273 - accuracy: 0.9934 - precision_53: 0.9937 - recall_53: 0.9934 - val_loss: 1.4829 - val_accuracy: 0.7556 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 55/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0186 - accuracy: 0.9964 - precision_53: 0.9967 - recall_53: 0.9962 - val_loss: 1.5373 - val_accuracy: 0.7432 - val_precision_53: 0.7538 - val_recall_53: 0.7407\n",
            "Epoch 56/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0163 - accuracy: 0.9959 - precision_53: 0.9964 - recall_53: 0.9953 - val_loss: 1.5612 - val_accuracy: 0.7358 - val_precision_53: 0.7425 - val_recall_53: 0.7333\n",
            "Epoch 57/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0111 - accuracy: 0.9978 - precision_53: 0.9981 - recall_53: 0.9978 - val_loss: 1.5293 - val_accuracy: 0.7531 - val_precision_53: 0.7638 - val_recall_53: 0.7506\n",
            "Epoch 58/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0093 - accuracy: 0.9978 - precision_53: 0.9986 - recall_53: 0.9975 - val_loss: 1.5171 - val_accuracy: 0.7580 - val_precision_53: 0.7683 - val_recall_53: 0.7531\n",
            "Epoch 59/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0106 - accuracy: 0.9978 - precision_53: 0.9981 - recall_53: 0.9970 - val_loss: 1.5880 - val_accuracy: 0.7481 - val_precision_53: 0.7620 - val_recall_53: 0.7432\n",
            "Epoch 60/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0121 - accuracy: 0.9978 - precision_53: 0.9984 - recall_53: 0.9975 - val_loss: 1.5884 - val_accuracy: 0.7556 - val_precision_53: 0.7619 - val_recall_53: 0.7506\n",
            "Epoch 61/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0080 - accuracy: 0.9986 - precision_53: 0.9986 - recall_53: 0.9986 - val_loss: 1.6579 - val_accuracy: 0.7457 - val_precision_53: 0.7531 - val_recall_53: 0.7457\n",
            "Epoch 62/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0077 - accuracy: 0.9986 - precision_53: 0.9986 - recall_53: 0.9984 - val_loss: 1.6173 - val_accuracy: 0.7506 - val_precision_53: 0.7556 - val_recall_53: 0.7481\n",
            "Epoch 63/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0052 - accuracy: 0.9992 - precision_53: 0.9997 - recall_53: 0.9992 - val_loss: 1.6025 - val_accuracy: 0.7506 - val_precision_53: 0.7607 - val_recall_53: 0.7457\n",
            "Epoch 64/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0059 - accuracy: 0.9989 - precision_53: 0.9989 - recall_53: 0.9989 - val_loss: 1.6505 - val_accuracy: 0.7457 - val_precision_53: 0.7512 - val_recall_53: 0.7457\n",
            "Epoch 65/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0066 - accuracy: 0.9989 - precision_53: 0.9989 - recall_53: 0.9989 - val_loss: 1.6761 - val_accuracy: 0.7457 - val_precision_53: 0.7525 - val_recall_53: 0.7432\n",
            "Epoch 66/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0181 - accuracy: 0.9956 - precision_53: 0.9959 - recall_53: 0.9951 - val_loss: 1.6025 - val_accuracy: 0.7556 - val_precision_53: 0.7612 - val_recall_53: 0.7556\n",
            "Epoch 67/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0948 - accuracy: 0.9759 - precision_53: 0.9769 - recall_53: 0.9739 - val_loss: 1.3313 - val_accuracy: 0.7432 - val_precision_53: 0.7551 - val_recall_53: 0.7383\n",
            "Epoch 68/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0452 - accuracy: 0.9879 - precision_53: 0.9890 - recall_53: 0.9877 - val_loss: 1.3216 - val_accuracy: 0.7531 - val_precision_53: 0.7663 - val_recall_53: 0.7531\n",
            "Epoch 69/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0269 - accuracy: 0.9929 - precision_53: 0.9940 - recall_53: 0.9926 - val_loss: 1.3957 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 70/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0105 - accuracy: 0.9973 - precision_53: 0.9978 - recall_53: 0.9973 - val_loss: 1.4364 - val_accuracy: 0.7358 - val_precision_53: 0.7469 - val_recall_53: 0.7358\n",
            "Epoch 71/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0056 - accuracy: 0.9989 - precision_53: 0.9992 - recall_53: 0.9986 - val_loss: 1.5048 - val_accuracy: 0.7432 - val_precision_53: 0.7519 - val_recall_53: 0.7407\n",
            "Epoch 72/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0048 - accuracy: 0.9992 - precision_53: 0.9995 - recall_53: 0.9992 - val_loss: 1.5334 - val_accuracy: 0.7407 - val_precision_53: 0.7487 - val_recall_53: 0.7358\n",
            "Epoch 73/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0038 - accuracy: 0.9995 - precision_53: 0.9997 - recall_53: 0.9995 - val_loss: 1.5663 - val_accuracy: 0.7457 - val_precision_53: 0.7525 - val_recall_53: 0.7432\n",
            "Epoch 74/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0033 - accuracy: 0.9995 - precision_53: 0.9995 - recall_53: 0.9995 - val_loss: 1.5938 - val_accuracy: 0.7457 - val_precision_53: 0.7475 - val_recall_53: 0.7383\n",
            "Epoch 75/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0039 - accuracy: 0.9995 - precision_53: 0.9995 - recall_53: 0.9995 - val_loss: 1.6269 - val_accuracy: 0.7432 - val_precision_53: 0.7475 - val_recall_53: 0.7383\n",
            "Epoch 76/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0034 - accuracy: 0.9995 - precision_53: 0.9995 - recall_53: 0.9995 - val_loss: 1.6198 - val_accuracy: 0.7457 - val_precision_53: 0.7481 - val_recall_53: 0.7407\n",
            "Epoch 77/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0028 - accuracy: 0.9995 - precision_53: 0.9995 - recall_53: 0.9995 - val_loss: 1.6317 - val_accuracy: 0.7407 - val_precision_53: 0.7456 - val_recall_53: 0.7383\n",
            "Epoch 78/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0024 - accuracy: 0.9997 - precision_53: 0.9997 - recall_53: 0.9997 - val_loss: 1.6596 - val_accuracy: 0.7407 - val_precision_53: 0.7481 - val_recall_53: 0.7407\n",
            "Epoch 79/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 0.0027 - accuracy: 0.9997 - precision_53: 0.9997 - recall_53: 0.9997 - val_loss: 1.6677 - val_accuracy: 0.7407 - val_precision_53: 0.7419 - val_recall_53: 0.7383\n",
            "Epoch 80/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.9997 - precision_53: 0.9997 - recall_53: 0.9997 - val_loss: 1.6616 - val_accuracy: 0.7432 - val_precision_53: 0.7488 - val_recall_53: 0.7432\n",
            "Epoch 81/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0016 - accuracy: 0.9997 - precision_53: 0.9997 - recall_53: 0.9997 - val_loss: 1.6934 - val_accuracy: 0.7407 - val_precision_53: 0.7419 - val_recall_53: 0.7383\n",
            "Epoch 82/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0015 - accuracy: 0.9997 - precision_53: 0.9997 - recall_53: 0.9997 - val_loss: 1.7136 - val_accuracy: 0.7457 - val_precision_53: 0.7450 - val_recall_53: 0.7432\n",
            "Epoch 83/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0011 - accuracy: 0.9997 - precision_53: 0.9997 - recall_53: 0.9997 - val_loss: 1.7343 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 84/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0011 - accuracy: 0.9997 - precision_53: 0.9997 - recall_53: 0.9997 - val_loss: 1.7484 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 85/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0010 - accuracy: 0.9997 - precision_53: 0.9997 - recall_53: 0.9997 - val_loss: 1.7611 - val_accuracy: 0.7506 - val_precision_53: 0.7506 - val_recall_53: 0.7506\n",
            "Epoch 86/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.4066e-04 - accuracy: 0.9997 - precision_53: 1.0000 - recall_53: 0.9997 - val_loss: 1.7778 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 87/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 7.5075e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 0.9997 - val_loss: 1.7914 - val_accuracy: 0.7531 - val_precision_53: 0.7531 - val_recall_53: 0.7531\n",
            "Epoch 88/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 6.2916e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.8094 - val_accuracy: 0.7506 - val_precision_53: 0.7506 - val_recall_53: 0.7506\n",
            "Epoch 89/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.0432e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.7858 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 90/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.0522e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.8291 - val_accuracy: 0.7531 - val_precision_53: 0.7531 - val_recall_53: 0.7531\n",
            "Epoch 91/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 5.0166e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.8492 - val_accuracy: 0.7531 - val_precision_53: 0.7531 - val_recall_53: 0.7531\n",
            "Epoch 92/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 5.6985e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.8586 - val_accuracy: 0.7506 - val_precision_53: 0.7506 - val_recall_53: 0.7506\n",
            "Epoch 93/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 4.3178e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.8666 - val_accuracy: 0.7506 - val_precision_53: 0.7506 - val_recall_53: 0.7506\n",
            "Epoch 94/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.8197e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.8775 - val_accuracy: 0.7531 - val_precision_53: 0.7531 - val_recall_53: 0.7531\n",
            "Epoch 95/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 3.8316e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.8814 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 96/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 3.6793e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.8883 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 97/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 3.3150e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.8985 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 98/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.3936e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9062 - val_accuracy: 0.7556 - val_precision_53: 0.7612 - val_recall_53: 0.7556\n",
            "Epoch 99/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 3.2833e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9127 - val_accuracy: 0.7531 - val_precision_53: 0.7587 - val_recall_53: 0.7531\n",
            "Epoch 100/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 3.0105e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9210 - val_accuracy: 0.7531 - val_precision_53: 0.7587 - val_recall_53: 0.7531\n",
            "Epoch 101/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.6961e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9301 - val_accuracy: 0.7556 - val_precision_53: 0.7631 - val_recall_53: 0.7556\n",
            "Epoch 102/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.9295e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9383 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 103/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.1114e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9460 - val_accuracy: 0.7531 - val_precision_53: 0.7562 - val_recall_53: 0.7506\n",
            "Epoch 104/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.6019e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9550 - val_accuracy: 0.7506 - val_precision_53: 0.7556 - val_recall_53: 0.7481\n",
            "Epoch 105/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1684e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9628 - val_accuracy: 0.7506 - val_precision_53: 0.7556 - val_recall_53: 0.7481\n",
            "Epoch 106/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.2753e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9712 - val_accuracy: 0.7506 - val_precision_53: 0.7537 - val_recall_53: 0.7481\n",
            "Epoch 107/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1924e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9802 - val_accuracy: 0.7506 - val_precision_53: 0.7543 - val_recall_53: 0.7506\n",
            "Epoch 108/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1331e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9871 - val_accuracy: 0.7506 - val_precision_53: 0.7537 - val_recall_53: 0.7481\n",
            "Epoch 109/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 1.8667e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9944 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 110/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.8767e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 1.9998 - val_accuracy: 0.7531 - val_precision_53: 0.7562 - val_recall_53: 0.7506\n",
            "Epoch 111/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0183e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0069 - val_accuracy: 0.7531 - val_precision_53: 0.7587 - val_recall_53: 0.7531\n",
            "Epoch 112/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.7871e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0151 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 113/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.6786e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0211 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 114/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.6256e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0267 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 115/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.5423e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0357 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 116/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.5270e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0428 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 117/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.4693e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0508 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 118/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.4683e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0577 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 119/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.3831e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0635 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 120/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.3279e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0702 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 121/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.3346e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0761 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 122/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.2480e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0834 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 123/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.2252e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0912 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 124/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1949e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.0993 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 125/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.2146e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1064 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 126/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0750e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1132 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 127/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1324e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1211 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 128/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1057e-04 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1267 - val_accuracy: 0.7556 - val_precision_53: 0.7612 - val_recall_53: 0.7556\n",
            "Epoch 129/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 9.8034e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1335 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 130/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 9.7645e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1392 - val_accuracy: 0.7556 - val_precision_53: 0.7574 - val_recall_53: 0.7556\n",
            "Epoch 131/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 9.7328e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1470 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 132/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 9.6974e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1542 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 133/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 9.1005e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1615 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 134/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 8.7057e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1678 - val_accuracy: 0.7556 - val_precision_53: 0.7574 - val_recall_53: 0.7556\n",
            "Epoch 135/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 8.2421e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1742 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 136/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 8.6050e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1816 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 137/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 7.9361e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1884 - val_accuracy: 0.7556 - val_precision_53: 0.7593 - val_recall_53: 0.7556\n",
            "Epoch 138/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 8.0407e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.1951 - val_accuracy: 0.7556 - val_precision_53: 0.7574 - val_recall_53: 0.7556\n",
            "Epoch 139/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 7.4267e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2011 - val_accuracy: 0.7556 - val_precision_53: 0.7574 - val_recall_53: 0.7556\n",
            "Epoch 140/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 7.8192e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2087 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 141/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 7.5053e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2146 - val_accuracy: 0.7531 - val_precision_53: 0.7568 - val_recall_53: 0.7531\n",
            "Epoch 142/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 7.0104e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2216 - val_accuracy: 0.7556 - val_precision_53: 0.7574 - val_recall_53: 0.7556\n",
            "Epoch 143/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 6.9382e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2277 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 144/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 6.9924e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2349 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 145/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 6.5012e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2423 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 146/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 6.2070e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2483 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 147/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 6.0479e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2538 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 148/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 5.8672e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2613 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 149/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.1583e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2670 - val_accuracy: 0.7531 - val_precision_53: 0.7550 - val_recall_53: 0.7531\n",
            "Epoch 150/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 5.9156e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2737 - val_accuracy: 0.7506 - val_precision_53: 0.7525 - val_recall_53: 0.7506\n",
            "Epoch 151/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 5.7182e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2799 - val_accuracy: 0.7506 - val_precision_53: 0.7525 - val_recall_53: 0.7506\n",
            "Epoch 152/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 5.4670e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2860 - val_accuracy: 0.7506 - val_precision_53: 0.7525 - val_recall_53: 0.7506\n",
            "Epoch 153/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 5.5885e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2926 - val_accuracy: 0.7506 - val_precision_53: 0.7525 - val_recall_53: 0.7506\n",
            "Epoch 154/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 5.2846e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.2995 - val_accuracy: 0.7506 - val_precision_53: 0.7525 - val_recall_53: 0.7506\n",
            "Epoch 155/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 5.1758e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3063 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 156/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 4.8127e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3120 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 157/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 4.8921e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3185 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 158/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 4.6875e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3232 - val_accuracy: 0.7481 - val_precision_53: 0.7519 - val_recall_53: 0.7481\n",
            "Epoch 159/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 4.8849e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3310 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 160/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 4.6190e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3376 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 161/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 4.2847e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3440 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 162/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 4.3366e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3500 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 163/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 4.2171e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3565 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 164/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 4.0977e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3623 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 165/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.9506e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3691 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 166/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.9574e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3767 - val_accuracy: 0.7481 - val_precision_53: 0.7500 - val_recall_53: 0.7481\n",
            "Epoch 167/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.9605e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3856 - val_accuracy: 0.7457 - val_precision_53: 0.7475 - val_recall_53: 0.7457\n",
            "Epoch 168/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.8645e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.3935 - val_accuracy: 0.7457 - val_precision_53: 0.7475 - val_recall_53: 0.7457\n",
            "Epoch 169/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.8231e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4034 - val_accuracy: 0.7457 - val_precision_53: 0.7475 - val_recall_53: 0.7457\n",
            "Epoch 170/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.6480e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4115 - val_accuracy: 0.7457 - val_precision_53: 0.7475 - val_recall_53: 0.7457\n",
            "Epoch 171/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.7176e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4168 - val_accuracy: 0.7457 - val_precision_53: 0.7475 - val_recall_53: 0.7457\n",
            "Epoch 172/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.3516e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4266 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 173/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.7561e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4337 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 174/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.6934e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4402 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 175/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.3709e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4447 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 176/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.1661e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4505 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 177/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 3.1988e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4559 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 178/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.0664e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4624 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 179/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.0509e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4709 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 180/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.9328e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4774 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 181/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.8798e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4842 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 182/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.0203e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4897 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 183/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.8539e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.4956 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 184/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.5976e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5020 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 185/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.6184e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5079 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 186/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.6205e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5142 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 187/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.5236e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5195 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 188/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.5266e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5254 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 189/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3407e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5311 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 190/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3541e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5371 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 191/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3847e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5434 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 192/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2859e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5496 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 193/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3464e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5547 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 194/200\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 2.2054e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5600 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 195/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2103e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5661 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 196/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0791e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5721 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 197/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.1195e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5767 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 198/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0641e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5831 - val_accuracy: 0.7432 - val_precision_53: 0.7469 - val_recall_53: 0.7432\n",
            "Epoch 199/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.9750e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5885 - val_accuracy: 0.7457 - val_precision_53: 0.7494 - val_recall_53: 0.7457\n",
            "Epoch 200/200\n",
            "37/37 [==============================] - 0s 14ms/step - loss: 1.9076e-05 - accuracy: 1.0000 - precision_53: 1.0000 - recall_53: 1.0000 - val_loss: 2.5942 - val_accuracy: 0.7457 - val_precision_53: 0.7494 - val_recall_53: 0.7457\n",
            "count: 40\n",
            "learning_rate: 0.01\n",
            "accuracy: 0.7679\n",
            "macro_f1: 0.7711\n"
          ]
        }
      ],
      "source": [
        "model = build_RNN(input_length=max_len, \n",
        "                      vocab_size=len(feats_dict), \n",
        "                      embedding_size=embedding_size,\n",
        "                      hidden_size=hidden_size, \n",
        "                      output_size=9,\n",
        "                      num_rnn_layers=num_rnn_layers, \n",
        "                      num_mlp_layers=num_mlp_layers,\n",
        "                      rnn_type=rnn_type,\n",
        "                      bidirectional=bidirectional,\n",
        "                      activation=activation,\n",
        "                      dropout_rate=dropout_rate,\n",
        "                      batch_norm=batch_norm,\n",
        "                      l2_reg=l2_reg,\n",
        "                      optimizer=optimizer,\n",
        "                      learning_rate=learning_rate,\n",
        "                    )\n",
        "\n",
        "checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(\"models\", f\"weights_RNN_final.hdf5\"),\n",
        "    monitor=\"val_accuracy\",\n",
        "    verbose=0,\n",
        "    save_best_only=True)\n",
        "\n",
        "mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                    validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                    epochs=epoch, batch_size=batch_size, verbose=1,\n",
        "                    callbacks=[checkpointer])\n",
        "\n",
        "model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_final.hdf5\"))\n",
        "\n",
        "print(f'count: {count}')\n",
        "print(f'learning_rate: {learning_rate}')\n",
        "\n",
        "# evaluation\n",
        "# generate prediction and format\n",
        "y_pred = model.predict(valid_feats_matrix)\n",
        "y_pred = [np.argmax(row) for row in y_pred]\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# evaluate performance\n",
        "acc = accuracy_score(valid_labels, y_pred)\n",
        "f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "print(f'accuracy: {acc:.4f}')\n",
        "print(f'macro_f1: {f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be611d51",
      "metadata": {
        "id": "be611d51"
      },
      "source": [
        "## Final Model Test Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "daeaba61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daeaba61",
        "outputId": "da755a8a-1792-41a4-fac1-9f7694a13ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.7489\n",
            "test macro_f1: 0.7543\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CC       0.93      0.82      0.87        50\n",
            "          NC       0.88      0.74      0.80        50\n",
            "          PW       0.91      0.80      0.85        50\n",
            "          HC       0.49      0.56      0.52        50\n",
            "          PL       0.59      0.68      0.63        50\n",
            "          CR       0.76      0.76      0.76        50\n",
            "          CG       0.89      0.80      0.84        50\n",
            "          BE       0.68      0.78      0.73        50\n",
            "           N       0.75      0.80      0.78        50\n",
            "\n",
            "    accuracy                           0.75       450\n",
            "   macro avg       0.77      0.75      0.75       450\n",
            "weighted avg       0.77      0.75      0.75       450\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# generate prediction and format\n",
        "y_pred = model.predict(test_feats_matrix)\n",
        "y_pred = [np.argmax(row) for row in y_pred]\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# evaluate performance\n",
        "acc = accuracy_score(test_labels, y_pred)\n",
        "f1 = f1_score(test_labels, y_pred, average='macro')\n",
        "print(f'test accuracy: {acc:.4f}')\n",
        "print(f'test macro_f1: {f1:.4f}')\n",
        "print(classification_report(test_labels, y_pred,target_names=labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "fd00c54b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "fd00c54b",
        "outputId": "3fb657ee-c1c0-46e1-9ca7-2d2fedb2094b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f1bc8f82a10>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot confusion matrix\n",
        "cm = confusion_matrix(test_labels, y_pred , normalize='pred')\n",
        "cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "cmp.plot(ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c481ea3e",
      "metadata": {
        "id": "c481ea3e"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "name": "UROP_gru.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}