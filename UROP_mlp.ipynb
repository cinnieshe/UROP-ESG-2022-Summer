{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b673ea",
   "metadata": {},
   "source": [
    "# Fine-Tune Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23389ab5",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25c38f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization, Activation, Input, Add, Concatenate\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f98f1",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca69b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Input: string filename\n",
    "    Output: a pandas dataframe for the whole dataset after droping missing values\n",
    "    Support google colab or local environments\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # local environment\n",
    "        df = pd.read_csv(filename)\n",
    "        df = df.dropna(subset=['sentence', 'label']) ## drop missing values\n",
    "        return df\n",
    "    except:\n",
    "        # google colab environment\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        df = pd.read_csv('/content/drive/MyDrive/' + filename)\n",
    "        df = df.dropna(subset=['sentence', 'label']) ## drop missing values\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33274281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    \"\"\"\n",
    "    Input: pandas dataframe\n",
    "    Output: training dataframe (81%), validation dataframe (9%), test dataframe (10%)\n",
    "    \"\"\"\n",
    "    df_train, df_val = train_test_split(df, stratify=df['label'],test_size=0.1, random_state=42)\n",
    "    \n",
    "    return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84af0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "    \n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
    "\n",
    "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list(list)\n",
    "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n",
    "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n",
    "    :param max_size: the max size of feature dict, type: int\n",
    "    return a feature dict that maps features to indices, sorted by frequencies\n",
    "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
    "    \"\"\"\n",
    "    # count all features\n",
    "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
    "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
    "        valid_feats = [f for f, cnt in feat_cnt.most_common(max_size)]\n",
    "    else:\n",
    "        valid_feats = list()\n",
    "        for f, cnt in feat_cnt.most_common():\n",
    "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
    "                (max_freq == -1 or cnt <= max_freq):\n",
    "                valid_feats.append(f)\n",
    "    if max_size > 0 and len(valid_feats) > max_size:\n",
    "        valid_feats = valid_feats[:max_size]        \n",
    "    print(\"Size of features:\", len(valid_feats))\n",
    "    \n",
    "    # build a mapping from features to indices\n",
    "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
    "    return feats_dict\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd20c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MLP(input_size, output_size, num_layers, hidden_size,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              layer_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"SGD\",\n",
    "              learning_rate=0.1,\n",
    "              metric=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]):\n",
    "    \"\"\"\n",
    "    :param input_size: the dimension of the input, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_layers: the number of layers, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param layer_norm: whether to enable layer normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a multi-layer perceptron,\n",
    "    # activation\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # layer normalization: https://github.com/CyberZHG/keras-layer-normalization\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        model.add(Dense(output_size,\n",
    "                        activation=\"softmax\",\n",
    "                        input_dim=input_size,\n",
    "                        kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                        bias_initializer=\"zeros\",\n",
    "                        kernel_regularizer=keras.regularizers.l2(l2_reg)))\n",
    "    else:\n",
    "        for i in range(num_layers-1):\n",
    "\n",
    "            if i == 0:\n",
    "                # fitst layer: input -> hidden\n",
    "                model.add(Dense(hidden_size,\n",
    "                                input_dim=input_size,\n",
    "                                kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                                bias_initializer=\"zeros\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(l2_reg)))\n",
    "            else:\n",
    "                # hidden layers: hidden -> hidden\n",
    "                model.add(Dense(hidden_size,\n",
    "                                input_dim=hidden_size,\n",
    "                                kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                                bias_initializer=\"zeros\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(l2_reg)))\n",
    "            # add layer_norm\n",
    "            if layer_norm:\n",
    "                model.add(LayerNormalization())\n",
    "            # add batch_norm\n",
    "            if batch_norm:\n",
    "                model.add(BatchNormalization())\n",
    "            # add activation\n",
    "            model.add(Activation(activation))\n",
    "            # add dropout here (set seed as 0 in order to reproduce)\n",
    "            if dropout_rate > 0.0:\n",
    "                model.add(Dropout(dropout_rate, seed=0))\n",
    "        \n",
    "        # last layer: hidden -> class\n",
    "        model.add(Dense(output_size,\n",
    "                        activation=\"softmax\",\n",
    "                        input_dim=hidden_size,\n",
    "                        kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                        bias_initializer=\"zeros\"))\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d91dfa",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b51b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'final_dataset_formatted.csv'\n",
    "TEST_FILENAME = 'final_dataset_formatted_test.csv'\n",
    "\n",
    "# load data\n",
    "df = load_data(FILENAME)\n",
    "df_test = load_data(TEST_FILENAME)\n",
    "\n",
    "# labels\n",
    "labels = ['CC', 'NC', 'PW', 'HC', 'PL', 'CR', 'CG', 'BE', 'N']\n",
    "num_labels = 9\n",
    "\n",
    "# split data\n",
    "df_train, df_val = split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d87645c",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a5e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text and labels\n",
    "train_texts = df_train.iloc[:, 0]\n",
    "train_labels = df_train.iloc[:, 1]\n",
    "valid_texts = df_val.iloc[:, 0]\n",
    "valid_labels = df_val.iloc[:, 1]\n",
    "test_texts = df_test.iloc[:, 0]\n",
    "test_labels = df_test.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff3f2c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 3645\n",
      "valid size: 405\n",
      "test size: 450\n"
     ]
    }
   ],
   "source": [
    "# get train, validation, and test dataset size\n",
    "train_size = len(train_texts)\n",
    "valid_size = len(valid_texts)\n",
    "test_size = len(test_texts)\n",
    "\n",
    "print(f'train size: {train_size}')\n",
    "print(f'valid size: {valid_size}')\n",
    "print(f'test size: {test_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edd0cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "valid_tokens = [tokenize(text) for text in valid_texts]\n",
    "test_tokens = [tokenize(text) for text in test_texts]\n",
    "\n",
    "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
    "valid_stemmed = [stem(tokens) for tokens in valid_tokens]\n",
    "test_stemmed = [stem(tokens) for tokens in test_tokens]\n",
    "\n",
    "train_stemmed = [filter_stopwords(tokens) for tokens in train_stemmed]\n",
    "valid_stemmed = [filter_stopwords(tokens) for tokens in valid_stemmed]\n",
    "test_stemmed = [filter_stopwords(tokens) for tokens in test_stemmed]\n",
    "\n",
    "train_2_gram = [n_gram(tokens, 2) for tokens in train_stemmed]\n",
    "train_3_gram = [n_gram(tokens, 3) for tokens in train_stemmed]\n",
    "train_4_gram = [n_gram(tokens, 4) for tokens in train_stemmed]\n",
    "valid_2_gram = [n_gram(tokens, 2) for tokens in valid_stemmed]\n",
    "valid_3_gram = [n_gram(tokens, 3) for tokens in valid_stemmed]\n",
    "valid_4_gram = [n_gram(tokens, 4) for tokens in valid_stemmed]\n",
    "test_2_gram = [n_gram(tokens, 2) for tokens in test_stemmed]\n",
    "test_3_gram = [n_gram(tokens, 3) for tokens in test_stemmed]\n",
    "test_4_gram = [n_gram(tokens, 4) for tokens in test_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0812996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of features: 4431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cinni\\AppData\\Local\\Temp\\ipykernel_14672\\3336279750.py:84: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  vector = np.zeros(len(feats_dict), dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "# build the feature list\n",
    "train_feats = list()\n",
    "for i in range(train_size):\n",
    "    train_feats.append(\n",
    "        train_stemmed[i] + train_2_gram[i] + train_3_gram[i] + train_4_gram[i])\n",
    "    \n",
    "valid_feats = list()\n",
    "for i in range(valid_size):\n",
    "    valid_feats.append(\n",
    "        valid_stemmed[i] + valid_2_gram[i] + valid_3_gram[i] + valid_4_gram[i])\n",
    "\n",
    "test_feats = list()\n",
    "for i in range(test_size):\n",
    "    test_feats.append(\n",
    "        test_stemmed[i] + test_2_gram[i] + test_3_gram[i] + test_4_gram[i])\n",
    "\n",
    "# build a mapping from features to indices\n",
    "feats_dict = get_feats_dict(\n",
    "                chain.from_iterable(train_feats),\n",
    "                min_freq=4)\n",
    "\n",
    "train_feats_matrix = np.vstack([get_onehot_vector(f, feats_dict) for f in train_feats])\n",
    "valid_feats_matrix = np.vstack([get_onehot_vector(f, feats_dict) for f in valid_feats])\n",
    "test_feats_matrix = np.vstack([get_onehot_vector(f, feats_dict) for f in test_feats])\n",
    "\n",
    "train_label_matrix = tf.keras.utils.to_categorical(train_labels, num_classes=9)\n",
    "valid_label_matrix = tf.keras.utils.to_categorical(valid_labels, num_classes=9)\n",
    "test_label_matrix = tf.keras.utils.to_categorical(test_labels, num_classes=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d2bdc",
   "metadata": {},
   "source": [
    "## Fine-Tune Model  \n",
    "  \n",
    "num_layers  \n",
    "hidden_size  \n",
    "activation  \n",
    "dropout_rate  \n",
    "batch_norm  \n",
    "layer_norm  \n",
    "l2_reg  \n",
    "optimizer  \n",
    "learning_rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2de8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf61bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicator for saving models' weights\n",
    "count = 0\n",
    "\n",
    "# initial settings\n",
    "epoch = 20\n",
    "batch_size = 100\n",
    "optimizer = \"SGD\"\n",
    "num_layers = 3\n",
    "hidden_size = 300\n",
    "activation = \"relu\"\n",
    "dropout_rate = 0.3\n",
    "batch_norm = False\n",
    "layer_norm = False\n",
    "l2_reg = 0.005\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fa35d",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ee7b825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 1\n",
      "optimizer: SGD\n",
      "13/13 [==============================] - 0s 6ms/step\n",
      "accuracy: 0.8469\n",
      "macro_f1: 0.8468\n",
      "----------------------------------------------------------------------------\n",
      "count: 2\n",
      "optimizer: RMSprop\n",
      "13/13 [==============================] - 0s 6ms/step\n",
      "accuracy: 0.8444\n",
      "macro_f1: 0.8435\n",
      "----------------------------------------------------------------------------\n",
      "count: 3\n",
      "optimizer: Adam\n",
      "13/13 [==============================] - 0s 6ms/step\n",
      "accuracy: 0.8519\n",
      "macro_f1: 0.8526\n",
      "----------------------------------------------------------------------------\n",
      "Best model:\n",
      "optimizer: Adam, accuracy: 0.8518518518518519, macro_f1: 0.8526174686856167\n"
     ]
    }
   ],
   "source": [
    "optimizer_list = ['SGD', 'RMSprop', 'Adam']\n",
    "best_optimizer = ''\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for optimizer in optimizer_list:\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    model = build_MLP(input_size=len(feats_dict), \n",
    "                      output_size=num_labels,\n",
    "                      num_layers=num_layers,\n",
    "                      hidden_size=hidden_size,\n",
    "                      activation=activation,\n",
    "                      dropout_rate=dropout_rate, \n",
    "                      batch_norm=batch_norm,\n",
    "                      layer_norm=layer_norm, \n",
    "                      l2_reg=l2_reg,\n",
    "                      optimizer=optimizer, \n",
    "                      learning_rate=learning_rate\n",
    "                     )\n",
    "\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"),\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=0,\n",
    "        save_best_only=True)\n",
    "\n",
    "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
    "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "\n",
    "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"))\n",
    "\n",
    "    print(f'count: {count}')\n",
    "    print(f'optimizer: {optimizer}')\n",
    "\n",
    "    # evaluation\n",
    "    # generate prediction and format\n",
    "    y_pred = model.predict(valid_feats_matrix)\n",
    "    y_pred = [np.argmax(row) for row in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # evaluate performance\n",
    "    acc = accuracy_score(valid_labels, y_pred)\n",
    "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
    "    print(f'accuracy: {acc:.4f}')\n",
    "    print(f'macro_f1: {f1:.4f}')\n",
    "\n",
    "    # save best model\n",
    "    if f1 > best_f1:\n",
    "        best_optimizer = optimizer\n",
    "        best_acc = acc\n",
    "        best_f1 = f1\n",
    "    \n",
    "    print('----------------------------------------------------------------------------')\n",
    "\n",
    "print('Best model:')\n",
    "print(f'optimizer: {best_optimizer}, accuracy: {best_acc}, macro_f1: {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e631e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = best_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a80369",
   "metadata": {},
   "source": [
    "### num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2155dc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 4\n",
      "num_layers: 1\n",
      "13/13 [==============================] - 0s 3ms/step\n",
      "accuracy: 0.8296\n",
      "macro_f1: 0.8304\n",
      "----------------------------------------------------------------------------\n",
      "count: 5\n",
      "num_layers: 2\n",
      "13/13 [==============================] - 0s 6ms/step\n",
      "accuracy: 0.8519\n",
      "macro_f1: 0.8515\n",
      "----------------------------------------------------------------------------\n",
      "count: 6\n",
      "num_layers: 3\n",
      "13/13 [==============================] - 0s 6ms/step\n",
      "accuracy: 0.8543\n",
      "macro_f1: 0.8548\n",
      "----------------------------------------------------------------------------\n",
      "count: 7\n",
      "num_layers: 4\n",
      "13/13 [==============================] - 0s 11ms/step\n",
      "accuracy: 0.8469\n",
      "macro_f1: 0.8473\n",
      "----------------------------------------------------------------------------\n",
      "count: 8\n",
      "num_layers: 5\n",
      "13/13 [==============================] - 0s 10ms/step\n",
      "accuracy: 0.8370\n",
      "macro_f1: 0.8378\n",
      "----------------------------------------------------------------------------\n",
      "Best model:\n",
      "num_layers: 3, accuracy: 0.854320987654321, macro_f1: 0.8548146572378331\n"
     ]
    }
   ],
   "source": [
    "num_layers_list = [1, 2, 3, 4, 5]\n",
    "best_num_layers = 0\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for num_layers in num_layers_list:\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    model = build_MLP(input_size=len(feats_dict), \n",
    "                      output_size=num_labels,\n",
    "                      num_layers=num_layers,\n",
    "                      hidden_size=hidden_size,\n",
    "                      activation=activation,\n",
    "                      dropout_rate=dropout_rate, \n",
    "                      batch_norm=batch_norm,\n",
    "                      layer_norm=layer_norm, \n",
    "                      l2_reg=l2_reg,\n",
    "                      optimizer=optimizer, \n",
    "                      learning_rate=learning_rate\n",
    "                     )\n",
    "\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"),\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=0,\n",
    "        save_best_only=True)\n",
    "\n",
    "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
    "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "\n",
    "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"))\n",
    "\n",
    "    print(f'count: {count}')\n",
    "    print(f'num_layers: {num_layers}')\n",
    "\n",
    "    # evaluation\n",
    "    # generate prediction and format\n",
    "    y_pred = model.predict(valid_feats_matrix)\n",
    "    y_pred = [np.argmax(row) for row in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # evaluate performance\n",
    "    acc = accuracy_score(valid_labels, y_pred)\n",
    "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
    "    print(f'accuracy: {acc:.4f}')\n",
    "    print(f'macro_f1: {f1:.4f}')\n",
    "\n",
    "    # save best model\n",
    "    if f1 > best_f1:\n",
    "        best_num_layers = num_layers\n",
    "        best_acc = acc\n",
    "        best_f1 = f1\n",
    "    \n",
    "    print('----------------------------------------------------------------------------')\n",
    "\n",
    "print('Best model:')\n",
    "print(f'num_layers: {best_num_layers}, accuracy: {best_acc}, macro_f1: {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a5ac272",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = best_num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df3c92",
   "metadata": {},
   "source": [
    "### hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd306e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 9\n",
      "hidden_size: 100\n",
      "13/13 [==============================] - 0s 7ms/step\n",
      "accuracy: 0.8543\n",
      "macro_f1: 0.8555\n",
      "----------------------------------------------------------------------------\n",
      "count: 10\n",
      "hidden_size: 200\n",
      "13/13 [==============================] - 0s 7ms/step\n",
      "accuracy: 0.8593\n",
      "macro_f1: 0.8592\n",
      "----------------------------------------------------------------------------\n",
      "count: 11\n",
      "hidden_size: 300\n",
      "13/13 [==============================] - 0s 7ms/step\n",
      "accuracy: 0.8494\n",
      "macro_f1: 0.8497\n",
      "----------------------------------------------------------------------------\n",
      "count: 12\n",
      "hidden_size: 400\n",
      "13/13 [==============================] - 0s 10ms/step\n",
      "accuracy: 0.8469\n",
      "macro_f1: 0.8473\n",
      "----------------------------------------------------------------------------\n",
      "count: 13\n",
      "hidden_size: 500\n",
      "13/13 [==============================] - 0s 10ms/step\n",
      "accuracy: 0.8494\n",
      "macro_f1: 0.8496\n",
      "----------------------------------------------------------------------------\n",
      "Best model:\n",
      "hidden_size: 200, accuracy: 0.8592592592592593, macro_f1: 0.8591924744695861\n"
     ]
    }
   ],
   "source": [
    "hidden_size_list = [100, 200, 300, 400, 500]\n",
    "best_hidden_size = 0\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for hidden_size in hidden_size_list:\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    model = build_MLP(input_size=len(feats_dict), \n",
    "                      output_size=num_labels,\n",
    "                      num_layers=num_layers,\n",
    "                      hidden_size=hidden_size,\n",
    "                      activation=activation,\n",
    "                      dropout_rate=dropout_rate, \n",
    "                      batch_norm=batch_norm,\n",
    "                      layer_norm=layer_norm, \n",
    "                      l2_reg=l2_reg,\n",
    "                      optimizer=optimizer, \n",
    "                      learning_rate=learning_rate\n",
    "                     )\n",
    "\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"),\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=0,\n",
    "        save_best_only=True)\n",
    "\n",
    "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
    "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "\n",
    "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"))\n",
    "\n",
    "    print(f'count: {count}')\n",
    "    print(f'hidden_size: {hidden_size}')\n",
    "\n",
    "    # evaluation\n",
    "    # generate prediction and format\n",
    "    y_pred = model.predict(valid_feats_matrix)\n",
    "    y_pred = [np.argmax(row) for row in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # evaluate performance\n",
    "    acc = accuracy_score(valid_labels, y_pred)\n",
    "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
    "    print(f'accuracy: {acc:.4f}')\n",
    "    print(f'macro_f1: {f1:.4f}')\n",
    "\n",
    "    # save best model\n",
    "    if f1 > best_f1:\n",
    "        best_hidden_size = hidden_size\n",
    "        best_acc = acc\n",
    "        best_f1 = f1\n",
    "    \n",
    "    print('----------------------------------------------------------------------------')\n",
    "\n",
    "print('Best model:')\n",
    "print(f'hidden_size: {best_hidden_size}, accuracy: {best_acc}, macro_f1: {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f5205d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = best_hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357cd57",
   "metadata": {},
   "source": [
    "### dropout_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db1a6843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 14\n",
      "dropout_rate: 0\n",
      "13/13 [==============================] - 0s 8ms/step\n",
      "accuracy: 0.8420\n",
      "macro_f1: 0.8426\n",
      "----------------------------------------------------------------------------\n",
      "count: 15\n",
      "dropout_rate: 0.1\n",
      "13/13 [==============================] - 0s 8ms/step\n",
      "accuracy: 0.8444\n",
      "macro_f1: 0.8452\n",
      "----------------------------------------------------------------------------\n",
      "count: 16\n",
      "dropout_rate: 0.2\n",
      "13/13 [==============================] - 0s 6ms/step\n",
      "accuracy: 0.8543\n",
      "macro_f1: 0.8546\n",
      "----------------------------------------------------------------------------\n",
      "count: 17\n",
      "dropout_rate: 0.3\n",
      "13/13 [==============================] - 1s 7ms/step\n",
      "accuracy: 0.8469\n",
      "macro_f1: 0.8477\n",
      "----------------------------------------------------------------------------\n",
      "count: 18\n",
      "dropout_rate: 0.4\n",
      "13/13 [==============================] - 1s 13ms/step\n",
      "accuracy: 0.8593\n",
      "macro_f1: 0.8591\n",
      "----------------------------------------------------------------------------\n",
      "count: 19\n",
      "dropout_rate: 0.5\n",
      "13/13 [==============================] - 1s 11ms/step\n",
      "accuracy: 0.8543\n",
      "macro_f1: 0.8541\n",
      "----------------------------------------------------------------------------\n",
      "count: 20\n",
      "dropout_rate: 0.6\n",
      "13/13 [==============================] - 1s 18ms/step\n",
      "accuracy: 0.8543\n",
      "macro_f1: 0.8539\n",
      "----------------------------------------------------------------------------\n",
      "count: 21\n",
      "dropout_rate: 0.7\n",
      "13/13 [==============================] - 2s 21ms/step\n",
      "accuracy: 0.8617\n",
      "macro_f1: 0.8619\n",
      "----------------------------------------------------------------------------\n",
      "Best model:\n",
      "dropout_rate: 0.7, accuracy: 0.8617283950617284, macro_f1: 0.8618539829412593\n"
     ]
    }
   ],
   "source": [
    "dropout_rate_list = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "best_dropout_rate = 0\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for dropout_rate in dropout_rate_list:\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    model = build_MLP(input_size=len(feats_dict), \n",
    "                      output_size=num_labels,\n",
    "                      num_layers=num_layers,\n",
    "                      hidden_size=hidden_size,\n",
    "                      activation=activation,\n",
    "                      dropout_rate=dropout_rate, \n",
    "                      batch_norm=batch_norm,\n",
    "                      layer_norm=layer_norm, \n",
    "                      l2_reg=l2_reg,\n",
    "                      optimizer=optimizer, \n",
    "                      learning_rate=learning_rate\n",
    "                     )\n",
    "\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"),\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=0,\n",
    "        save_best_only=True)\n",
    "\n",
    "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
    "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "\n",
    "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"))\n",
    "\n",
    "    print(f'count: {count}')\n",
    "    print(f'dropout_rate: {dropout_rate}')\n",
    "\n",
    "    # evaluation\n",
    "    # generate prediction and format\n",
    "    y_pred = model.predict(valid_feats_matrix)\n",
    "    y_pred = [np.argmax(row) for row in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # evaluate performance\n",
    "    acc = accuracy_score(valid_labels, y_pred)\n",
    "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
    "    print(f'accuracy: {acc:.4f}')\n",
    "    print(f'macro_f1: {f1:.4f}')\n",
    "\n",
    "    # save best model\n",
    "    if f1 > best_f1:\n",
    "        best_dropout_rate = dropout_rate\n",
    "        best_acc = acc\n",
    "        best_f1 = f1\n",
    "    \n",
    "    print('----------------------------------------------------------------------------')\n",
    "\n",
    "print('Best model:')\n",
    "print(f'dropout_rate: {best_dropout_rate}, accuracy: {best_acc}, macro_f1: {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6607fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = best_dropout_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c1aebf",
   "metadata": {},
   "source": [
    "### batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdf115a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 22\n",
      "batch_norm: True\n",
      "13/13 [==============================] - 4s 17ms/step\n",
      "accuracy: 0.8444\n",
      "macro_f1: 0.8450\n",
      "----------------------------------------------------------------------------\n",
      "count: 23\n",
      "batch_norm: False\n",
      "13/13 [==============================] - 3s 19ms/step\n",
      "accuracy: 0.8593\n",
      "macro_f1: 0.8591\n",
      "----------------------------------------------------------------------------\n",
      "Best model:\n",
      "batch_norm: False, accuracy: 0.8592592592592593, macro_f1: 0.8590514347883086\n"
     ]
    }
   ],
   "source": [
    "batch_norm_list = [True, False]\n",
    "best_batch_norm = False\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for batch_norm in batch_norm_list:\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    model = build_MLP(input_size=len(feats_dict), \n",
    "                      output_size=num_labels,\n",
    "                      num_layers=num_layers,\n",
    "                      hidden_size=hidden_size,\n",
    "                      activation=activation,\n",
    "                      dropout_rate=dropout_rate, \n",
    "                      batch_norm=batch_norm,\n",
    "                      layer_norm=layer_norm, \n",
    "                      l2_reg=l2_reg,\n",
    "                      optimizer=optimizer, \n",
    "                      learning_rate=learning_rate\n",
    "                     )\n",
    "\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"),\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=0,\n",
    "        save_best_only=True)\n",
    "\n",
    "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
    "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "\n",
    "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"))\n",
    "\n",
    "    print(f'count: {count}')\n",
    "    print(f'batch_norm: {batch_norm}')\n",
    "\n",
    "    # evaluation\n",
    "    # generate prediction and format\n",
    "    y_pred = model.predict(valid_feats_matrix)\n",
    "    y_pred = [np.argmax(row) for row in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # evaluate performance\n",
    "    acc = accuracy_score(valid_labels, y_pred)\n",
    "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
    "    print(f'accuracy: {acc:.4f}')\n",
    "    print(f'macro_f1: {f1:.4f}')\n",
    "\n",
    "    # save best model\n",
    "    if f1 > best_f1:\n",
    "        best_batch_norm = batch_norm\n",
    "        best_acc = acc\n",
    "        best_f1 = f1\n",
    "    \n",
    "    print('----------------------------------------------------------------------------')\n",
    "\n",
    "print('Best model:')\n",
    "print(f'batch_norm: {best_batch_norm}, accuracy: {best_acc}, macro_f1: {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "464dbcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_norm = best_batch_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df83f6c",
   "metadata": {},
   "source": [
    "### layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc099fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 24\n",
      "layer_norm: True\n",
      "13/13 [==============================] - 8s 19ms/step\n",
      "accuracy: 0.8469\n",
      "macro_f1: 0.8468\n",
      "----------------------------------------------------------------------------\n",
      "count: 25\n",
      "layer_norm: False\n",
      "13/13 [==============================] - 1s 13ms/step\n",
      "accuracy: 0.8543\n",
      "macro_f1: 0.8546\n",
      "----------------------------------------------------------------------------\n",
      "Best model:\n",
      "layer_norm: False, accuracy: 0.854320987654321, macro_f1: 0.854624655867402\n"
     ]
    }
   ],
   "source": [
    "layer_norm_list = [True, False]\n",
    "best_layer_norm = False\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for layer_norm in layer_norm_list:\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    model = build_MLP(input_size=len(feats_dict), \n",
    "                      output_size=num_labels,\n",
    "                      num_layers=num_layers,\n",
    "                      hidden_size=hidden_size,\n",
    "                      activation=activation,\n",
    "                      dropout_rate=dropout_rate, \n",
    "                      batch_norm=batch_norm,\n",
    "                      layer_norm=layer_norm, \n",
    "                      l2_reg=l2_reg,\n",
    "                      optimizer=optimizer, \n",
    "                      learning_rate=learning_rate\n",
    "                     )\n",
    "\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"),\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=0,\n",
    "        save_best_only=True)\n",
    "\n",
    "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
    "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "\n",
    "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"))\n",
    "\n",
    "    print(f'count: {count}')\n",
    "    print(f'layer_norm: {layer_norm}')\n",
    "\n",
    "    # evaluation\n",
    "    # generate prediction and format\n",
    "    y_pred = model.predict(valid_feats_matrix)\n",
    "    y_pred = [np.argmax(row) for row in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # evaluate performance\n",
    "    acc = accuracy_score(valid_labels, y_pred)\n",
    "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
    "    print(f'accuracy: {acc:.4f}')\n",
    "    print(f'macro_f1: {f1:.4f}')\n",
    "\n",
    "    # save best model\n",
    "    if f1 > best_f1:\n",
    "        best_layer_norm = layer_norm\n",
    "        best_acc = acc\n",
    "        best_f1 = f1\n",
    "    \n",
    "    print('----------------------------------------------------------------------------')\n",
    "\n",
    "print('Best model:')\n",
    "print(f'layer_norm: {best_layer_norm}, accuracy: {best_acc}, macro_f1: {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fefd3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = best_layer_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf8fca",
   "metadata": {},
   "source": [
    "### l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67723f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 26\n",
      "l2_reg: 0.001\n",
      "13/13 [==============================] - 2s 21ms/step\n",
      "accuracy: 0.8568\n",
      "macro_f1: 0.8569\n",
      "----------------------------------------------------------------------------\n",
      "count: 27\n",
      "l2_reg: 0.005\n",
      "13/13 [==============================] - 1s 12ms/step\n",
      "accuracy: 0.8617\n",
      "macro_f1: 0.8617\n",
      "----------------------------------------------------------------------------\n",
      "count: 28\n",
      "l2_reg: 0.01\n",
      "13/13 [==============================] - 3s 16ms/step\n",
      "accuracy: 0.8568\n",
      "macro_f1: 0.8571\n",
      "----------------------------------------------------------------------------\n",
      "count: 29\n",
      "l2_reg: 0.1\n",
      "13/13 [==============================] - 5s 21ms/step\n",
      "accuracy: 0.7481\n",
      "macro_f1: 0.7424\n",
      "----------------------------------------------------------------------------\n",
      "Best model:\n",
      "l2_reg: 0.005, accuracy: 0.8617283950617284, macro_f1: 0.8617415180616969\n"
     ]
    }
   ],
   "source": [
    "l2_reg_list = [0.001, 0.005, 0.01, 0.1]\n",
    "best_l2_reg = 0\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for l2_reg in l2_reg_list:\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    model = build_MLP(input_size=len(feats_dict), \n",
    "                      output_size=num_labels,\n",
    "                      num_layers=num_layers,\n",
    "                      hidden_size=hidden_size,\n",
    "                      activation=activation,\n",
    "                      dropout_rate=dropout_rate, \n",
    "                      batch_norm=batch_norm,\n",
    "                      layer_norm=layer_norm, \n",
    "                      l2_reg=l2_reg,\n",
    "                      optimizer=optimizer, \n",
    "                      learning_rate=learning_rate\n",
    "                     )\n",
    "\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"),\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=0,\n",
    "        save_best_only=True)\n",
    "\n",
    "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
    "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "\n",
    "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"))\n",
    "\n",
    "    print(f'count: {count}')\n",
    "    print(f'l2_reg: {l2_reg}')\n",
    "\n",
    "    # evaluation\n",
    "    # generate prediction and format\n",
    "    y_pred = model.predict(valid_feats_matrix)\n",
    "    y_pred = [np.argmax(row) for row in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # evaluate performance\n",
    "    acc = accuracy_score(valid_labels, y_pred)\n",
    "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
    "    print(f'accuracy: {acc:.4f}')\n",
    "    print(f'macro_f1: {f1:.4f}')\n",
    "\n",
    "    # save best model\n",
    "    if f1 > best_f1:\n",
    "        best_l2_reg = l2_reg\n",
    "        best_acc = acc\n",
    "        best_f1 = f1\n",
    "    \n",
    "    print('----------------------------------------------------------------------------')\n",
    "\n",
    "print('Best model:')\n",
    "print(f'l2_reg: {best_l2_reg}, accuracy: {best_acc}, macro_f1: {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1703dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = best_l2_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe14b568",
   "metadata": {},
   "source": [
    "### learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68b8e46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 30\n",
      "learning_rate: 0.0001\n",
      "13/13 [==============================] - 4s 18ms/step\n",
      "accuracy: 0.8543\n",
      "macro_f1: 0.8535\n",
      "----------------------------------------------------------------------------\n",
      "count: 31\n",
      "learning_rate: 0.001\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "accuracy: 0.8543\n",
      "macro_f1: 0.8542\n",
      "----------------------------------------------------------------------------\n",
      "count: 32\n",
      "learning_rate: 0.01\n",
      "13/13 [==============================] - 3s 17ms/step\n",
      "accuracy: 0.8593\n",
      "macro_f1: 0.8595\n",
      "----------------------------------------------------------------------------\n",
      "count: 33\n",
      "learning_rate: 0.1\n",
      "13/13 [==============================] - 2s 17ms/step\n",
      "accuracy: 0.8543\n",
      "macro_f1: 0.8549\n",
      "----------------------------------------------------------------------------\n",
      "Best model:\n",
      "learning_rate: 0.01, accuracy: 0.8592592592592593, macro_f1: 0.8595305056652044\n"
     ]
    }
   ],
   "source": [
    "learning_rate_list = [0.0001, 0.001, 0.01, 0.1]\n",
    "best_learning_rate = 0\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    model = build_MLP(input_size=len(feats_dict), \n",
    "                      output_size=num_labels,\n",
    "                      num_layers=num_layers,\n",
    "                      hidden_size=hidden_size,\n",
    "                      activation=activation,\n",
    "                      dropout_rate=dropout_rate, \n",
    "                      batch_norm=batch_norm,\n",
    "                      layer_norm=layer_norm, \n",
    "                      l2_reg=l2_reg,\n",
    "                      optimizer=optimizer, \n",
    "                      learning_rate=learning_rate\n",
    "                     )\n",
    "\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"),\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=0,\n",
    "        save_best_only=True)\n",
    "\n",
    "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
    "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "\n",
    "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_MLP_tune{count}.hdf5\"))\n",
    "\n",
    "    print(f'count: {count}')\n",
    "    print(f'learning_rate: {learning_rate}')\n",
    "\n",
    "    # evaluation\n",
    "    # generate prediction and format\n",
    "    y_pred = model.predict(valid_feats_matrix)\n",
    "    y_pred = [np.argmax(row) for row in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # evaluate performance\n",
    "    acc = accuracy_score(valid_labels, y_pred)\n",
    "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
    "    print(f'accuracy: {acc:.4f}')\n",
    "    print(f'macro_f1: {f1:.4f}')\n",
    "\n",
    "    # save best model\n",
    "    if f1 > best_f1:\n",
    "        best_learning_rate = learning_rate\n",
    "        best_acc = acc\n",
    "        best_f1 = f1\n",
    "    \n",
    "    print('----------------------------------------------------------------------------')\n",
    "\n",
    "print('Best model:')\n",
    "print(f'learning_rate: {best_learning_rate}, accuracy: {best_acc}, macro_f1: {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e17b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = best_learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb4df7",
   "metadata": {},
   "source": [
    "## Final Multi-Layer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44fcdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28a64862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 107s 1s/step - loss: 4.6720 - accuracy: 0.1748 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.7637 - val_accuracy: 0.6815 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 3.3253 - accuracy: 0.3811 - precision: 0.9306 - recall: 0.0184 - val_loss: 2.7671 - val_accuracy: 0.7457 - val_precision: 1.0000 - val_recall: 0.0519\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 2.4537 - accuracy: 0.6019 - precision: 0.9171 - recall: 0.1852 - val_loss: 1.9966 - val_accuracy: 0.7951 - val_precision: 0.9742 - val_recall: 0.3728\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 1.8779 - accuracy: 0.7396 - precision: 0.9110 - recall: 0.4549 - val_loss: 1.6621 - val_accuracy: 0.8222 - val_precision: 0.9556 - val_recall: 0.5852\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 1.5528 - accuracy: 0.8170 - precision: 0.9208 - recall: 0.6187 - val_loss: 1.4977 - val_accuracy: 0.8469 - val_precision: 0.9336 - val_recall: 0.6593\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 1.3486 - accuracy: 0.8642 - precision: 0.9414 - recall: 0.7139 - val_loss: 1.3916 - val_accuracy: 0.8444 - val_precision: 0.9256 - val_recall: 0.7062\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 1.2087 - accuracy: 0.8916 - precision: 0.9531 - recall: 0.7646 - val_loss: 1.3297 - val_accuracy: 0.8444 - val_precision: 0.9136 - val_recall: 0.7309\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 4s 114ms/step - loss: 1.1040 - accuracy: 0.9163 - precision: 0.9618 - recall: 0.8019 - val_loss: 1.2799 - val_accuracy: 0.8494 - val_precision: 0.9190 - val_recall: 0.7284\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 1.0273 - accuracy: 0.9188 - precision: 0.9663 - recall: 0.8192 - val_loss: 1.2366 - val_accuracy: 0.8346 - val_precision: 0.9146 - val_recall: 0.7407\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 0.9692 - accuracy: 0.9235 - precision: 0.9691 - recall: 0.8357 - val_loss: 1.2067 - val_accuracy: 0.8321 - val_precision: 0.8982 - val_recall: 0.7407\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 4s 99ms/step - loss: 0.9330 - accuracy: 0.9331 - precision: 0.9691 - recall: 0.8513 - val_loss: 1.1956 - val_accuracy: 0.8346 - val_precision: 0.8938 - val_recall: 0.7481\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 0.8973 - accuracy: 0.9385 - precision: 0.9731 - recall: 0.8529 - val_loss: 1.1676 - val_accuracy: 0.8494 - val_precision: 0.9099 - val_recall: 0.7481\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 4s 118ms/step - loss: 0.8723 - accuracy: 0.9421 - precision: 0.9707 - recall: 0.8628 - val_loss: 1.1754 - val_accuracy: 0.8395 - val_precision: 0.8979 - val_recall: 0.7383\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 4s 97ms/step - loss: 0.8527 - accuracy: 0.9427 - precision: 0.9725 - recall: 0.8642 - val_loss: 1.1405 - val_accuracy: 0.8444 - val_precision: 0.9056 - val_recall: 0.7580\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 4s 100ms/step - loss: 0.8248 - accuracy: 0.9479 - precision: 0.9775 - recall: 0.8809 - val_loss: 1.1336 - val_accuracy: 0.8346 - val_precision: 0.8968 - val_recall: 0.7506\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 4s 115ms/step - loss: 0.8036 - accuracy: 0.9503 - precision: 0.9773 - recall: 0.8853 - val_loss: 1.1162 - val_accuracy: 0.8395 - val_precision: 0.9032 - val_recall: 0.7605\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 4s 116ms/step - loss: 0.7816 - accuracy: 0.9556 - precision: 0.9766 - recall: 0.8919 - val_loss: 1.1319 - val_accuracy: 0.8296 - val_precision: 0.8825 - val_recall: 0.7605\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 4s 102ms/step - loss: 0.7799 - accuracy: 0.9517 - precision: 0.9765 - recall: 0.8881 - val_loss: 1.1235 - val_accuracy: 0.8469 - val_precision: 0.8924 - val_recall: 0.7580\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 3s 90ms/step - loss: 0.7600 - accuracy: 0.9561 - precision: 0.9738 - recall: 0.8982 - val_loss: 1.1144 - val_accuracy: 0.8272 - val_precision: 0.9029 - val_recall: 0.7580\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 0.7498 - accuracy: 0.9583 - precision: 0.9809 - recall: 0.9001 - val_loss: 1.1200 - val_accuracy: 0.8420 - val_precision: 0.8812 - val_recall: 0.7506\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.7377 - accuracy: 0.9610 - precision: 0.9779 - recall: 0.9103 - val_loss: 1.0995 - val_accuracy: 0.8519 - val_precision: 0.8963 - val_recall: 0.7679\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 0.7351 - accuracy: 0.9542 - precision: 0.9772 - recall: 0.9045 - val_loss: 1.0911 - val_accuracy: 0.8346 - val_precision: 0.8980 - val_recall: 0.7605\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.7251 - accuracy: 0.9591 - precision: 0.9795 - recall: 0.9158 - val_loss: 1.0959 - val_accuracy: 0.8321 - val_precision: 0.8983 - val_recall: 0.7630\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.7153 - accuracy: 0.9627 - precision: 0.9821 - recall: 0.9160 - val_loss: 1.1005 - val_accuracy: 0.8321 - val_precision: 0.8905 - val_recall: 0.7630\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 0.7161 - accuracy: 0.9613 - precision: 0.9811 - recall: 0.9136 - val_loss: 1.0935 - val_accuracy: 0.8296 - val_precision: 0.8797 - val_recall: 0.7580\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 0.7128 - accuracy: 0.9578 - precision: 0.9768 - recall: 0.9125 - val_loss: 1.0980 - val_accuracy: 0.8321 - val_precision: 0.8937 - val_recall: 0.7679\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 3s 95ms/step - loss: 0.7052 - accuracy: 0.9591 - precision: 0.9767 - recall: 0.9199 - val_loss: 1.0813 - val_accuracy: 0.8444 - val_precision: 0.8920 - val_recall: 0.7753\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 4s 114ms/step - loss: 0.6823 - accuracy: 0.9671 - precision: 0.9820 - recall: 0.9295 - val_loss: 1.0798 - val_accuracy: 0.8321 - val_precision: 0.8814 - val_recall: 0.7704\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 0.6896 - accuracy: 0.9580 - precision: 0.9786 - recall: 0.9171 - val_loss: 1.0705 - val_accuracy: 0.8272 - val_precision: 0.8861 - val_recall: 0.7877\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 0.6794 - accuracy: 0.9632 - precision: 0.9789 - recall: 0.9281 - val_loss: 1.0896 - val_accuracy: 0.8395 - val_precision: 0.8911 - val_recall: 0.7679\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 3s 73ms/step - loss: 0.6590 - accuracy: 0.9682 - precision: 0.9821 - recall: 0.9309 - val_loss: 1.0820 - val_accuracy: 0.8420 - val_precision: 0.8949 - val_recall: 0.7778\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 0.6630 - accuracy: 0.9687 - precision: 0.9826 - recall: 0.9300 - val_loss: 1.0757 - val_accuracy: 0.8420 - val_precision: 0.8842 - val_recall: 0.7728\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.6549 - accuracy: 0.9663 - precision: 0.9821 - recall: 0.9333 - val_loss: 1.0692 - val_accuracy: 0.8296 - val_precision: 0.8851 - val_recall: 0.7605\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6582 - accuracy: 0.9635 - precision: 0.9800 - recall: 0.9273 - val_loss: 1.0596 - val_accuracy: 0.8370 - val_precision: 0.8905 - val_recall: 0.7630\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.6499 - accuracy: 0.9654 - precision: 0.9807 - recall: 0.9320 - val_loss: 1.0847 - val_accuracy: 0.8198 - val_precision: 0.8860 - val_recall: 0.7679\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 4s 103ms/step - loss: 0.6495 - accuracy: 0.9706 - precision: 0.9839 - recall: 0.9364 - val_loss: 1.0949 - val_accuracy: 0.8247 - val_precision: 0.8796 - val_recall: 0.7753\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 0.6413 - accuracy: 0.9709 - precision: 0.9839 - recall: 0.9407 - val_loss: 1.0847 - val_accuracy: 0.8222 - val_precision: 0.8778 - val_recall: 0.7630\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 4s 114ms/step - loss: 0.6498 - accuracy: 0.9643 - precision: 0.9784 - recall: 0.9303 - val_loss: 1.0579 - val_accuracy: 0.8370 - val_precision: 0.8822 - val_recall: 0.7951\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 4s 104ms/step - loss: 0.6321 - accuracy: 0.9748 - precision: 0.9854 - recall: 0.9429 - val_loss: 1.0675 - val_accuracy: 0.8321 - val_precision: 0.8736 - val_recall: 0.7679\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 4s 112ms/step - loss: 0.6405 - accuracy: 0.9668 - precision: 0.9805 - recall: 0.9374 - val_loss: 1.0430 - val_accuracy: 0.8346 - val_precision: 0.8827 - val_recall: 0.7802\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.6282 - accuracy: 0.9668 - precision: 0.9805 - recall: 0.9391 - val_loss: 1.0435 - val_accuracy: 0.8346 - val_precision: 0.8898 - val_recall: 0.7778\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 4s 109ms/step - loss: 0.6280 - accuracy: 0.9682 - precision: 0.9834 - recall: 0.9405 - val_loss: 1.0665 - val_accuracy: 0.8370 - val_precision: 0.8810 - val_recall: 0.7679\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 0.6173 - accuracy: 0.9665 - precision: 0.9794 - recall: 0.9385 - val_loss: 1.0528 - val_accuracy: 0.8370 - val_precision: 0.8796 - val_recall: 0.7753\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 4s 117ms/step - loss: 0.6116 - accuracy: 0.9717 - precision: 0.9817 - recall: 0.9443 - val_loss: 1.0583 - val_accuracy: 0.8494 - val_precision: 0.8955 - val_recall: 0.7827\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6124 - accuracy: 0.9684 - precision: 0.9825 - recall: 0.9410 - val_loss: 1.0620 - val_accuracy: 0.8321 - val_precision: 0.8750 - val_recall: 0.7778\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.5917 - accuracy: 0.9739 - precision: 0.9852 - recall: 0.9509 - val_loss: 1.0455 - val_accuracy: 0.8420 - val_precision: 0.8771 - val_recall: 0.7753\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 3s 73ms/step - loss: 0.6020 - accuracy: 0.9682 - precision: 0.9828 - recall: 0.9421 - val_loss: 1.0522 - val_accuracy: 0.8272 - val_precision: 0.8814 - val_recall: 0.7704\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 3s 74ms/step - loss: 0.5913 - accuracy: 0.9706 - precision: 0.9813 - recall: 0.9512 - val_loss: 1.0746 - val_accuracy: 0.8247 - val_precision: 0.8876 - val_recall: 0.7802\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 3s 80ms/step - loss: 0.6077 - accuracy: 0.9654 - precision: 0.9799 - recall: 0.9383 - val_loss: 1.0595 - val_accuracy: 0.8296 - val_precision: 0.8895 - val_recall: 0.7753\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 3s 75ms/step - loss: 0.5993 - accuracy: 0.9728 - precision: 0.9846 - recall: 0.9487 - val_loss: 1.0688 - val_accuracy: 0.8247 - val_precision: 0.8768 - val_recall: 0.7556\n",
      "13/13 [==============================] - 2s 16ms/step\n",
      "accuracy: 0.8519\n",
      "macro_f1: 0.8524\n"
     ]
    }
   ],
   "source": [
    "model = build_MLP(input_size=len(feats_dict), \n",
    "                      output_size=num_labels,\n",
    "                      num_layers=num_layers,\n",
    "                      hidden_size=hidden_size,\n",
    "                      activation=activation,\n",
    "                      dropout_rate=dropout_rate, \n",
    "                      batch_norm=batch_norm,\n",
    "                      layer_norm=layer_norm, \n",
    "                      l2_reg=l2_reg,\n",
    "                      optimizer=optimizer, \n",
    "                      learning_rate=learning_rate\n",
    "                     )\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", f\"weights_MLP_final.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                    validation_data=(valid_feats_matrix, valid_label_matrix),\n",
    "                    epochs=epoch, batch_size=batch_size, verbose=1,\n",
    "                    callbacks=[checkpointer])\n",
    "\n",
    "model = keras.models.load_model(os.path.join(\"models\", f\"weights_MLP_final.hdf5\"))\n",
    "\n",
    "\n",
    "# evaluation\n",
    "# generate prediction and format\n",
    "y_pred = model.predict(valid_feats_matrix)\n",
    "y_pred = [np.argmax(row) for row in y_pred]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# evaluate performance\n",
    "acc = accuracy_score(valid_labels, y_pred)\n",
    "f1 = f1_score(valid_labels, y_pred, average='macro')\n",
    "print(f'accuracy: {acc:.4f}')\n",
    "print(f'macro_f1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58719967",
   "metadata": {},
   "source": [
    "## Final Model Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "118c278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(\"models\", f\"weights_MLP_final.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b190565f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 3ms/step\n",
      "test accuracy: 0.8800\n",
      "test macro_f1: 0.8803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.92      0.94      0.93        50\n",
      "          NC       0.94      0.88      0.91        50\n",
      "          PW       0.88      0.86      0.87        50\n",
      "          HC       0.78      0.80      0.79        50\n",
      "          PL       0.82      0.90      0.86        50\n",
      "          CR       0.84      0.82      0.83        50\n",
      "          CG       0.94      0.92      0.93        50\n",
      "          BE       0.90      0.90      0.90        50\n",
      "           N       0.92      0.90      0.91        50\n",
      "\n",
      "    accuracy                           0.88       450\n",
      "   macro avg       0.88      0.88      0.88       450\n",
      "weighted avg       0.88      0.88      0.88       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate prediction and format\n",
    "y_pred = model.predict(test_feats_matrix)\n",
    "y_pred = [np.argmax(row) for row in y_pred]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# evaluate performance\n",
    "acc = accuracy_score(test_labels, y_pred)\n",
    "f1 = f1_score(test_labels, y_pred, average='macro')\n",
    "print(f'test accuracy: {acc:.4f}')\n",
    "print(f'test macro_f1: {f1:.4f}')\n",
    "print(classification_report(test_labels, y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64e53fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1f4873d33d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot confusion matrix\n",
    "cm = confusion_matrix(test_labels, y_pred , normalize='pred')\n",
    "cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cmp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0decea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
