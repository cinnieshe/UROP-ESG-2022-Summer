{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7fb4fc00",
      "metadata": {
        "id": "7fb4fc00"
      },
      "source": [
        "# Fine-Tune LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f59f343b",
      "metadata": {
        "id": "f59f343b"
      },
      "source": [
        "## Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cf684272",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf684272",
        "outputId": "cd55a672-7613-4e75-f297-16ee00ff6da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import os # store and load weight\n",
        "import pandas as pd # load data\n",
        "import nltk # text processing\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer # text processing\n",
        "from nltk.corpus import stopwords #text processing\n",
        "import numpy as np # one-hot vector\n",
        "import matplotlib.pyplot as plt # model analysis\n",
        "from itertools import chain # feature construction\n",
        "from collections import Counter # build feats-dict\n",
        "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import keras\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Input, Add, Concatenate,\\\n",
        "    Bidirectional, SimpleRNN, LSTM, GRU\n",
        "\n",
        "\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bb6f96",
      "metadata": {
        "id": "75bb6f96"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fa2022f1",
      "metadata": {
        "id": "fa2022f1"
      },
      "outputs": [],
      "source": [
        "def load_data(filename):\n",
        "    \"\"\"\n",
        "    Input: string filename\n",
        "    Output: a pandas dataframe for the whole dataset after droping missing values\n",
        "    Support google colab or local environments\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # local environment\n",
        "        df = pd.read_csv(filename)\n",
        "        df = df.dropna(subset=['sentence', 'label']) ## drop missing values\n",
        "        return df\n",
        "    except:\n",
        "        # google colab environment\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        \n",
        "        df = pd.read_csv('/content/drive/MyDrive/' + filename)\n",
        "        df = df.dropna(subset=['sentence', 'label']) ## drop missing values\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ae420543",
      "metadata": {
        "id": "ae420543"
      },
      "outputs": [],
      "source": [
        "def split_data(df):\n",
        "    \"\"\"\n",
        "    Input: pandas dataframe\n",
        "    Output: training dataframe (81%), validation dataframe (9%), test dataframe (10%)\n",
        "    \"\"\"\n",
        "    df_train, df_val = train_test_split(df, stratify=df['label'],test_size=0.1, random_state=42)\n",
        "    \n",
        "    return df_train, df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a0728ddb",
      "metadata": {
        "id": "a0728ddb"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    :param text: a doc with multiple sentences, type: str\n",
        "    return a word list, type: list\n",
        "    e.g.\n",
        "    Input: 'Text mining is to identify useful information.'\n",
        "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "def stem(tokens):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    return a list of stemmed words, type: list\n",
        "    e.g.\n",
        "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
        "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
        "    \"\"\"\n",
        "\n",
        "    return [ps.stem(token) for token in tokens]\n",
        "\n",
        "def n_gram(tokens, n=1):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    :param n: the corresponding n-gram, type: int\n",
        "    return a list of n-gram tokens, type: list\n",
        "    e.g.\n",
        "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
        "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
        "    \"\"\"\n",
        "    if n == 1:\n",
        "        return tokens\n",
        "    else:\n",
        "        results = list()\n",
        "        for i in range(len(tokens)-n+1):\n",
        "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
        "            results.append(\" \".join(tokens[i:i+n]))\n",
        "        return results\n",
        "    \n",
        "def filter_stopwords(tokens):\n",
        "    \"\"\"\n",
        "    :param tokens: a list of tokens, type: list\n",
        "    return a list of filtered tokens, type: list\n",
        "    e.g.\n",
        "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
        "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
        "    \"\"\"\n",
        "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
        "\n",
        "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
        "    \"\"\"\n",
        "    :param data: a list of features, type: list(list)\n",
        "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n",
        "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n",
        "    :param max_size: the max size of feature dict, type: int\n",
        "    return a feature dict that maps features to indices, sorted by frequencies\n",
        "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
        "    \"\"\"\n",
        "    # count all features\n",
        "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
        "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
        "        valid_feats = [f for f, cnt in feat_cnt.most_common(max_size)]\n",
        "    else:\n",
        "        valid_feats = list()\n",
        "        for f, cnt in feat_cnt.most_common():\n",
        "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
        "                (max_freq == -1 or cnt <= max_freq):\n",
        "                valid_feats.append(f)\n",
        "    if max_size > 0 and len(valid_feats) > max_size:\n",
        "        valid_feats = valid_feats[:max_size]        \n",
        "    print(\"Size of features:\", len(valid_feats))\n",
        "    \n",
        "    # build a mapping from features to indices\n",
        "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
        "    return feats_dict\n",
        "\n",
        "def get_onehot_vector(feats, feats_dict):\n",
        "    \"\"\"\n",
        "    :param feats: a list of features, type: list\n",
        "    :param feats_dict: a dict from features to indices, type: dict\n",
        "    return a feature vector,\n",
        "    \"\"\"\n",
        "    # initialize the vector as all zeros\n",
        "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
        "    for f in feats:\n",
        "        # get the feature index, return -1 if the feature is not existed\n",
        "        f_idx = feats_dict.get(f, -1)\n",
        "        if f_idx != -1:\n",
        "            # set the corresponding element as 1\n",
        "            vector[f_idx] = 1\n",
        "    return vector\n",
        "\n",
        "# Get index vector\n",
        "def get_index_vector(feats, feats_dict, max_len):\n",
        "    \"\"\"\n",
        "    :param feats: a list of features, type: list\n",
        "    :param feats_dict: a dict from features to indices, type: dict\n",
        "    :param feats: a list of features, type: list\n",
        "    return a feature vector,\n",
        "    \"\"\"\n",
        "    # initialize the vector as all zeros\n",
        "    vector = np.zeros(max_len, dtype=np.int64)\n",
        "    for i, f in enumerate(feats):\n",
        "        if i == max_len:\n",
        "            break\n",
        "        # get the feature index, return 1 (<unk>) if the feature is not existed\n",
        "        try:\n",
        "            vector[i] = feats_dict[f]\n",
        "        except KeyError:\n",
        "            vector[i] = 1\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f809cb83",
      "metadata": {
        "id": "f809cb83"
      },
      "outputs": [],
      "source": [
        "def build_RNN(input_length, vocab_size, embedding_size,\n",
        "              hidden_size, output_size,\n",
        "              num_rnn_layers, num_mlp_layers,\n",
        "              rnn_type=\"lstm\",\n",
        "              bidirectional=False,\n",
        "              embedding_matrix=None,\n",
        "              activation=\"tanh\",\n",
        "              dropout_rate=0.0,\n",
        "              batch_norm=False,\n",
        "              l2_reg=0.0,\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              optimizer=\"Adam\",\n",
        "              learning_rate=0.001,\n",
        "              metric=\"accuracy\"):\n",
        "\n",
        "    x = Input(shape=(input_length,))\n",
        "    \n",
        "    ################################\n",
        "    ###### Word Representation #####\n",
        "    ################################\n",
        "    # word representation layer\n",
        "    if embedding_matrix is not None:\n",
        "        emb = Embedding(input_dim=vocab_size,\n",
        "                        output_dim=embedding_size,\n",
        "                        input_length=input_length,\n",
        "                        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                        trainable=False)(x)\n",
        "    else:\n",
        "        emb = Embedding(input_dim=vocab_size,\n",
        "                        output_dim=embedding_size,\n",
        "                        input_length=input_length,\n",
        "                        embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
        "    \n",
        "    ################################\n",
        "    ####### Recurrent Layers #######\n",
        "    ################################\n",
        "    # recurrent layers\n",
        "    if rnn_type == \"rnn\":\n",
        "        fn = SimpleRNN\n",
        "    elif rnn_type == \"lstm\":\n",
        "        fn = LSTM\n",
        "    elif rnn_type == \"gru\":\n",
        "        fn = GRU\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    h = emb\n",
        "    for i in range(num_rnn_layers):\n",
        "        is_last = (i == num_rnn_layers-1)\n",
        "        if bidirectional:\n",
        "            h = Bidirectional(fn(hidden_size,\n",
        "                                 kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
        "                                 recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
        "                                 return_sequences=not is_last))(h)\n",
        "        else:\n",
        "            h = fn(hidden_size,\n",
        "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
        "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
        "                   return_sequences=not is_last)(h)\n",
        "        h = Dropout(dropout_rate, seed=0)(h)\n",
        "    \n",
        "    ################################\n",
        "    #### Fully Connected Layers ####\n",
        "    ################################\n",
        "    # multi-layer perceptron\n",
        "    for i in range(num_mlp_layers-1):\n",
        "        new_h = Dense(hidden_size,\n",
        "                      kernel_initializer=keras.initializers.he_normal(seed=0),\n",
        "                      bias_initializer=\"zeros\",\n",
        "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
        "        # add batch normalization layer\n",
        "        if batch_norm:\n",
        "            new_h = BatchNormalization()(new_h)\n",
        "        # add residual connection\n",
        "        if i == 0:\n",
        "            h = new_h\n",
        "        else:\n",
        "            h = Add()([h, new_h])\n",
        "        # add activation\n",
        "        h = Activation(activation)(h)\n",
        "    y = Dense(output_size,\n",
        "              activation=\"softmax\",\n",
        "              kernel_initializer=keras.initializers.he_normal(seed=0),\n",
        "              bias_initializer=\"zeros\")(h)\n",
        "    \n",
        "    # set the loss, the optimizer, and the metric\n",
        "    if optimizer == \"SGD\":\n",
        "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    elif optimizer == \"RMSprop\":\n",
        "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "    elif optimizer == \"Adam\":\n",
        "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    model = Model(x, y)\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649b6c07",
      "metadata": {
        "id": "649b6c07"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fbaaf200",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbaaf200",
        "outputId": "15372abe-5ba3-494e-86ca-d2e4f30d96a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "FILENAME = 'final_dataset_formatted.csv'\n",
        "TEST_FILENAME = 'final_dataset_formatted_test.csv'\n",
        "\n",
        "# load data\n",
        "df = load_data(FILENAME)\n",
        "df_test = load_data(TEST_FILENAME)\n",
        "\n",
        "# labels\n",
        "labels = ['CC', 'NC', 'PW', 'HC', 'PL', 'CR', 'CG', 'BE', 'N']\n",
        "num_labels = 9\n",
        "\n",
        "# split data\n",
        "df_train, df_val = split_data(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22eb3bbb",
      "metadata": {
        "id": "22eb3bbb"
      },
      "source": [
        "## Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "666f5e7c",
      "metadata": {
        "id": "666f5e7c"
      },
      "outputs": [],
      "source": [
        "# split text and labels\n",
        "train_texts = df_train.iloc[:, 0]\n",
        "train_labels = df_train.iloc[:, 1]\n",
        "valid_texts = df_val.iloc[:, 0]\n",
        "valid_labels = df_val.iloc[:, 1]\n",
        "test_texts = df_test.iloc[:, 0]\n",
        "test_labels = df_test.iloc[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "15ceb2f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15ceb2f6",
        "outputId": "557463ff-1937-4674-ec63-18d7a7051864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train size: 3645\n",
            "valid size: 405\n",
            "test size: 450\n"
          ]
        }
      ],
      "source": [
        "# get train, validation, and test dataset size\n",
        "train_size = len(train_texts)\n",
        "valid_size = len(valid_texts)\n",
        "test_size = len(test_texts)\n",
        "\n",
        "print(f'train size: {train_size}')\n",
        "print(f'valid size: {valid_size}')\n",
        "print(f'test size: {test_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "df2ff723",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df2ff723",
        "outputId": "427a81f6-caf2-4801-9a11-099543a8336e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of features: 2465\n"
          ]
        }
      ],
      "source": [
        "# extract features\n",
        "min_freq = 3\n",
        "\n",
        "train_tokens = [tokenize(text) for text in train_texts]\n",
        "valid_tokens = [tokenize(text) for text in valid_texts]\n",
        "test_tokens = [tokenize(text) for text in test_texts]\n",
        "\n",
        "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
        "valid_stemmed = [stem(tokens) for tokens in valid_tokens]\n",
        "test_stemmed = [stem(tokens) for tokens in test_tokens]\n",
        "\n",
        "train_feats = [filter_stopwords(tokens) for tokens in train_stemmed]\n",
        "valid_feats = [filter_stopwords(tokens) for tokens in valid_stemmed]\n",
        "test_feats = [filter_stopwords(tokens) for tokens in test_stemmed]\n",
        "\n",
        "# build a mapping from features to indices\n",
        "feats_dict = get_feats_dict(\n",
        "    chain.from_iterable(train_feats),\n",
        "    min_freq=min_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2f3bb94a",
      "metadata": {
        "id": "2f3bb94a"
      },
      "outputs": [],
      "source": [
        "max_len = 75 # from EDA\n",
        "\n",
        "# build the feats_matrix\n",
        "# convert each example to a index vector, and then stack vectors as a matrix\n",
        "train_feats_matrix = np.vstack(\n",
        "    [get_index_vector(f, feats_dict, max_len) for f in train_feats])\n",
        "valid_feats_matrix = np.vstack(\n",
        "    [get_index_vector(f, feats_dict, max_len) for f in valid_feats])\n",
        "test_feats_matrix = np.vstack(\n",
        "    [get_index_vector(f, feats_dict, max_len) for f in test_feats])\n",
        "\n",
        "# convert each label to a ont-hot vector, and then stack vectors as a matrix\n",
        "train_label_matrix = keras.utils.to_categorical(train_labels, num_classes=num_labels)\n",
        "valid_label_matrix = keras.utils.to_categorical(valid_labels, num_classes=num_labels)\n",
        "test_label_matrix = tf.keras.utils.to_categorical(test_labels, num_classes=num_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a9d1a8",
      "metadata": {
        "id": "39a9d1a8"
      },
      "source": [
        "## Fine-Tune Model  \n",
        "    \n",
        "embedding_size  \n",
        "hidden_size  \n",
        "num_rnn_layers  \n",
        "num_mlp_layers  \n",
        "activation  \n",
        "dropout_rate  \n",
        "batch_norm  \n",
        "l2_reg  \n",
        "optimizer  \n",
        "learning_rate  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "be60294e",
      "metadata": {
        "id": "be60294e"
      },
      "outputs": [],
      "source": [
        "# set seed\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5f6be489",
      "metadata": {
        "id": "5f6be489"
      },
      "outputs": [],
      "source": [
        "# indicator for saving models' weights\n",
        "count = 0\n",
        "\n",
        "# lstm\n",
        "rnn_type=\"lstm\"\n",
        "bidirectional=False\n",
        "\n",
        "# initial settings\n",
        "epoch = 50\n",
        "batch_size = 100\n",
        "\n",
        "embedding_size=100 #256\n",
        "hidden_size=100 #256\n",
        "num_rnn_layers=1\n",
        "num_mlp_layers=2\n",
        "activation='tanh'\n",
        "dropout_rate=0.5\n",
        "batch_norm=True\n",
        "l2_reg=0.005\n",
        "optimizer=\"Adam\"\n",
        "learning_rate=0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d253c6d",
      "metadata": {
        "id": "0d253c6d"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d8d7ccfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8d7ccfb",
        "outputId": "f3b589c9-4320-477b-fcdf-9f8fa179b399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "37/37 [==============================] - 9s 31ms/step - loss: 3.3551 - accuracy: 0.1150 - precision: 0.0714 - recall: 5.4870e-04 - val_loss: 3.1295 - val_accuracy: 0.1086 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 3.1731 - accuracy: 0.1155 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.0730 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 3.0967 - accuracy: 0.1095 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.0133 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.0210 - accuracy: 0.1106 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.9590 - val_accuracy: 0.1086 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.9636 - accuracy: 0.1111 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.9024 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.9107 - accuracy: 0.1141 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.8555 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.8607 - accuracy: 0.1089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.8013 - val_accuracy: 0.1160 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.8134 - accuracy: 0.1128 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.7666 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.7763 - accuracy: 0.1081 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.7274 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.7330 - accuracy: 0.1125 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.6898 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.6918 - accuracy: 0.1160 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.6622 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.6670 - accuracy: 0.1040 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.6219 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.6306 - accuracy: 0.1128 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.5993 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 14/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.6026 - accuracy: 0.1240 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.5621 - val_accuracy: 0.1185 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.5773 - accuracy: 0.1012 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.5430 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 16/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.5479 - accuracy: 0.1136 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.5169 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 17/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.5316 - accuracy: 0.1064 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.4961 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 18/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.5076 - accuracy: 0.1034 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.4804 - val_accuracy: 0.1160 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 19/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.4904 - accuracy: 0.1048 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.4545 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 20/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.4695 - accuracy: 0.1070 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.4445 - val_accuracy: 0.1086 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 21/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.4502 - accuracy: 0.1064 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.4273 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 22/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.4313 - accuracy: 0.1174 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.4124 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 23/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.4210 - accuracy: 0.1097 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3949 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 24/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.4073 - accuracy: 0.1032 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3822 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 25/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.3956 - accuracy: 0.1100 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3698 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 26/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.3780 - accuracy: 0.1196 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3594 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 27/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.3729 - accuracy: 0.1040 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3457 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 28/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.3637 - accuracy: 0.1111 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3427 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 29/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.3463 - accuracy: 0.1130 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3254 - val_accuracy: 0.1160 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 30/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.3434 - accuracy: 0.1152 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3194 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 31/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.3284 - accuracy: 0.1199 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3170 - val_accuracy: 0.1160 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 32/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3224 - accuracy: 0.1171 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3050 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 33/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3153 - accuracy: 0.1210 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2963 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 34/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.3100 - accuracy: 0.1180 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2870 - val_accuracy: 0.1160 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 35/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.3065 - accuracy: 0.1086 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2966 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 36/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.2945 - accuracy: 0.1224 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2782 - val_accuracy: 0.1160 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 37/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.2931 - accuracy: 0.1141 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2737 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 38/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.2894 - accuracy: 0.1018 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2703 - val_accuracy: 0.1160 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 39/50\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.2798 - accuracy: 0.1141 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2633 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 40/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.2816 - accuracy: 0.1051 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2661 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 41/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.2724 - accuracy: 0.1078 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2588 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 42/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2685 - accuracy: 0.1166 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2563 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 43/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2632 - accuracy: 0.1139 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2487 - val_accuracy: 0.1136 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 44/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2618 - accuracy: 0.1095 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2465 - val_accuracy: 0.1086 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 45/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.2590 - accuracy: 0.1119 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2445 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 46/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2574 - accuracy: 0.1141 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2437 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 47/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.2537 - accuracy: 0.0955 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2372 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 48/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.2472 - accuracy: 0.1163 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2373 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 49/50\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.2461 - accuracy: 0.1032 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2386 - val_accuracy: 0.1111 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 50/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2450 - accuracy: 0.1045 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.2349 - val_accuracy: 0.1160 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "count: 1\n",
            "optimizer: SGD\n",
            "accuracy: 0.1185\n",
            "macro_f1: 0.0367\n",
            "----------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "37/37 [==============================] - 4s 31ms/step - loss: 3.2458 - accuracy: 0.1273 - precision_1: 0.0500 - recall_1: 2.7435e-04 - val_loss: 2.9932 - val_accuracy: 0.1111 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 2/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.9738 - accuracy: 0.1045 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.8208 - val_accuracy: 0.1111 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 3/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.7787 - accuracy: 0.1053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.6769 - val_accuracy: 0.1111 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 4/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.6435 - accuracy: 0.1089 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.5518 - val_accuracy: 0.1160 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 5/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.5275 - accuracy: 0.1150 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.4647 - val_accuracy: 0.1136 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 6/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.4506 - accuracy: 0.1097 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.3976 - val_accuracy: 0.1136 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 7/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3975 - accuracy: 0.1177 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.3568 - val_accuracy: 0.1136 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 8/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3598 - accuracy: 0.1070 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.3254 - val_accuracy: 0.1136 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 9/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.3303 - accuracy: 0.1073 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.2985 - val_accuracy: 0.1160 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 10/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.3082 - accuracy: 0.1130 - precision_1: 0.4444 - recall_1: 0.0011 - val_loss: 2.2781 - val_accuracy: 0.1358 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 11/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.2928 - accuracy: 0.1314 - precision_1: 0.5714 - recall_1: 0.0011 - val_loss: 2.4540 - val_accuracy: 0.1383 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 12/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.1707 - accuracy: 0.1742 - precision_1: 0.4272 - recall_1: 0.0121 - val_loss: 2.1698 - val_accuracy: 0.2074 - val_precision_1: 0.3034 - val_recall_1: 0.1086\n",
            "Epoch 13/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0835 - accuracy: 0.1929 - precision_1: 0.3077 - recall_1: 0.0011 - val_loss: 2.0704 - val_accuracy: 0.2148 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 14/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0790 - accuracy: 0.1937 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.1665 - val_accuracy: 0.1926 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 15/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0904 - accuracy: 0.1920 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.0956 - val_accuracy: 0.1704 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 16/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0908 - accuracy: 0.1808 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.0892 - val_accuracy: 0.2000 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 17/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0448 - accuracy: 0.2044 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.0704 - val_accuracy: 0.1926 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 18/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.9996 - accuracy: 0.1953 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.0775 - val_accuracy: 0.2099 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 19/50\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 1.9641 - accuracy: 0.2005 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.1251 - val_accuracy: 0.2173 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 20/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.9215 - accuracy: 0.2107 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 2.0123 - val_accuracy: 0.1975 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 21/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.8771 - accuracy: 0.2102 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 1.9752 - val_accuracy: 0.1877 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 22/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.8457 - accuracy: 0.2069 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 1.9902 - val_accuracy: 0.2099 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 23/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 1.8227 - accuracy: 0.2296 - precision_1: 0.1818 - recall_1: 5.4870e-04 - val_loss: 1.9238 - val_accuracy: 0.2370 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 24/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 1.7350 - accuracy: 0.2647 - precision_1: 0.3750 - recall_1: 0.0016 - val_loss: 2.2435 - val_accuracy: 0.2321 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 25/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.6346 - accuracy: 0.3015 - precision_1: 0.4821 - recall_1: 0.0074 - val_loss: 2.3268 - val_accuracy: 0.2222 - val_precision_1: 0.2683 - val_recall_1: 0.1086\n",
            "Epoch 26/50\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 1.5584 - accuracy: 0.3235 - precision_1: 0.5253 - recall_1: 0.0285 - val_loss: 1.7851 - val_accuracy: 0.3012 - val_precision_1: 0.4175 - val_recall_1: 0.1062\n",
            "Epoch 27/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 1.4989 - accuracy: 0.3396 - precision_1: 0.5319 - recall_1: 0.0206 - val_loss: 1.6895 - val_accuracy: 0.3210 - val_precision_1: 0.4831 - val_recall_1: 0.1062\n",
            "Epoch 28/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 1.4509 - accuracy: 0.3512 - precision_1: 0.5872 - recall_1: 0.0453 - val_loss: 1.7905 - val_accuracy: 0.3062 - val_precision_1: 0.8636 - val_recall_1: 0.0469\n",
            "Epoch 29/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.3583 - accuracy: 0.3835 - precision_1: 0.5779 - recall_1: 0.0631 - val_loss: 1.7776 - val_accuracy: 0.3259 - val_precision_1: 0.8387 - val_recall_1: 0.0642\n",
            "Epoch 30/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 1.3453 - accuracy: 0.3907 - precision_1: 0.5830 - recall_1: 0.0790 - val_loss: 1.6028 - val_accuracy: 0.3333 - val_precision_1: 0.7000 - val_recall_1: 0.0864\n",
            "Epoch 31/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.2891 - accuracy: 0.4217 - precision_1: 0.5653 - recall_1: 0.0974 - val_loss: 1.5731 - val_accuracy: 0.3951 - val_precision_1: 0.6721 - val_recall_1: 0.1012\n",
            "Epoch 32/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 1.2572 - accuracy: 0.4365 - precision_1: 0.5682 - recall_1: 0.1188 - val_loss: 1.6165 - val_accuracy: 0.3556 - val_precision_1: 0.5161 - val_recall_1: 0.1580\n",
            "Epoch 33/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.2289 - accuracy: 0.4615 - precision_1: 0.6237 - recall_1: 0.1937 - val_loss: 1.5777 - val_accuracy: 0.4173 - val_precision_1: 0.5098 - val_recall_1: 0.1926\n",
            "Epoch 34/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.1715 - accuracy: 0.5026 - precision_1: 0.6141 - recall_1: 0.2554 - val_loss: 1.8312 - val_accuracy: 0.3630 - val_precision_1: 0.4110 - val_recall_1: 0.1654\n",
            "Epoch 35/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.1480 - accuracy: 0.5032 - precision_1: 0.5931 - recall_1: 0.2595 - val_loss: 1.8263 - val_accuracy: 0.3259 - val_precision_1: 0.4096 - val_recall_1: 0.1679\n",
            "Epoch 36/50\n",
            "37/37 [==============================] - 1s 19ms/step - loss: 1.0735 - accuracy: 0.5462 - precision_1: 0.6255 - recall_1: 0.3322 - val_loss: 1.9154 - val_accuracy: 0.3185 - val_precision_1: 0.3654 - val_recall_1: 0.2815\n",
            "Epoch 37/50\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 1.0299 - accuracy: 0.5632 - precision_1: 0.6298 - recall_1: 0.3888 - val_loss: 1.6004 - val_accuracy: 0.4370 - val_precision_1: 0.4765 - val_recall_1: 0.3259\n",
            "Epoch 38/50\n",
            "37/37 [==============================] - 1s 19ms/step - loss: 0.9995 - accuracy: 0.5772 - precision_1: 0.6565 - recall_1: 0.4200 - val_loss: 1.7014 - val_accuracy: 0.4272 - val_precision_1: 0.4601 - val_recall_1: 0.3556\n",
            "Epoch 39/50\n",
            "37/37 [==============================] - 1s 21ms/step - loss: 0.9568 - accuracy: 0.6060 - precision_1: 0.6656 - recall_1: 0.4620 - val_loss: 1.9459 - val_accuracy: 0.4617 - val_precision_1: 0.4720 - val_recall_1: 0.2914\n",
            "Epoch 40/50\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.9304 - accuracy: 0.6132 - precision_1: 0.6490 - recall_1: 0.5073 - val_loss: 1.6538 - val_accuracy: 0.4765 - val_precision_1: 0.5028 - val_recall_1: 0.4444\n",
            "Epoch 41/50\n",
            "37/37 [==============================] - 1s 16ms/step - loss: 0.8703 - accuracy: 0.6494 - precision_1: 0.6866 - recall_1: 0.5775 - val_loss: 1.5482 - val_accuracy: 0.5481 - val_precision_1: 0.5955 - val_recall_1: 0.4543\n",
            "Epoch 42/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.8410 - accuracy: 0.6442 - precision_1: 0.6842 - recall_1: 0.5569 - val_loss: 1.7810 - val_accuracy: 0.4321 - val_precision_1: 0.4468 - val_recall_1: 0.4148\n",
            "Epoch 43/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 0.8030 - accuracy: 0.6606 - precision_1: 0.7008 - recall_1: 0.5739 - val_loss: 1.6201 - val_accuracy: 0.5580 - val_precision_1: 0.5736 - val_recall_1: 0.5481\n",
            "Epoch 44/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.7566 - accuracy: 0.6774 - precision_1: 0.7218 - recall_1: 0.6102 - val_loss: 1.8641 - val_accuracy: 0.4988 - val_precision_1: 0.5446 - val_recall_1: 0.4074\n",
            "Epoch 45/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.7710 - accuracy: 0.6754 - precision_1: 0.7363 - recall_1: 0.5951 - val_loss: 1.6550 - val_accuracy: 0.5185 - val_precision_1: 0.5395 - val_recall_1: 0.5062\n",
            "Epoch 46/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 0.7222 - accuracy: 0.6963 - precision_1: 0.7391 - recall_1: 0.6250 - val_loss: 1.8509 - val_accuracy: 0.5012 - val_precision_1: 0.5051 - val_recall_1: 0.4864\n",
            "Epoch 47/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 0.7609 - accuracy: 0.6818 - precision_1: 0.7363 - recall_1: 0.5929 - val_loss: 1.6075 - val_accuracy: 0.5704 - val_precision_1: 0.6588 - val_recall_1: 0.4815\n",
            "Epoch 48/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 0.6652 - accuracy: 0.7325 - precision_1: 0.8033 - recall_1: 0.6554 - val_loss: 1.6465 - val_accuracy: 0.5852 - val_precision_1: 0.5885 - val_recall_1: 0.5827\n",
            "Epoch 49/50\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 0.6419 - accuracy: 0.7690 - precision_1: 0.8031 - recall_1: 0.7141 - val_loss: 1.4275 - val_accuracy: 0.6198 - val_precision_1: 0.6275 - val_recall_1: 0.6198\n",
            "Epoch 50/50\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 0.5831 - accuracy: 0.7995 - precision_1: 0.8332 - recall_1: 0.7580 - val_loss: 1.4166 - val_accuracy: 0.6420 - val_precision_1: 0.6425 - val_recall_1: 0.6346\n",
            "count: 2\n",
            "optimizer: RMSprop\n",
            "accuracy: 0.6420\n",
            "macro_f1: 0.6150\n",
            "----------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "37/37 [==============================] - 3s 33ms/step - loss: 3.3056 - accuracy: 0.1114 - precision_2: 0.1429 - recall_2: 0.0014 - val_loss: 3.0417 - val_accuracy: 0.1111 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 2/50\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 3.0253 - accuracy: 0.1092 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.8984 - val_accuracy: 0.1136 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 3/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.8640 - accuracy: 0.1073 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.7753 - val_accuracy: 0.1136 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 4/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.7470 - accuracy: 0.1125 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.6663 - val_accuracy: 0.1086 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 5/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.6465 - accuracy: 0.1095 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.5798 - val_accuracy: 0.1111 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 6/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.5637 - accuracy: 0.1078 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.5099 - val_accuracy: 0.1086 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 7/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.4969 - accuracy: 0.1125 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.4534 - val_accuracy: 0.1136 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 8/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.4438 - accuracy: 0.1128 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.4067 - val_accuracy: 0.1086 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 9/50\n",
            "37/37 [==============================] - 0s 14ms/step - loss: 2.4028 - accuracy: 0.1081 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.3710 - val_accuracy: 0.1111 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 10/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.3703 - accuracy: 0.1108 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.3426 - val_accuracy: 0.1111 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 11/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3399 - accuracy: 0.1108 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.3196 - val_accuracy: 0.1136 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 12/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3254 - accuracy: 0.1133 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.3055 - val_accuracy: 0.1136 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 13/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3063 - accuracy: 0.1147 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2884 - val_accuracy: 0.1136 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 14/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.2906 - accuracy: 0.1169 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2791 - val_accuracy: 0.1111 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 15/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.2828 - accuracy: 0.1160 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2698 - val_accuracy: 0.1160 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 16/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2786 - accuracy: 0.1075 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2614 - val_accuracy: 0.1160 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 17/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2680 - accuracy: 0.1207 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2579 - val_accuracy: 0.1136 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 18/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2644 - accuracy: 0.1103 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2507 - val_accuracy: 0.1136 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 19/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2568 - accuracy: 0.1075 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2490 - val_accuracy: 0.1136 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 20/50\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 2.2578 - accuracy: 0.1182 - precision_2: 0.3421 - recall_2: 0.0036 - val_loss: 2.2595 - val_accuracy: 0.1012 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 21/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.2565 - accuracy: 0.1092 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2652 - val_accuracy: 0.1062 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 22/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.2520 - accuracy: 0.1139 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2501 - val_accuracy: 0.1062 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 23/50\n",
            "37/37 [==============================] - 1s 13ms/step - loss: 2.2494 - accuracy: 0.1073 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2412 - val_accuracy: 0.1136 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 24/50\n",
            "37/37 [==============================] - 1s 18ms/step - loss: 2.2469 - accuracy: 0.1193 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2353 - val_accuracy: 0.1160 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 25/50\n",
            "37/37 [==============================] - 1s 19ms/step - loss: 2.2394 - accuracy: 0.1155 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 2.2367 - val_accuracy: 0.1160 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 26/50\n",
            "37/37 [==============================] - 1s 19ms/step - loss: 2.2347 - accuracy: 0.1092 - precision_2: 0.3333 - recall_2: 0.0011 - val_loss: 2.0437 - val_accuracy: 0.2123 - val_precision_2: 0.5893 - val_recall_2: 0.0815\n",
            "Epoch 27/50\n",
            "37/37 [==============================] - 1s 18ms/step - loss: 2.1469 - accuracy: 0.1835 - precision_2: 0.3694 - recall_2: 0.0159 - val_loss: 2.0605 - val_accuracy: 0.2074 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 28/50\n",
            "37/37 [==============================] - 1s 19ms/step - loss: 2.0960 - accuracy: 0.1923 - precision_2: 0.6127 - recall_2: 0.0634 - val_loss: 2.0232 - val_accuracy: 0.2049 - val_precision_2: 0.5125 - val_recall_2: 0.1012\n",
            "Epoch 29/50\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 2.0645 - accuracy: 0.2011 - precision_2: 0.4463 - recall_2: 0.0296 - val_loss: 2.0298 - val_accuracy: 0.2148 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 30/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0900 - accuracy: 0.1805 - precision_2: 0.4438 - recall_2: 0.0206 - val_loss: 2.0471 - val_accuracy: 0.2025 - val_precision_2: 0.5781 - val_recall_2: 0.0914\n",
            "Epoch 31/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0399 - accuracy: 0.2071 - precision_2: 0.4938 - recall_2: 0.0436 - val_loss: 2.0141 - val_accuracy: 0.2099 - val_precision_2: 0.5405 - val_recall_2: 0.0988\n",
            "Epoch 32/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0294 - accuracy: 0.2038 - precision_2: 0.5319 - recall_2: 0.0527 - val_loss: 2.0243 - val_accuracy: 0.2148 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 33/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0222 - accuracy: 0.1937 - precision_2: 0.4407 - recall_2: 0.0326 - val_loss: 2.0197 - val_accuracy: 0.2099 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 34/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0203 - accuracy: 0.2030 - precision_2: 0.4478 - recall_2: 0.0247 - val_loss: 2.0028 - val_accuracy: 0.2074 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 35/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0142 - accuracy: 0.2030 - precision_2: 0.4830 - recall_2: 0.0390 - val_loss: 1.9815 - val_accuracy: 0.2148 - val_precision_2: 0.5375 - val_recall_2: 0.1062\n",
            "Epoch 36/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0136 - accuracy: 0.1995 - precision_2: 0.4919 - recall_2: 0.0502 - val_loss: 1.9850 - val_accuracy: 0.2123 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 37/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0119 - accuracy: 0.2060 - precision_2: 0.4164 - recall_2: 0.0335 - val_loss: 1.9891 - val_accuracy: 0.2148 - val_precision_2: 1.0000 - val_recall_2: 0.0025\n",
            "Epoch 38/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0100 - accuracy: 0.2008 - precision_2: 0.5050 - recall_2: 0.0417 - val_loss: 1.9795 - val_accuracy: 0.2198 - val_precision_2: 0.5375 - val_recall_2: 0.1062\n",
            "Epoch 39/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0103 - accuracy: 0.2025 - precision_2: 0.4641 - recall_2: 0.0302 - val_loss: 1.9873 - val_accuracy: 0.2123 - val_precision_2: 0.5309 - val_recall_2: 0.1062\n",
            "Epoch 40/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0087 - accuracy: 0.1986 - precision_2: 0.4620 - recall_2: 0.0450 - val_loss: 1.9963 - val_accuracy: 0.2173 - val_precision_2: 1.0000 - val_recall_2: 0.0025\n",
            "Epoch 41/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0060 - accuracy: 0.1964 - precision_2: 0.5173 - recall_2: 0.0532 - val_loss: 2.0067 - val_accuracy: 0.2173 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 42/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.0047 - accuracy: 0.2016 - precision_2: 0.4384 - recall_2: 0.0244 - val_loss: 2.0097 - val_accuracy: 0.2049 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 43/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0065 - accuracy: 0.2000 - precision_2: 0.4397 - recall_2: 0.0370 - val_loss: 1.9984 - val_accuracy: 0.2148 - val_precision_2: 1.0000 - val_recall_2: 0.0025\n",
            "Epoch 44/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0049 - accuracy: 0.2022 - precision_2: 0.4197 - recall_2: 0.0316 - val_loss: 1.9942 - val_accuracy: 0.2099 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 45/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0036 - accuracy: 0.2005 - precision_2: 0.5164 - recall_2: 0.0302 - val_loss: 1.9986 - val_accuracy: 0.2099 - val_precision_2: 0.5059 - val_recall_2: 0.1062\n",
            "Epoch 46/50\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 2.0039 - accuracy: 0.1929 - precision_2: 0.4810 - recall_2: 0.0277 - val_loss: 1.9972 - val_accuracy: 0.2148 - val_precision_2: 0.5059 - val_recall_2: 0.1062\n",
            "Epoch 47/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.9996 - accuracy: 0.2041 - precision_2: 0.4322 - recall_2: 0.0376 - val_loss: 1.9921 - val_accuracy: 0.2173 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 48/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.9989 - accuracy: 0.2049 - precision_2: 0.4154 - recall_2: 0.0222 - val_loss: 1.9934 - val_accuracy: 0.2148 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 49/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.9983 - accuracy: 0.1934 - precision_2: 0.4472 - recall_2: 0.0302 - val_loss: 1.9972 - val_accuracy: 0.2099 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
            "Epoch 50/50\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.9992 - accuracy: 0.2033 - precision_2: 0.4440 - recall_2: 0.0283 - val_loss: 1.9870 - val_accuracy: 0.2099 - val_precision_2: 0.4886 - val_recall_2: 0.1062\n",
            "count: 3\n",
            "optimizer: Adam\n",
            "accuracy: 0.2198\n",
            "macro_f1: 0.1083\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "optimizer: RMSprop, accuracy: 0.6419753086419753, macro_f1: 0.6150255275654946\n"
          ]
        }
      ],
      "source": [
        "optimizer_list = ['SGD', 'RMSprop', 'Adam']\n",
        "best_optimizer = ''\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for optimizer in optimizer_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=1,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'optimizer: {optimizer}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_optimizer = optimizer\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'optimizer: {best_optimizer}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2118221b",
      "metadata": {
        "id": "2118221b"
      },
      "outputs": [],
      "source": [
        "optimizer = best_optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c67dc7",
      "metadata": {
        "id": "d4c67dc7"
      },
      "source": [
        "### embedding_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "89a1adfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89a1adfc",
        "outputId": "8099924c-1ee1-47af-833f-8c49fc59e2ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 10\n",
            "embedding_size: 50\n",
            "accuracy: 0.5531\n",
            "macro_f1: 0.5503\n",
            "----------------------------------------------------------------------------\n",
            "count: 11\n",
            "embedding_size: 75\n",
            "accuracy: 0.2741\n",
            "macro_f1: 0.1431\n",
            "----------------------------------------------------------------------------\n",
            "count: 12\n",
            "embedding_size: 100\n",
            "accuracy: 0.2494\n",
            "macro_f1: 0.1499\n",
            "----------------------------------------------------------------------------\n",
            "count: 13\n",
            "embedding_size: 128\n",
            "accuracy: 0.2148\n",
            "macro_f1: 0.1028\n",
            "----------------------------------------------------------------------------\n",
            "count: 14\n",
            "embedding_size: 256\n",
            "accuracy: 0.3111\n",
            "macro_f1: 0.1951\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "embedding_size: 50, accuracy: 0.5530864197530864, macro_f1: 0.5503406992243404\n"
          ]
        }
      ],
      "source": [
        "embedding_size_list = [50, 75, 100, 128, 256]\n",
        "best_embedding_size = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for embedding_size in embedding_size_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'embedding_size: {embedding_size}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_embedding_size = embedding_size\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'embedding_size: {best_embedding_size}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b76cb43c",
      "metadata": {
        "id": "b76cb43c"
      },
      "outputs": [],
      "source": [
        "embedding_size = best_embedding_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1e2703",
      "metadata": {
        "id": "4f1e2703"
      },
      "source": [
        "### hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b2c03c76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2c03c76",
        "outputId": "5eac6b67-1605-4a4e-b181-a6975973ce6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 15\n",
            "hidden_size: 50\n",
            "accuracy: 0.3654\n",
            "macro_f1: 0.3234\n",
            "----------------------------------------------------------------------------\n",
            "count: 16\n",
            "hidden_size: 75\n",
            "accuracy: 0.2815\n",
            "macro_f1: 0.1666\n",
            "----------------------------------------------------------------------------\n",
            "count: 17\n",
            "hidden_size: 100\n",
            "accuracy: 0.4815\n",
            "macro_f1: 0.4209\n",
            "----------------------------------------------------------------------------\n",
            "count: 18\n",
            "hidden_size: 128\n",
            "accuracy: 0.2395\n",
            "macro_f1: 0.1217\n",
            "----------------------------------------------------------------------------\n",
            "count: 19\n",
            "hidden_size: 256\n",
            "accuracy: 0.4346\n",
            "macro_f1: 0.3739\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "hidden_size: 100, accuracy: 0.48148148148148145, macro_f1: 0.4209012345517303\n"
          ]
        }
      ],
      "source": [
        "hidden_size_list = [50, 75, 100, 128, 256]\n",
        "best_hidden_size = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for hidden_size in hidden_size_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'hidden_size: {hidden_size}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_hidden_size = hidden_size\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'hidden_size: {best_hidden_size}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0337e7e1",
      "metadata": {
        "id": "0337e7e1"
      },
      "outputs": [],
      "source": [
        "hidden_size = best_hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45e12e8",
      "metadata": {
        "id": "c45e12e8"
      },
      "source": [
        "### num_rnn_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "76fe4f15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76fe4f15",
        "outputId": "2b24e909-34e2-416c-8918-69813ff29b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 20\n",
            "num_rnn_layers: 1\n",
            "accuracy: 0.2790\n",
            "macro_f1: 0.1676\n",
            "----------------------------------------------------------------------------\n",
            "count: 21\n",
            "num_rnn_layers: 2\n",
            "accuracy: 0.2123\n",
            "macro_f1: 0.0885\n",
            "----------------------------------------------------------------------------\n",
            "count: 22\n",
            "num_rnn_layers: 3\n",
            "accuracy: 0.7136\n",
            "macro_f1: 0.7153\n",
            "----------------------------------------------------------------------------\n",
            "count: 23\n",
            "num_rnn_layers: 4\n",
            "accuracy: 0.7259\n",
            "macro_f1: 0.7240\n",
            "----------------------------------------------------------------------------\n",
            "count: 24\n",
            "num_rnn_layers: 5\n",
            "accuracy: 0.4840\n",
            "macro_f1: 0.4742\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "num_rnn_layers: 4, accuracy: 0.725925925925926, macro_f1: 0.7240019332033676\n"
          ]
        }
      ],
      "source": [
        "num_rnn_layers_list = [1, 2, 3, 4, 5]\n",
        "best_num_rnn_layers = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for num_rnn_layers in num_rnn_layers_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'num_rnn_layers: {num_rnn_layers}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_num_rnn_layers = num_rnn_layers\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'num_rnn_layers: {best_num_rnn_layers}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "0d38d6ea",
      "metadata": {
        "id": "0d38d6ea"
      },
      "outputs": [],
      "source": [
        "num_rnn_layers = best_num_rnn_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e1aaaf",
      "metadata": {
        "id": "f5e1aaaf"
      },
      "source": [
        "### num_mlp_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "50423afe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50423afe",
        "outputId": "fbc78a0f-b078-4936-99b0-2b5cb7403172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 25\n",
            "num_mlp_layers: 1\n",
            "accuracy: 0.6938\n",
            "macro_f1: 0.6933\n",
            "----------------------------------------------------------------------------\n",
            "count: 26\n",
            "num_mlp_layers: 2\n",
            "accuracy: 0.7481\n",
            "macro_f1: 0.7481\n",
            "----------------------------------------------------------------------------\n",
            "count: 27\n",
            "num_mlp_layers: 3\n",
            "accuracy: 0.1160\n",
            "macro_f1: 0.0318\n",
            "----------------------------------------------------------------------------\n",
            "count: 28\n",
            "num_mlp_layers: 4\n",
            "accuracy: 0.5580\n",
            "macro_f1: 0.5304\n",
            "----------------------------------------------------------------------------\n",
            "count: 29\n",
            "num_mlp_layers: 5\n",
            "accuracy: 0.1111\n",
            "macro_f1: 0.0223\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "num_mlp_layers: 2, accuracy: 0.7481481481481481, macro_f1: 0.74813020078548\n"
          ]
        }
      ],
      "source": [
        "num_mlp_layers_list = [1, 2, 3, 4, 5]\n",
        "best_num_mlp_layers = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for num_mlp_layers in num_mlp_layers_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'num_mlp_layers: {num_mlp_layers}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_num_mlp_layers = num_mlp_layers\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'num_mlp_layers: {best_num_mlp_layers}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f2bc2d8c",
      "metadata": {
        "id": "f2bc2d8c"
      },
      "outputs": [],
      "source": [
        "num_mlp_layers = best_num_mlp_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eca5e24b",
      "metadata": {
        "id": "eca5e24b"
      },
      "source": [
        "### dropout_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "40d933db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40d933db",
        "outputId": "a23859cf-4892-4923-e3c1-8c42ece41c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 30\n",
            "dropout_rate: 0.1\n",
            "accuracy: 0.7457\n",
            "macro_f1: 0.7462\n",
            "----------------------------------------------------------------------------\n",
            "count: 31\n",
            "dropout_rate: 0.3\n",
            "accuracy: 0.7432\n",
            "macro_f1: 0.7435\n",
            "----------------------------------------------------------------------------\n",
            "count: 32\n",
            "dropout_rate: 0.5\n",
            "accuracy: 0.7432\n",
            "macro_f1: 0.7447\n",
            "----------------------------------------------------------------------------\n",
            "count: 33\n",
            "dropout_rate: 0.6\n",
            "accuracy: 0.6914\n",
            "macro_f1: 0.6883\n",
            "----------------------------------------------------------------------------\n",
            "count: 34\n",
            "dropout_rate: 0.7\n",
            "accuracy: 0.6667\n",
            "macro_f1: 0.6665\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "dropout_rate: 0.1, accuracy: 0.745679012345679, macro_f1: 0.7462466268603757\n"
          ]
        }
      ],
      "source": [
        "dropout_rate_list = [0.1, 0.3, 0.5, 0.6, 0.7]\n",
        "best_dropout_rate = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for dropout_rate in dropout_rate_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'dropout_rate: {dropout_rate}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_dropout_rate = dropout_rate\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'dropout_rate: {best_dropout_rate}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "cbcd1271",
      "metadata": {
        "id": "cbcd1271"
      },
      "outputs": [],
      "source": [
        "dropout_rate = best_dropout_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9517fded",
      "metadata": {
        "id": "9517fded"
      },
      "source": [
        "### batch_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4efb9e61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4efb9e61",
        "outputId": "7cec022a-eba1-44e6-d9ba-67aa04c4cdf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 35\n",
            "batch_norm: True\n",
            "accuracy: 0.7481\n",
            "macro_f1: 0.7514\n",
            "----------------------------------------------------------------------------\n",
            "count: 36\n",
            "batch_norm: False\n",
            "accuracy: 0.5160\n",
            "macro_f1: 0.4911\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "batch_norm: True, accuracy: 0.7481481481481481, macro_f1: 0.7513985752395633\n"
          ]
        }
      ],
      "source": [
        "batch_norm_list = [True, False]\n",
        "best_batch_norm = False\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for batch_norm in batch_norm_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'batch_norm: {batch_norm}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_batch_norm = batch_norm\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'batch_norm: {best_batch_norm}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "1200315a",
      "metadata": {
        "id": "1200315a"
      },
      "outputs": [],
      "source": [
        "batch_norm = best_batch_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b214d3d",
      "metadata": {
        "id": "1b214d3d"
      },
      "source": [
        "### l2_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "c6fb98f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6fb98f2",
        "outputId": "558ceb8a-7c27-425e-9437-31c7ae66b5c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 37\n",
            "l2_reg: 0.001\n",
            "accuracy: 0.7506\n",
            "macro_f1: 0.7466\n",
            "----------------------------------------------------------------------------\n",
            "count: 38\n",
            "l2_reg: 0.005\n",
            "accuracy: 0.7506\n",
            "macro_f1: 0.7519\n",
            "----------------------------------------------------------------------------\n",
            "count: 39\n",
            "l2_reg: 0.01\n",
            "accuracy: 0.7383\n",
            "macro_f1: 0.7367\n",
            "----------------------------------------------------------------------------\n",
            "count: 40\n",
            "l2_reg: 0.1\n",
            "accuracy: 0.7383\n",
            "macro_f1: 0.7367\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "l2_reg: 0.005, accuracy: 0.7506172839506173, macro_f1: 0.7518850451938072\n"
          ]
        }
      ],
      "source": [
        "l2_reg_list = [0.001, 0.005, 0.01, 0.1]\n",
        "best_l2_reg = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for l2_reg in l2_reg_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'l2_reg: {l2_reg}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_l2_reg = l2_reg\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'l2_reg: {best_l2_reg}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "d4807d48",
      "metadata": {
        "id": "d4807d48"
      },
      "outputs": [],
      "source": [
        "l2_reg = best_l2_reg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5cad02e",
      "metadata": {
        "id": "e5cad02e"
      },
      "source": [
        "### learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "40a5f60c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40a5f60c",
        "outputId": "5fc79eb3-68ff-46fa-c187-a5731cca9c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 41\n",
            "learning_rate: 0.0001\n",
            "accuracy: 0.7580\n",
            "macro_f1: 0.7582\n",
            "----------------------------------------------------------------------------\n",
            "count: 42\n",
            "learning_rate: 0.001\n",
            "accuracy: 0.7580\n",
            "macro_f1: 0.7622\n",
            "----------------------------------------------------------------------------\n",
            "count: 43\n",
            "learning_rate: 0.01\n",
            "accuracy: 0.7679\n",
            "macro_f1: 0.7686\n",
            "----------------------------------------------------------------------------\n",
            "count: 44\n",
            "learning_rate: 0.1\n",
            "accuracy: 0.7457\n",
            "macro_f1: 0.7464\n",
            "----------------------------------------------------------------------------\n",
            "Best model:\n",
            "learning_rate: 0.01, accuracy: 0.7679012345679013, macro_f1: 0.7685730272578604\n"
          ]
        }
      ],
      "source": [
        "learning_rate_list = [0.0001, 0.001, 0.01, 0.1]\n",
        "best_learning_rate = 0\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "\n",
        "for learning_rate in learning_rate_list:\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    model = build_RNN(input_length=max_len, \n",
        "                          vocab_size=len(feats_dict), \n",
        "                          embedding_size=embedding_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=9,\n",
        "                          num_rnn_layers=num_rnn_layers, \n",
        "                          num_mlp_layers=num_mlp_layers,\n",
        "                          rnn_type=rnn_type,\n",
        "                          bidirectional=bidirectional,\n",
        "                          activation=activation,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          batch_norm=batch_norm,\n",
        "                          l2_reg=l2_reg,\n",
        "                          optimizer=optimizer,\n",
        "                          learning_rate=learning_rate,\n",
        "                        )\n",
        "\n",
        "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"),\n",
        "        monitor=\"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True)\n",
        "\n",
        "    mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                        validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                        epochs=epoch, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[checkpointer])\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_tune{count}.hdf5\"))\n",
        "\n",
        "    print(f'count: {count}')\n",
        "    print(f'learning_rate: {learning_rate}')\n",
        "\n",
        "    # evaluation\n",
        "    # generate prediction and format\n",
        "    y_pred = model.predict(valid_feats_matrix)\n",
        "    y_pred = [np.argmax(row) for row in y_pred]\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # evaluate performance\n",
        "    acc = accuracy_score(valid_labels, y_pred)\n",
        "    f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "    print(f'accuracy: {acc:.4f}')\n",
        "    print(f'macro_f1: {f1:.4f}')\n",
        "\n",
        "    # save best model\n",
        "    if f1 > best_f1:\n",
        "        best_learning_rate = learning_rate\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "    \n",
        "    print('----------------------------------------------------------------------------')\n",
        "\n",
        "print('Best model:')\n",
        "print(f'learning_rate: {best_learning_rate}, accuracy: {best_acc}, macro_f1: {best_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "30ce3367",
      "metadata": {
        "id": "30ce3367"
      },
      "outputs": [],
      "source": [
        "learning_rate = best_learning_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15c2dd2a",
      "metadata": {
        "id": "15c2dd2a"
      },
      "source": [
        "## Final LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "4f3d5720",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f3d5720",
        "outputId": "66a1fdc2-cdc9-4742-845a-4f1a260b4b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "37/37 [==============================] - 9s 75ms/step - loss: 3.1815 - accuracy: 0.1139 - precision_48: 0.0385 - recall_48: 2.7435e-04 - val_loss: 2.9903 - val_accuracy: 0.1160 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00\n",
            "Epoch 2/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 2.8342 - accuracy: 0.1427 - precision_48: 0.3626 - recall_48: 0.0091 - val_loss: 2.7740 - val_accuracy: 0.1160 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00\n",
            "Epoch 3/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 2.3468 - accuracy: 0.2532 - precision_48: 0.5635 - recall_48: 0.0390 - val_loss: 2.4353 - val_accuracy: 0.1951 - val_precision_48: 1.0000 - val_recall_48: 0.0198\n",
            "Epoch 4/50\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 2.0781 - accuracy: 0.2988 - precision_48: 0.6102 - recall_48: 0.0623 - val_loss: 2.2604 - val_accuracy: 0.2938 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00\n",
            "Epoch 5/50\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 1.8597 - accuracy: 0.3314 - precision_48: 0.5929 - recall_48: 0.0735 - val_loss: 1.9401 - val_accuracy: 0.3383 - val_precision_48: 0.7692 - val_recall_48: 0.0988\n",
            "Epoch 6/50\n",
            "37/37 [==============================] - 1s 31ms/step - loss: 1.6930 - accuracy: 0.3827 - precision_48: 0.6392 - recall_48: 0.0743 - val_loss: 1.8797 - val_accuracy: 0.3481 - val_precision_48: 0.6556 - val_recall_48: 0.1457\n",
            "Epoch 7/50\n",
            "37/37 [==============================] - 2s 57ms/step - loss: 1.5625 - accuracy: 0.4263 - precision_48: 0.6615 - recall_48: 0.1163 - val_loss: 1.6476 - val_accuracy: 0.4346 - val_precision_48: 0.7361 - val_recall_48: 0.1309\n",
            "Epoch 8/50\n",
            "37/37 [==============================] - 2s 47ms/step - loss: 1.4259 - accuracy: 0.4645 - precision_48: 0.6482 - recall_48: 0.1800 - val_loss: 2.1583 - val_accuracy: 0.3506 - val_precision_48: 0.6966 - val_recall_48: 0.1531\n",
            "Epoch 9/50\n",
            "37/37 [==============================] - 2s 44ms/step - loss: 1.3441 - accuracy: 0.5084 - precision_48: 0.6637 - recall_48: 0.2280 - val_loss: 1.7375 - val_accuracy: 0.3432 - val_precision_48: 0.4364 - val_recall_48: 0.0593\n",
            "Epoch 10/50\n",
            "37/37 [==============================] - 2s 50ms/step - loss: 1.1949 - accuracy: 0.5715 - precision_48: 0.7005 - recall_48: 0.3182 - val_loss: 1.6554 - val_accuracy: 0.4494 - val_precision_48: 0.5631 - val_recall_48: 0.2864\n",
            "Epoch 11/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 1.1193 - accuracy: 0.6074 - precision_48: 0.6820 - recall_48: 0.4195 - val_loss: 1.7220 - val_accuracy: 0.4198 - val_precision_48: 0.4937 - val_recall_48: 0.2914\n",
            "Epoch 12/50\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 1.0219 - accuracy: 0.6406 - precision_48: 0.6915 - recall_48: 0.5306 - val_loss: 1.6253 - val_accuracy: 0.5333 - val_precision_48: 0.5748 - val_recall_48: 0.4173\n",
            "Epoch 13/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.9049 - accuracy: 0.7048 - precision_48: 0.7347 - recall_48: 0.6510 - val_loss: 1.8821 - val_accuracy: 0.3901 - val_precision_48: 0.4396 - val_recall_48: 0.3235\n",
            "Epoch 14/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.8305 - accuracy: 0.7380 - precision_48: 0.7657 - recall_48: 0.6823 - val_loss: 1.5083 - val_accuracy: 0.5877 - val_precision_48: 0.6206 - val_recall_48: 0.5654\n",
            "Epoch 15/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.7362 - accuracy: 0.7797 - precision_48: 0.7975 - recall_48: 0.7564 - val_loss: 2.1924 - val_accuracy: 0.4247 - val_precision_48: 0.4511 - val_recall_48: 0.4099\n",
            "Epoch 16/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.6577 - accuracy: 0.8132 - precision_48: 0.8246 - recall_48: 0.8008 - val_loss: 1.9819 - val_accuracy: 0.5481 - val_precision_48: 0.5561 - val_recall_48: 0.5383\n",
            "Epoch 17/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.5746 - accuracy: 0.8450 - precision_48: 0.8527 - recall_48: 0.8337 - val_loss: 1.4929 - val_accuracy: 0.6321 - val_precision_48: 0.6517 - val_recall_48: 0.6099\n",
            "Epoch 18/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.5235 - accuracy: 0.8519 - precision_48: 0.8596 - recall_48: 0.8450 - val_loss: 1.7339 - val_accuracy: 0.5654 - val_precision_48: 0.5725 - val_recall_48: 0.5556\n",
            "Epoch 19/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.4667 - accuracy: 0.8757 - precision_48: 0.8819 - recall_48: 0.8689 - val_loss: 1.7130 - val_accuracy: 0.6222 - val_precision_48: 0.6320 - val_recall_48: 0.6148\n",
            "Epoch 20/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.4336 - accuracy: 0.8853 - precision_48: 0.8918 - recall_48: 0.8798 - val_loss: 1.9253 - val_accuracy: 0.5506 - val_precision_48: 0.5615 - val_recall_48: 0.5407\n",
            "Epoch 21/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.4128 - accuracy: 0.9023 - precision_48: 0.9061 - recall_48: 0.8979 - val_loss: 1.8861 - val_accuracy: 0.5827 - val_precision_48: 0.5922 - val_recall_48: 0.5630\n",
            "Epoch 22/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.3608 - accuracy: 0.9128 - precision_48: 0.9178 - recall_48: 0.9097 - val_loss: 1.7154 - val_accuracy: 0.6222 - val_precision_48: 0.6320 - val_recall_48: 0.6148\n",
            "Epoch 23/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.3436 - accuracy: 0.9213 - precision_48: 0.9256 - recall_48: 0.9182 - val_loss: 1.6118 - val_accuracy: 0.6321 - val_precision_48: 0.6442 - val_recall_48: 0.6123\n",
            "Epoch 24/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.3026 - accuracy: 0.9267 - precision_48: 0.9317 - recall_48: 0.9237 - val_loss: 1.6702 - val_accuracy: 0.6395 - val_precision_48: 0.6443 - val_recall_48: 0.6395\n",
            "Epoch 25/50\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.3385 - accuracy: 0.9215 - precision_48: 0.9268 - recall_48: 0.9169 - val_loss: 1.4039 - val_accuracy: 0.6914 - val_precision_48: 0.7008 - val_recall_48: 0.6765\n",
            "Epoch 26/50\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.2444 - accuracy: 0.9498 - precision_48: 0.9518 - recall_48: 0.9479 - val_loss: 1.4959 - val_accuracy: 0.6938 - val_precision_48: 0.6973 - val_recall_48: 0.6938\n",
            "Epoch 27/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.2474 - accuracy: 0.9443 - precision_48: 0.9489 - recall_48: 0.9416 - val_loss: 1.4977 - val_accuracy: 0.6914 - val_precision_48: 0.6977 - val_recall_48: 0.6840\n",
            "Epoch 28/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.2180 - accuracy: 0.9534 - precision_48: 0.9562 - recall_48: 0.9523 - val_loss: 1.5393 - val_accuracy: 0.6938 - val_precision_48: 0.6975 - val_recall_48: 0.6889\n",
            "Epoch 29/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.2099 - accuracy: 0.9553 - precision_48: 0.9556 - recall_48: 0.9517 - val_loss: 1.5299 - val_accuracy: 0.6963 - val_precision_48: 0.7000 - val_recall_48: 0.6914\n",
            "Epoch 30/50\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.2008 - accuracy: 0.9564 - precision_48: 0.9584 - recall_48: 0.9547 - val_loss: 1.4216 - val_accuracy: 0.7160 - val_precision_48: 0.7207 - val_recall_48: 0.7136\n",
            "Epoch 31/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.1588 - accuracy: 0.9695 - precision_48: 0.9711 - recall_48: 0.9690 - val_loss: 1.5344 - val_accuracy: 0.7136 - val_precision_48: 0.7236 - val_recall_48: 0.7111\n",
            "Epoch 32/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.1718 - accuracy: 0.9671 - precision_48: 0.9689 - recall_48: 0.9646 - val_loss: 1.5669 - val_accuracy: 0.7012 - val_precision_48: 0.7053 - val_recall_48: 0.6914\n",
            "Epoch 33/50\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.1811 - accuracy: 0.9624 - precision_48: 0.9645 - recall_48: 0.9610 - val_loss: 1.4895 - val_accuracy: 0.7111 - val_precision_48: 0.7168 - val_recall_48: 0.7062\n",
            "Epoch 34/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.1499 - accuracy: 0.9715 - precision_48: 0.9728 - recall_48: 0.9704 - val_loss: 1.4575 - val_accuracy: 0.7111 - val_precision_48: 0.7186 - val_recall_48: 0.7062\n",
            "Epoch 35/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.1520 - accuracy: 0.9676 - precision_48: 0.9694 - recall_48: 0.9660 - val_loss: 1.5669 - val_accuracy: 0.7037 - val_precision_48: 0.7128 - val_recall_48: 0.6988\n",
            "Epoch 36/50\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.1610 - accuracy: 0.9676 - precision_48: 0.9692 - recall_48: 0.9660 - val_loss: 1.5694 - val_accuracy: 0.7284 - val_precision_48: 0.7375 - val_recall_48: 0.7284\n",
            "Epoch 37/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.1275 - accuracy: 0.9756 - precision_48: 0.9782 - recall_48: 0.9739 - val_loss: 1.6310 - val_accuracy: 0.6914 - val_precision_48: 0.6940 - val_recall_48: 0.6889\n",
            "Epoch 38/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.1321 - accuracy: 0.9739 - precision_48: 0.9757 - recall_48: 0.9712 - val_loss: 1.4468 - val_accuracy: 0.7235 - val_precision_48: 0.7298 - val_recall_48: 0.7136\n",
            "Epoch 39/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.1342 - accuracy: 0.9726 - precision_48: 0.9752 - recall_48: 0.9698 - val_loss: 1.4972 - val_accuracy: 0.7284 - val_precision_48: 0.7300 - val_recall_48: 0.7210\n",
            "Epoch 40/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.1143 - accuracy: 0.9775 - precision_48: 0.9783 - recall_48: 0.9770 - val_loss: 1.7356 - val_accuracy: 0.6963 - val_precision_48: 0.6967 - val_recall_48: 0.6864\n",
            "Epoch 41/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.1116 - accuracy: 0.9778 - precision_48: 0.9788 - recall_48: 0.9772 - val_loss: 1.6923 - val_accuracy: 0.7136 - val_precision_48: 0.7218 - val_recall_48: 0.7111\n",
            "Epoch 42/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.1260 - accuracy: 0.9761 - precision_48: 0.9801 - recall_48: 0.9731 - val_loss: 1.4772 - val_accuracy: 0.7210 - val_precision_48: 0.7275 - val_recall_48: 0.7185\n",
            "Epoch 43/50\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.0993 - accuracy: 0.9794 - precision_48: 0.9807 - recall_48: 0.9778 - val_loss: 1.5692 - val_accuracy: 0.7333 - val_precision_48: 0.7382 - val_recall_48: 0.7309\n",
            "Epoch 44/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.1142 - accuracy: 0.9783 - precision_48: 0.9815 - recall_48: 0.9778 - val_loss: 1.7636 - val_accuracy: 0.6938 - val_precision_48: 0.6990 - val_recall_48: 0.6938\n",
            "Epoch 45/50\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.1232 - accuracy: 0.9778 - precision_48: 0.9802 - recall_48: 0.9764 - val_loss: 1.5578 - val_accuracy: 0.7457 - val_precision_48: 0.7531 - val_recall_48: 0.7457\n",
            "Epoch 46/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.1061 - accuracy: 0.9783 - precision_48: 0.9810 - recall_48: 0.9764 - val_loss: 1.4635 - val_accuracy: 0.7383 - val_precision_48: 0.7469 - val_recall_48: 0.7358\n",
            "Epoch 47/50\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.1057 - accuracy: 0.9748 - precision_48: 0.9777 - recall_48: 0.9731 - val_loss: 1.6277 - val_accuracy: 0.7235 - val_precision_48: 0.7282 - val_recall_48: 0.7210\n",
            "Epoch 48/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.0859 - accuracy: 0.9830 - precision_48: 0.9851 - recall_48: 0.9822 - val_loss: 1.5616 - val_accuracy: 0.7210 - val_precision_48: 0.7293 - val_recall_48: 0.7185\n",
            "Epoch 49/50\n",
            "37/37 [==============================] - 1s 32ms/step - loss: 0.0901 - accuracy: 0.9819 - precision_48: 0.9838 - recall_48: 0.9808 - val_loss: 1.5872 - val_accuracy: 0.7284 - val_precision_48: 0.7332 - val_recall_48: 0.7259\n",
            "Epoch 50/50\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.0926 - accuracy: 0.9824 - precision_48: 0.9841 - recall_48: 0.9819 - val_loss: 1.5500 - val_accuracy: 0.7210 - val_precision_48: 0.7282 - val_recall_48: 0.7210\n",
            "count: 44\n",
            "learning_rate: 0.01\n",
            "accuracy: 0.7457\n",
            "macro_f1: 0.7481\n"
          ]
        }
      ],
      "source": [
        "model = build_RNN(input_length=max_len, \n",
        "                      vocab_size=len(feats_dict), \n",
        "                      embedding_size=embedding_size,\n",
        "                      hidden_size=hidden_size, \n",
        "                      output_size=9,\n",
        "                      num_rnn_layers=num_rnn_layers, \n",
        "                      num_mlp_layers=num_mlp_layers,\n",
        "                      rnn_type=rnn_type,\n",
        "                      bidirectional=bidirectional,\n",
        "                      activation=activation,\n",
        "                      dropout_rate=dropout_rate,\n",
        "                      batch_norm=batch_norm,\n",
        "                      l2_reg=l2_reg,\n",
        "                      optimizer=optimizer,\n",
        "                      learning_rate=learning_rate,\n",
        "                    )\n",
        "\n",
        "checkpointer = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(\"models\", f\"weights_RNN_final.hdf5\"),\n",
        "    monitor=\"val_accuracy\",\n",
        "    verbose=0,\n",
        "    save_best_only=True)\n",
        "\n",
        "mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
        "                    validation_data=(valid_feats_matrix, valid_label_matrix),\n",
        "                    epochs=epoch, batch_size=batch_size, verbose=1,\n",
        "                    callbacks=[checkpointer])\n",
        "\n",
        "model = keras.models.load_model(os.path.join(\"models\", f\"weights_RNN_final.hdf5\"))\n",
        "\n",
        "print(f'count: {count}')\n",
        "print(f'learning_rate: {learning_rate}')\n",
        "\n",
        "# evaluation\n",
        "# generate prediction and format\n",
        "y_pred = model.predict(valid_feats_matrix)\n",
        "y_pred = [np.argmax(row) for row in y_pred]\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# evaluate performance\n",
        "acc = accuracy_score(valid_labels, y_pred)\n",
        "f1 = f1_score(valid_labels, y_pred, average='macro')\n",
        "print(f'accuracy: {acc:.4f}')\n",
        "print(f'macro_f1: {f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e37ced5",
      "metadata": {
        "id": "2e37ced5"
      },
      "source": [
        "## Final Model Test Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "ff8b095f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff8b095f",
        "outputId": "26855ffd-5a8f-4f18-ac01-a8a9744562a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.7644\n",
            "test macro_f1: 0.7672\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CC       0.96      0.88      0.92        50\n",
            "          NC       0.79      0.82      0.80        50\n",
            "          PW       0.79      0.84      0.82        50\n",
            "          HC       0.51      0.56      0.53        50\n",
            "          PL       0.78      0.72      0.75        50\n",
            "          CR       0.64      0.76      0.70        50\n",
            "          CG       0.90      0.74      0.81        50\n",
            "          BE       0.79      0.82      0.80        50\n",
            "           N       0.80      0.74      0.77        50\n",
            "\n",
            "    accuracy                           0.76       450\n",
            "   macro avg       0.77      0.76      0.77       450\n",
            "weighted avg       0.77      0.76      0.77       450\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# generate prediction and format\n",
        "y_pred = model.predict(test_feats_matrix)\n",
        "y_pred = [np.argmax(row) for row in y_pred]\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# evaluate performance\n",
        "acc = accuracy_score(test_labels, y_pred)\n",
        "f1 = f1_score(test_labels, y_pred, average='macro')\n",
        "print(f'test accuracy: {acc:.4f}')\n",
        "print(f'test macro_f1: {f1:.4f}')\n",
        "print(classification_report(test_labels, y_pred,target_names=labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "31e3c0e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "31e3c0e5",
        "outputId": "4372151f-e18e-4aa9-95b9-22551269375d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f18732bd350>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot confusion matrix\n",
        "cm = confusion_matrix(test_labels, y_pred , normalize='pred')\n",
        "cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "cmp.plot(ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "12dcc624",
      "metadata": {
        "id": "12dcc624"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "name": "UROP_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}