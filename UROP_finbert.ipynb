{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:17:10.121092Z",
     "start_time": "2022-06-02T02:17:10.098888Z"
    },
    "id": "xt8INOdGEH6y"
   },
   "source": [
    "The FinBERT pretrained model can be fine-tuned on downstream financial NLP tasks. This notebook illustrates the process of fine-tuning FinBERT using Huggingface ðŸ¤—'s tranformers library. You can modify this notebook accordingly to meet you needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNt9GHQ6EWKO",
    "outputId": "76568832-a57f-45e8-8171-0ef038647e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.7 MB 17.9 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 9.7 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 30.6 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101 kB 12.3 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365 kB 12.9 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115 kB 57.4 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141 kB 20.7 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212 kB 55.3 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127 kB 48.7 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "!pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:30:37.970878Z",
     "start_time": "2022-06-02T02:30:34.740917Z"
    },
    "id": "qzLjYunsEH67"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from transformers import BertTokenizer, Trainer, BertForSequenceClassification, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:45:07.491364Z",
     "start_time": "2022-06-02T02:45:07.482746Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feXeWBEbEH69",
    "outputId": "ecfa313c-1c67-4bb2-fdc6-2aefd884392f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.12.0+cu113', '4.21.1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tested in transformers==4.18.0, pytorch==1.7.1 \n",
    "import torch\n",
    "import transformers\n",
    "torch.__version__, transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:44:25.431428Z",
     "start_time": "2022-06-02T02:44:25.423127Z"
    },
    "id": "XMRatd3MEH6_"
   },
   "source": [
    "*Note: the following code is for demonstration purpose. Please use GPU for fast inference on large scale dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:44:18.286115Z",
     "start_time": "2022-06-02T02:44:18.222850Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThtE8KarEH7A",
    "outputId": "d6ec7792-5527-44d1-9ca8-a6a2b84040f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rM5XgG2iEH7B"
   },
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c98-dI6AEjqx",
    "outputId": "52412c1e-199f-4f2e-b226-c5333bb7c2d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:06.431016Z",
     "start_time": "2022-06-02T02:54:06.391735Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bld9ujGZEH7C",
    "outputId": "8b7bef13-5a9e-4a8b-810c-2cfe9d20d17c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-acfbd5ab-f317-476a-8eba-a9baca99dca7\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product Stewardship Manage potential risks of ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNL and UNDP will develop a coordinated master...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ChevronTexaco continues to advocate for consis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maturing oil fields, more technically challeng...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We found that units in which employees rated t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acfbd5ab-f317-476a-8eba-a9baca99dca7')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-acfbd5ab-f317-476a-8eba-a9baca99dca7 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-acfbd5ab-f317-476a-8eba-a9baca99dca7');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  Product Stewardship Manage potential risks of ...      4\n",
       "1  CNL and UNDP will develop a coordinated master...      5\n",
       "2  ChevronTexaco continues to advocate for consis...      0\n",
       "3  Maturing oil fields, more technically challeng...      1\n",
       "4  We found that units in which employees rated t...      3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/final_dataset_formatted.csv') ## use your own customized dataset\n",
    "df_test = pd.read_csv('/content/drive/MyDrive/final_dataset_formatted_test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:06.895445Z",
     "start_time": "2022-06-02T02:54:06.880863Z"
    },
    "id": "PusiqSfLEH7C"
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['sentence', 'label']) ## drop missing values\n",
    "df_test = df_test.dropna(subset=['sentence', 'label']) ## drop missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmVl8L1bHX2d",
    "outputId": "3887d67b-4a5f-4048-a12a-f2fc8b4f11c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY31kalTEH7D"
   },
   "source": [
    "### prepare training/validation/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:08.030875Z",
     "start_time": "2022-06-02T02:54:07.999354Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUuxNysNEH7E",
    "outputId": "c066d868-9d3b-4f03-b82c-6912893ea234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3645, 2) (450, 2) (405, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test, = train_test_split(df, stratify=df['label'], test_size=0.1, random_state=42)\n",
    "df_train, df_val = train_test_split(df_train, stratify=df_train['label'],test_size=0.1, random_state=42)\n",
    "print(df_train.shape, df_test.shape, df_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fodlT46EH7F"
   },
   "source": [
    "### load FinBERT pretrained model\n",
    "The pretrained FinBERT model path on Huggingface is https://huggingface.co/yiyanghkust/finbert-pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BOZXEcxij5Qp"
   },
   "outputs": [],
   "source": [
    "model_name = 'yiyanghkust/finbert-pretrain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:17.945203Z",
     "start_time": "2022-06-02T02:54:10.422200Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "a48d76d3284c4c1a90db79f846afc40a",
      "6c79b7e30e934869aefe34d2e20decd5",
      "b9c4197e248f4a3f94a7b7bba758ef80",
      "52a82004a3d343c299fa6bc9061f076b",
      "0a1acf5d901746bcbaabe2a79e719785",
      "62d9547882fb469c80b95642c14da5fa",
      "b568bb1e17d04f3daca49c1b4205756e",
      "bae6c12dc8274fd290756de5d1461f24",
      "15d526c2547344e9960af67106128c74",
      "75861f5053fc4703a6cee058848daa74",
      "dab0b987427a49278b7e3878c39965f0",
      "3ab9bebdb83b41b5ab5bbcd451b6f8fd",
      "8f4ab46d2e5d405a835c69dc75ee72b6",
      "c106b02e4a8243cab51fa8b3583482a5",
      "5bad19119a98492daf93ae2dc1f7a76d",
      "9058a75c97ee40ac9e030e9e663fcc3a",
      "116ae829e6894877baa4c955d7e68d74",
      "241f216b860e484da93c158bb773d1cb",
      "6ce1471fcc4f48b49f32a4df740fdc33",
      "dbe3b5b35c8a4f0eacfa780641406d66",
      "de194c8ba40d459197f1374a24dfaa3b",
      "f6b220e1bbfe48d78a93551151b6b1f2",
      "047369990f1a4fc9844718486460a200",
      "b117cdba02cf44409a9b401f21c3a336",
      "b35e16297d664f728538ed769e69bb09",
      "3e04da375d82459aba41ab7dc52ed37d",
      "d5fc29e255f44e948e7e7963fc0d7049",
      "f45a9ecff5344c5a94a07f98e880fb0f",
      "ff631c22406c4ebd92d0b63f2b94ce84",
      "72a8b0d08c8e4498997ddfa908d9643f",
      "f3bb504ae1904a0699a5b1aa62d4879a",
      "40b01628efb04b34bf94c9f54cfc1822",
      "f9c1b9b34f0a40139f4f5e2c3bff77b4"
     ]
    },
    "id": "oWu2LHUhEH7F",
    "outputId": "fbcba881-ba7e-4b1a-fdbc-531e6aa34664"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48d76d3284c4c1a90db79f846afc40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/359 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab9bebdb83b41b5ab5bbcd451b6f8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/421M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047369990f1a4fc9844718486460a200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/221k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain',num_labels=9)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:37:33.687054Z",
     "start_time": "2022-06-02T02:37:33.664650Z"
    },
    "id": "w22rTOuUEH7G"
   },
   "source": [
    "### prepare dataset for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:33.660010Z",
     "start_time": "2022-06-02T02:54:17.948143Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "4539e98f920f4870bd783e8f94ec0032",
      "dc88c00f01c34b65a0e5f1e78ad436a2",
      "d7d86bede479489db6d90a5e7c954413",
      "3df9905d42bd45a18eb292ce1a6c3409",
      "3275dfc58b3643cd8af7bba5918a0e4d",
      "a8aa7c7824a8422086d2ea59696cf465",
      "138080dd5bd241cfad5a7b793ba35e9c",
      "1a0650735182482b9c85a8b0635eb0b3",
      "f8b9d274a98049e79b1f1a3ca7e6eb0c",
      "1857e6d6c98a4811a37411ec8da59012",
      "2be10749b03a4a74a62e83428daf5e36",
      "d1064fc7ad6641a792cd03899f0e1b25",
      "d5acb9e6dc804dcc937c7243600588cb",
      "339a7edf9f964f68ad470a5dd8c9c87f",
      "3b00b8b52a844a41ace99f163a4a88d4",
      "174b8dc9c91d4be2a14ab9855f1b94f2",
      "60b800f860674dd89d7c0fe9e5abac52",
      "b4a2f52f379240fc9c278ddf18db4038",
      "a69f97661dca4ee58fe9ba87f5179562",
      "8049e9b7b2b444a6983d5f07d67ae182",
      "5c191fd3223346d89cbd883027a18c5f",
      "4b80ce4d44214e8389ea517c75eba7d0",
      "cd831b6b92234a76b30e112127d8d4dd",
      "ab49c6c16dd34fe0b0e6c977f0cbf10c",
      "f9310230e50b459db761b8920d2ccfa7",
      "294ed8517bbc41469739a2a953f01a99",
      "b29de32cc0e54d32a15dcb2b937cf5d6",
      "6877534a9d6f4137981e9344ce34e2f0",
      "f5a8a95652ec4819bcadba10f60ad514",
      "17b4ebdfd5b44cfba12db5d23d0f7116",
      "b9797530666c4d13894784774b2c9cb3",
      "b18ef91d44f24a55a9b9d91c6484a578",
      "ddd05d64f67946549441be1cc0c6de3c"
     ]
    },
    "id": "BxmvuLu-EH7G",
    "outputId": "06e6f52c-758b-47de-f3ff-e087744c2b35"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4539e98f920f4870bd783e8f94ec0032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1064fc7ad6641a792cd03899f0e1b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd831b6b92234a76b30e112127d8d4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['CC', 'NC', 'PW', 'HC', 'PL', 'CR', 'CG', 'BE', 'N']\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_val = Dataset.from_pandas(df_val)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "dataset_train = dataset_train.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
    "dataset_val = dataset_val.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
    "dataset_test = dataset_test.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length' , max_length=128), batched=True)\n",
    "\n",
    "dataset_train.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataset_val.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataset_test.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7gjFwjEEH7G"
   },
   "source": [
    "### **First Trial:** \n",
    "### define training options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kGjmYDFGw7rG"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy' : accuracy_score(predictions, labels), 'macro-f1' : f1_score(predictions, labels, average='macro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:57:15.963784Z",
     "start_time": "2022-06-02T02:54:33.662575Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Q1a8JrgYEH7H",
    "outputId": "52a98723-1469-4fe4-e9fb-f94ac97ddb55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 06:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.610258</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.799519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.520416</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.836424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.498828</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.850855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.530829</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.858516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.532925</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.848495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-114\n",
      "Configuration saved in temp/checkpoint-114/config.json\n",
      "Model weights saved in temp/checkpoint-114/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-342\n",
      "Configuration saved in temp/checkpoint-342/config.json\n",
      "Model weights saved in temp/checkpoint-342/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-570\n",
      "Configuration saved in temp/checkpoint-570/config.json\n",
      "Model weights saved in temp/checkpoint-570/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-456 (score: 0.8592592592592593).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=570, training_loss=0.4094081435287208, metrics={'train_runtime': 413.1109, 'train_samples_per_second': 44.116, 'train_steps_per_second': 1.38, 'total_flos': 1198875090758400.0, 'train_loss': 0.4094081435287208, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "        output_dir = 'temp/',\n",
    "        evaluation_strategy = 'epoch',\n",
    "        save_strategy = 'epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=args,                  # training arguments, defined above\n",
    "        train_dataset=dataset_train,         # training dataset\n",
    "        eval_dataset=dataset_val,            # evaluation dataset\n",
    "        compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv0eZ5NSEH7H"
   },
   "source": [
    "### evaluate on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "9iebJjbYUOgr",
    "outputId": "e021730c-dace-4731-ff01-f39b384b6e32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.5308288335800171, 'test_accuracy': 0.8592592592592593, 'test_macro-f1': 0.8585164618366923, 'test_runtime': 2.9505, 'test_samples_per_second': 137.265, 'test_steps_per_second': 4.406} \n",
      "\n",
      "accuracy 0.8592592592592593\n",
      "macro_f1 score: 0.8585164618366923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.87      0.89      0.88        45\n",
      "          NC       0.86      0.84      0.85        45\n",
      "          PW       0.87      0.91      0.89        45\n",
      "          HC       0.80      0.80      0.80        45\n",
      "          PL       0.80      0.80      0.80        45\n",
      "          CR       0.80      0.73      0.77        45\n",
      "          CG       0.86      0.98      0.92        45\n",
      "          BE       0.88      0.84      0.86        45\n",
      "           N       0.98      0.93      0.95        45\n",
      "\n",
      "    accuracy                           0.86       405\n",
      "   macro avg       0.86      0.86      0.86       405\n",
      "weighted avg       0.86      0.86      0.86       405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate prediction\n",
    "predictions = trainer.predict(dataset_val)\n",
    "print(predictions.metrics, '\\n')\n",
    "y_pred = np.array([np.argmax(pred) for pred in predictions[0]])\n",
    "\n",
    "# evaluate model performance\n",
    "f1 = f1_score(predictions[1], y_pred, average='macro')\n",
    "print('accuracy %s' % accuracy_score(y_pred, predictions[1]))\n",
    "print(f'macro_f1 score: {f1}')\n",
    "print(classification_report(predictions[1], y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tJjzhWiEH7I"
   },
   "source": [
    "### save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T03:09:11.599174Z",
     "start_time": "2022-06-02T03:09:10.847115Z"
    },
    "id": "eP_i-umoEH7I"
   },
   "outputs": [],
   "source": [
    "# trainer.save_model('finbert-sentiment/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PixbmS6TInTc"
   },
   "source": [
    "## **Fine-Tuning FinBERT Model**  \n",
    "  \n",
    "learning_rate   \n",
    "weight_decay   \n",
    "num_train_epochs  \n",
    "per_device_train_batch_size  \n",
    "per_device_eval_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZOEo6bhGSk_L"
   },
   "outputs": [],
   "source": [
    "# initial hyperparameters settings\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0\n",
    "num_train_epochs = 5.0\n",
    "per_device_train_batch_size = 32\n",
    "per_device_eval_batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s72LWCKTPGW"
   },
   "source": [
    "### learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KJyKml6gIqr9",
    "outputId": "728900b1-9afa-451e-90f6-e2700185e7d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 06:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.624900</td>\n",
       "      <td>0.938687</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>0.763678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>0.599866</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.824744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.425300</td>\n",
       "      <td>0.538154</td>\n",
       "      <td>0.834568</td>\n",
       "      <td>0.832941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.326000</td>\n",
       "      <td>0.515066</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.836203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.506741</td>\n",
       "      <td>0.846914</td>\n",
       "      <td>0.846059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-114\n",
      "Configuration saved in temp/checkpoint-114/config.json\n",
      "Model weights saved in temp/checkpoint-114/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-342\n",
      "Configuration saved in temp/checkpoint-342/config.json\n",
      "Model weights saved in temp/checkpoint-342/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-570\n",
      "Configuration saved in temp/checkpoint-570/config.json\n",
      "Model weights saved in temp/checkpoint-570/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-570 (score: 0.8469135802469135).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 1e-05\n",
      " {'test_loss': 0.5067413449287415, 'test_accuracy': 0.8469135802469135, 'test_macro-f1': 0.8460591973659878, 'test_runtime': 2.9225, 'test_samples_per_second': 138.579, 'test_steps_per_second': 4.448} \n",
      "\n",
      "accuracy 0.8469135802469135\n",
      "macro_f1 score: 0.8460591973659878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.85      0.89      0.87        45\n",
      "          NC       0.89      0.87      0.88        45\n",
      "          PW       0.82      0.93      0.87        45\n",
      "          HC       0.77      0.80      0.78        45\n",
      "          PL       0.82      0.71      0.76        45\n",
      "          CR       0.79      0.76      0.77        45\n",
      "          CG       0.89      0.93      0.91        45\n",
      "          BE       0.84      0.82      0.83        45\n",
      "           N       0.95      0.91      0.93        45\n",
      "\n",
      "    accuracy                           0.85       405\n",
      "   macro avg       0.85      0.85      0.85       405\n",
      "weighted avg       0.85      0.85      0.85       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 06:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.011400</td>\n",
       "      <td>0.541175</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.835672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.501556</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.855433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.537148</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.849684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.591692</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.848753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>0.600464</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.850752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-114\n",
      "Configuration saved in temp/checkpoint-114/config.json\n",
      "Model weights saved in temp/checkpoint-114/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-342\n",
      "Configuration saved in temp/checkpoint-342/config.json\n",
      "Model weights saved in temp/checkpoint-342/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-570\n",
      "Configuration saved in temp/checkpoint-570/config.json\n",
      "Model weights saved in temp/checkpoint-570/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-228 (score: 0.8567901234567902).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 3e-05\n",
      " {'test_loss': 0.5015555620193481, 'test_accuracy': 0.8567901234567902, 'test_macro-f1': 0.8554332425864959, 'test_runtime': 2.9327, 'test_samples_per_second': 138.098, 'test_steps_per_second': 4.433} \n",
      "\n",
      "accuracy 0.8567901234567902\n",
      "macro_f1 score: 0.8554332425864959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.83      0.89      0.86        45\n",
      "          NC       0.85      0.89      0.87        45\n",
      "          PW       0.84      0.93      0.88        45\n",
      "          HC       0.78      0.80      0.79        45\n",
      "          PL       0.85      0.78      0.81        45\n",
      "          CR       0.79      0.69      0.74        45\n",
      "          CG       0.88      0.96      0.91        45\n",
      "          BE       0.90      0.82      0.86        45\n",
      "           N       0.98      0.96      0.97        45\n",
      "\n",
      "    accuracy                           0.86       405\n",
      "   macro avg       0.86      0.86      0.86       405\n",
      "weighted avg       0.86      0.86      0.86       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 06:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.838900</td>\n",
       "      <td>0.542318</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.820524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.255400</td>\n",
       "      <td>0.477961</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.865232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092200</td>\n",
       "      <td>0.580618</td>\n",
       "      <td>0.861728</td>\n",
       "      <td>0.860862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.610449</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.849554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.623253</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.852045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-114\n",
      "Configuration saved in temp/checkpoint-114/config.json\n",
      "Model weights saved in temp/checkpoint-114/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-342\n",
      "Configuration saved in temp/checkpoint-342/config.json\n",
      "Model weights saved in temp/checkpoint-342/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-570\n",
      "Configuration saved in temp/checkpoint-570/config.json\n",
      "Model weights saved in temp/checkpoint-570/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-228 (score: 0.8666666666666667).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 5e-05\n",
      " {'test_loss': 0.4779607355594635, 'test_accuracy': 0.8666666666666667, 'test_macro-f1': 0.8652322408856377, 'test_runtime': 2.9303, 'test_samples_per_second': 138.21, 'test_steps_per_second': 4.436} \n",
      "\n",
      "accuracy 0.8666666666666667\n",
      "macro_f1 score: 0.8652322408856377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.83      0.96      0.89        45\n",
      "          NC       0.89      0.91      0.90        45\n",
      "          PW       0.86      0.93      0.89        45\n",
      "          HC       0.85      0.76      0.80        45\n",
      "          PL       0.83      0.78      0.80        45\n",
      "          CR       0.80      0.73      0.77        45\n",
      "          CG       0.88      0.98      0.93        45\n",
      "          BE       0.86      0.84      0.85        45\n",
      "           N       1.00      0.91      0.95        45\n",
      "\n",
      "    accuracy                           0.87       405\n",
      "   macro avg       0.87      0.87      0.87       405\n",
      "weighted avg       0.87      0.87      0.87       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 06:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.765200</td>\n",
       "      <td>0.581873</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.824396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.236900</td>\n",
       "      <td>0.543654</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.843912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.669242</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.844026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.662362</td>\n",
       "      <td>0.861728</td>\n",
       "      <td>0.861511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.663413</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-114\n",
      "Configuration saved in temp/checkpoint-114/config.json\n",
      "Model weights saved in temp/checkpoint-114/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-342\n",
      "Configuration saved in temp/checkpoint-342/config.json\n",
      "Model weights saved in temp/checkpoint-342/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-570\n",
      "Configuration saved in temp/checkpoint-570/config.json\n",
      "Model weights saved in temp/checkpoint-570/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-570 (score: 0.8666666666666667).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 7e-05\n",
      " {'test_loss': 0.663412868976593, 'test_accuracy': 0.8666666666666667, 'test_macro-f1': 0.8660411669708937, 'test_runtime': 2.9319, 'test_samples_per_second': 138.134, 'test_steps_per_second': 4.434} \n",
      "\n",
      "accuracy 0.8666666666666667\n",
      "macro_f1 score: 0.8660411669708937\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.91      0.91      0.91        45\n",
      "          NC       0.89      0.87      0.88        45\n",
      "          PW       0.88      0.93      0.90        45\n",
      "          HC       0.82      0.80      0.81        45\n",
      "          PL       0.83      0.76      0.79        45\n",
      "          CR       0.78      0.78      0.78        45\n",
      "          CG       0.85      0.91      0.88        45\n",
      "          BE       0.89      0.89      0.89        45\n",
      "           N       0.96      0.96      0.96        45\n",
      "\n",
      "    accuracy                           0.87       405\n",
      "   macro avg       0.87      0.87      0.87       405\n",
      "weighted avg       0.87      0.87      0.87       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 06:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.735100</td>\n",
       "      <td>0.623900</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.812457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.642751</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.826365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.621429</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.863882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.674918</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.864501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.668588</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.859365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-114\n",
      "Configuration saved in temp/checkpoint-114/config.json\n",
      "Model weights saved in temp/checkpoint-114/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-342\n",
      "Configuration saved in temp/checkpoint-342/config.json\n",
      "Model weights saved in temp/checkpoint-342/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-570\n",
      "Configuration saved in temp/checkpoint-570/config.json\n",
      "Model weights saved in temp/checkpoint-570/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-342 (score: 0.8641975308641975).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 9e-05\n",
      " {'test_loss': 0.6214293241500854, 'test_accuracy': 0.8641975308641975, 'test_macro-f1': 0.8638818622523636, 'test_runtime': 2.9428, 'test_samples_per_second': 137.624, 'test_steps_per_second': 4.418} \n",
      "\n",
      "accuracy 0.8641975308641975\n",
      "macro_f1 score: 0.8638818622523636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.93      0.93      0.93        45\n",
      "          NC       0.87      0.91      0.89        45\n",
      "          PW       0.91      0.93      0.92        45\n",
      "          HC       0.79      0.82      0.80        45\n",
      "          PL       0.89      0.71      0.79        45\n",
      "          CR       0.78      0.80      0.79        45\n",
      "          CG       0.80      0.91      0.85        45\n",
      "          BE       0.85      0.87      0.86        45\n",
      "           N       0.98      0.89      0.93        45\n",
      "\n",
      "    accuracy                           0.86       405\n",
      "   macro avg       0.87      0.86      0.86       405\n",
      "weighted avg       0.87      0.86      0.86       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n",
      "\n",
      "learning_rate: 1e-05, acc: 0.8469135802469135, macro-f1: 0.8460591973659878\n",
      "learning_rate: 3e-05, acc: 0.8567901234567902, macro-f1: 0.8554332425864959\n",
      "learning_rate: 5e-05, acc: 0.8666666666666667, macro-f1: 0.8652322408856377\n",
      "learning_rate: 7e-05, acc: 0.8666666666666667, macro-f1: 0.8660411669708937\n",
      "\n",
      "Best Model: learning_rate = 5e-05, acc = 0.8666666666666667, macro-f1 = 0\n"
     ]
    }
   ],
   "source": [
    "learning_rate_list = [1e-5, 3e-5, 5e-5, 7e-5, 9e-5]\n",
    "\n",
    "best_learning_rate = 0\n",
    "best_acc = 0\n",
    "best_macro_f1 = 0\n",
    "\n",
    "acc_list = list()\n",
    "macro_f1_list = list()\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "  # reload pre-trained model\n",
    "  model = BertForSequenceClassification.from_pretrained(model_name,num_labels=9)\n",
    "\n",
    "  # set training arguments\n",
    "  args = TrainingArguments(\n",
    "          output_dir = 'temp/',\n",
    "          evaluation_strategy = 'epoch',\n",
    "          save_strategy = 'epoch',\n",
    "          logging_strategy='epoch',\n",
    "          learning_rate=learning_rate,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "          weight_decay=weight_decay,\n",
    "          load_best_model_at_end=True,\n",
    "          metric_for_best_model='accuracy',\n",
    "  )\n",
    "\n",
    "  # set trainer\n",
    "  trainer = Trainer(\n",
    "          model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "          args=args,                  # training arguments, defined above\n",
    "          train_dataset=dataset_train,         # training dataset\n",
    "          eval_dataset=dataset_val,            # evaluation dataset\n",
    "          compute_metrics=compute_metrics\n",
    "  )\n",
    "\n",
    "  # train model\n",
    "  trainer.train() \n",
    "\n",
    "  # generate prediction\n",
    "  predictions = trainer.predict(dataset_val)\n",
    "  print(f'learning_rate: {learning_rate}\\n', predictions.metrics, '\\n')\n",
    "  y_pred = np.array([np.argmax(pred) for pred in predictions[0]])\n",
    "\n",
    "  # evaluate model performance\n",
    "  acc = accuracy_score(y_pred, predictions[1])\n",
    "  f1 = f1_score(predictions[1], y_pred, average='macro')\n",
    "  print(f'accuracy {acc}')\n",
    "  print(f'macro_f1 score: {f1}')\n",
    "  print(classification_report(predictions[1], y_pred,target_names=labels))\n",
    "  print('------------------------------------------------------------------------------------------------------')  \n",
    "  acc_list.append(acc)\n",
    "  macro_f1_list.append(f1)\n",
    "\n",
    "  # save best model hyperparameter\n",
    "  if best_acc < acc:\n",
    "    best_acc = acc\n",
    "    best_macro_f1 = f1\n",
    "    best_learning_rate = learning_rate\n",
    "\n",
    "print()\n",
    "for i in range(len(learning_rate_list)):\n",
    "  print(f'learning_rate: {learning_rate_list[i]}, acc: {acc_list[i]}, macro-f1: {macro_f1_list[i]}')\n",
    "print()\n",
    "print(f'Best Model: learning_rate = {best_learning_rate}, acc = {best_acc}, macro-f1 = {best_macro_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcMmfd9UWHTT"
   },
   "outputs": [],
   "source": [
    "learning_rate = best_learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxnNRttLgfRf"
   },
   "source": [
    "### per_device_train_batch_size & per_device_eval_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AH38fYZXgmf1",
    "outputId": "49e6809f-c602-4f15-cce6-9aa767044b7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2280\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2280' max='2280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2280/2280 08:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.684700</td>\n",
       "      <td>0.583453</td>\n",
       "      <td>0.834568</td>\n",
       "      <td>0.834709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.255600</td>\n",
       "      <td>0.678964</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.858786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.847522</td>\n",
       "      <td>0.854321</td>\n",
       "      <td>0.854145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.868910</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.851829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.902029</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.858723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to temp/checkpoint-1368\n",
      "Configuration saved in temp/checkpoint-1368/config.json\n",
      "Model weights saved in temp/checkpoint-1368/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to temp/checkpoint-1824\n",
      "Configuration saved in temp/checkpoint-1824/config.json\n",
      "Model weights saved in temp/checkpoint-1824/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to temp/checkpoint-2280\n",
      "Configuration saved in temp/checkpoint-2280/config.json\n",
      "Model weights saved in temp/checkpoint-2280/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-912 (score: 0.8592592592592593).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 8\n",
      " {'test_loss': 0.6789635419845581, 'test_accuracy': 0.8592592592592593, 'test_macro-f1': 0.8587863348198308, 'test_runtime': 3.1767, 'test_samples_per_second': 127.489, 'test_steps_per_second': 16.054} \n",
      "\n",
      "accuracy 0.8592592592592593\n",
      "macro_f1 score: 0.8587863348198308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.87      0.91      0.89        45\n",
      "          NC       0.93      0.87      0.90        45\n",
      "          PW       0.86      0.93      0.89        45\n",
      "          HC       0.82      0.80      0.81        45\n",
      "          PL       0.73      0.80      0.77        45\n",
      "          CR       0.87      0.73      0.80        45\n",
      "          CG       0.83      0.96      0.89        45\n",
      "          BE       0.90      0.80      0.85        45\n",
      "           N       0.95      0.93      0.94        45\n",
      "\n",
      "    accuracy                           0.86       405\n",
      "   macro avg       0.86      0.86      0.86       405\n",
      "weighted avg       0.86      0.86      0.86       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1140 07:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.735300</td>\n",
       "      <td>0.544562</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.825088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.626387</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.850520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.714460</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.858742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.739759</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.773540</td>\n",
       "      <td>0.861728</td>\n",
       "      <td>0.861835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1140\n",
      "Configuration saved in temp/checkpoint-1140/config.json\n",
      "Model weights saved in temp/checkpoint-1140/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-912 (score: 0.8666666666666667).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 16\n",
      " {'test_loss': 0.7397588491439819, 'test_accuracy': 0.8666666666666667, 'test_macro-f1': 0.8668474353283122, 'test_runtime': 2.9191, 'test_samples_per_second': 138.744, 'test_steps_per_second': 8.907} \n",
      "\n",
      "accuracy 0.8666666666666667\n",
      "macro_f1 score: 0.8668474353283122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.91      0.93      0.92        45\n",
      "          NC       0.91      0.89      0.90        45\n",
      "          PW       0.91      0.91      0.91        45\n",
      "          HC       0.81      0.78      0.80        45\n",
      "          PL       0.80      0.82      0.81        45\n",
      "          CR       0.78      0.78      0.78        45\n",
      "          CG       0.84      0.93      0.88        45\n",
      "          BE       0.84      0.84      0.84        45\n",
      "           N       1.00      0.91      0.95        45\n",
      "\n",
      "    accuracy                           0.87       405\n",
      "   macro avg       0.87      0.87      0.87       405\n",
      "weighted avg       0.87      0.87      0.87       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 06:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.838900</td>\n",
       "      <td>0.542318</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.820524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.255400</td>\n",
       "      <td>0.477961</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.865232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092200</td>\n",
       "      <td>0.580618</td>\n",
       "      <td>0.861728</td>\n",
       "      <td>0.860862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.610449</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.849554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.623253</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.852045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-114\n",
      "Configuration saved in temp/checkpoint-114/config.json\n",
      "Model weights saved in temp/checkpoint-114/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-342\n",
      "Configuration saved in temp/checkpoint-342/config.json\n",
      "Model weights saved in temp/checkpoint-342/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to temp/checkpoint-570\n",
      "Configuration saved in temp/checkpoint-570/config.json\n",
      "Model weights saved in temp/checkpoint-570/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-228 (score: 0.8666666666666667).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32\n",
      " {'test_loss': 0.4779607355594635, 'test_accuracy': 0.8666666666666667, 'test_macro-f1': 0.8652322408856377, 'test_runtime': 2.9231, 'test_samples_per_second': 138.551, 'test_steps_per_second': 4.447} \n",
      "\n",
      "accuracy 0.8666666666666667\n",
      "macro_f1 score: 0.8652322408856377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.83      0.96      0.89        45\n",
      "          NC       0.89      0.91      0.90        45\n",
      "          PW       0.86      0.93      0.89        45\n",
      "          HC       0.85      0.76      0.80        45\n",
      "          PL       0.83      0.78      0.80        45\n",
      "          CR       0.80      0.73      0.77        45\n",
      "          CG       0.88      0.98      0.93        45\n",
      "          BE       0.86      0.84      0.85        45\n",
      "           N       1.00      0.91      0.95        45\n",
      "\n",
      "    accuracy                           0.87       405\n",
      "   macro avg       0.87      0.87      0.87       405\n",
      "weighted avg       0.87      0.87      0.87       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 285\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/285 06:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.034200</td>\n",
       "      <td>0.545420</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.837663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.324400</td>\n",
       "      <td>0.497801</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.842876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>0.537230</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.848911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.578633</td>\n",
       "      <td>0.854321</td>\n",
       "      <td>0.854385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.576193</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.858591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to temp/checkpoint-57\n",
      "Configuration saved in temp/checkpoint-57/config.json\n",
      "Model weights saved in temp/checkpoint-57/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to temp/checkpoint-114\n",
      "Configuration saved in temp/checkpoint-114/config.json\n",
      "Model weights saved in temp/checkpoint-114/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to temp/checkpoint-171\n",
      "Configuration saved in temp/checkpoint-171/config.json\n",
      "Model weights saved in temp/checkpoint-171/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to temp/checkpoint-285\n",
      "Configuration saved in temp/checkpoint-285/config.json\n",
      "Model weights saved in temp/checkpoint-285/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-285 (score: 0.8592592592592593).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      " {'test_loss': 0.5761932730674744, 'test_accuracy': 0.8592592592592593, 'test_macro-f1': 0.8585913082776989, 'test_runtime': 2.933, 'test_samples_per_second': 138.082, 'test_steps_per_second': 2.387} \n",
      "\n",
      "accuracy 0.8592592592592593\n",
      "macro_f1 score: 0.8585913082776989\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.91      0.91      0.91        45\n",
      "          NC       0.89      0.87      0.88        45\n",
      "          PW       0.88      0.93      0.90        45\n",
      "          HC       0.81      0.78      0.80        45\n",
      "          PL       0.79      0.76      0.77        45\n",
      "          CR       0.82      0.80      0.81        45\n",
      "          CG       0.86      0.93      0.89        45\n",
      "          BE       0.84      0.84      0.84        45\n",
      "           N       0.93      0.91      0.92        45\n",
      "\n",
      "    accuracy                           0.86       405\n",
      "   macro avg       0.86      0.86      0.86       405\n",
      "weighted avg       0.86      0.86      0.86       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence. If __index_level_0__, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 145\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e22539a97b20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;31m# generate prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1503\u001b[0m         )\n\u001b[1;32m   1504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1738\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1740\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m                 if (\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2470\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2500\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2502\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2503\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2504\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         )\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         )\n\u001b[1;32m   1030\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m                 )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         )\n\u001b[1;32m    500\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         )\n\u001b[1;32m    432\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;31m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# Mask heads if we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 14.76 GiB total capacity; 13.49 GiB already allocated; 25.75 MiB free; 13.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "batch_size_list = [8, 16, 32, 64]\n",
    "\n",
    "best_batch_size = 0\n",
    "best_acc = 0\n",
    "best_macro_f1 = 0\n",
    "\n",
    "acc_list = list()\n",
    "macro_f1_list = list()\n",
    "\n",
    "for batch_size in batch_size_list:\n",
    "  # reload pre-trained model\n",
    "  model = BertForSequenceClassification.from_pretrained(model_name,num_labels=9)\n",
    "\n",
    "  # set training arguments\n",
    "  args = TrainingArguments(\n",
    "          output_dir = 'temp/',\n",
    "          evaluation_strategy = 'epoch',\n",
    "          save_strategy = 'epoch',\n",
    "          logging_strategy='epoch',\n",
    "          learning_rate=learning_rate,\n",
    "          per_device_train_batch_size=batch_size,\n",
    "          per_device_eval_batch_size=batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "          weight_decay=weight_decay,\n",
    "          load_best_model_at_end=True,\n",
    "          metric_for_best_model='accuracy',\n",
    "  )\n",
    "\n",
    "  # set trainer\n",
    "  trainer = Trainer(\n",
    "          model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "          args=args,                  # training arguments, defined above\n",
    "          train_dataset=dataset_train,         # training dataset\n",
    "          eval_dataset=dataset_val,            # evaluation dataset\n",
    "          compute_metrics=compute_metrics\n",
    "  )\n",
    "\n",
    "  # train model\n",
    "  trainer.train() \n",
    "\n",
    "  # generate prediction\n",
    "  predictions = trainer.predict(dataset_val)\n",
    "  print(f'batch_size: {batch_size}\\n', predictions.metrics, '\\n')\n",
    "  y_pred = np.array([np.argmax(pred) for pred in predictions[0]])\n",
    "\n",
    "  # evaluate model performance\n",
    "  acc = accuracy_score(y_pred, predictions[1])\n",
    "  f1 = f1_score(predictions[1], y_pred, average='macro')\n",
    "  print(f'accuracy {acc}')\n",
    "  print(f'macro_f1 score: {f1}')\n",
    "  print(classification_report(predictions[1], y_pred,target_names=labels))\n",
    "  print('------------------------------------------------------------------------------------------------------') \n",
    "  acc_list.append(acc)\n",
    "  macro_f1_list.append(f1) \n",
    "\n",
    "  # save best model hyperparameter\n",
    "  if best_acc < acc:\n",
    "    best_acc = acc\n",
    "    best_macro_f1 = f1\n",
    "    best_batch_size = batch_size\n",
    "\n",
    "print()\n",
    "for i in range(len(batch_size_list)):\n",
    "  print(f'batch_size: {batch_size_list[i]}, acc: {acc_list[i]}, macro-f1: {macro_f1_list[i]}')\n",
    "print()\n",
    "print(f'Best Model: batch_size = {best_batch_size}, acc = {best_acc}, macro-f1 = {best_macro_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYQOoq4UgmjO"
   },
   "outputs": [],
   "source": [
    "# though memory error occurred, best model is batch_size=16, with acc=0.86667, and f1=0.866847\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP90JCr_WLMm"
   },
   "source": [
    "### weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Cu4QZleMWHan",
    "outputId": "10460221-56aa-44a4-f5a2-3ca79007bf2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1140 07:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.725200</td>\n",
       "      <td>0.509731</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.835360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.607128</td>\n",
       "      <td>0.869136</td>\n",
       "      <td>0.867540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.074300</td>\n",
       "      <td>0.725732</td>\n",
       "      <td>0.874074</td>\n",
       "      <td>0.873662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.739249</td>\n",
       "      <td>0.861728</td>\n",
       "      <td>0.861387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.764860</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.864037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1140\n",
      "Configuration saved in temp/checkpoint-1140/config.json\n",
      "Model weights saved in temp/checkpoint-1140/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-684 (score: 0.8740740740740741).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay: 0\n",
      " {'test_loss': 0.725732147693634, 'test_accuracy': 0.8740740740740741, 'test_macro-f1': 0.873661697273395, 'test_runtime': 2.9731, 'test_samples_per_second': 136.22, 'test_steps_per_second': 8.745} \n",
      "\n",
      "accuracy 0.8740740740740741\n",
      "macro_f1 score: 0.873661697273395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.91      0.91      0.91        45\n",
      "          NC       0.91      0.87      0.89        45\n",
      "          PW       0.89      0.91      0.90        45\n",
      "          HC       0.84      0.84      0.84        45\n",
      "          PL       0.80      0.89      0.84        45\n",
      "          CR       0.82      0.80      0.81        45\n",
      "          CG       0.81      0.96      0.88        45\n",
      "          BE       0.94      0.73      0.83        45\n",
      "           N       0.98      0.96      0.97        45\n",
      "\n",
      "    accuracy                           0.87       405\n",
      "   macro avg       0.88      0.87      0.87       405\n",
      "weighted avg       0.88      0.87      0.87       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1140 07:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.735300</td>\n",
       "      <td>0.544562</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.825088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.626387</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.850520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.714460</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.858742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.739759</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.773540</td>\n",
       "      <td>0.861728</td>\n",
       "      <td>0.861835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1140\n",
      "Configuration saved in temp/checkpoint-1140/config.json\n",
      "Model weights saved in temp/checkpoint-1140/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-912 (score: 0.8666666666666667).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay: 0.0001\n",
      " {'test_loss': 0.7397588491439819, 'test_accuracy': 0.8666666666666667, 'test_macro-f1': 0.8668474353283122, 'test_runtime': 2.9403, 'test_samples_per_second': 137.743, 'test_steps_per_second': 8.843} \n",
      "\n",
      "accuracy 0.8666666666666667\n",
      "macro_f1 score: 0.8668474353283122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.91      0.93      0.92        45\n",
      "          NC       0.91      0.89      0.90        45\n",
      "          PW       0.91      0.91      0.91        45\n",
      "          HC       0.81      0.78      0.80        45\n",
      "          PL       0.80      0.82      0.81        45\n",
      "          CR       0.78      0.78      0.78        45\n",
      "          CG       0.84      0.93      0.88        45\n",
      "          BE       0.84      0.84      0.84        45\n",
      "           N       1.00      0.91      0.95        45\n",
      "\n",
      "    accuracy                           0.87       405\n",
      "   macro avg       0.87      0.87      0.87       405\n",
      "weighted avg       0.87      0.87      0.87       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1140 07:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.735300</td>\n",
       "      <td>0.544562</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.825088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.626386</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.850520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.714481</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.858742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.740799</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.774360</td>\n",
       "      <td>0.861728</td>\n",
       "      <td>0.861835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1140\n",
      "Configuration saved in temp/checkpoint-1140/config.json\n",
      "Model weights saved in temp/checkpoint-1140/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-912 (score: 0.8666666666666667).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay: 0.001\n",
      " {'test_loss': 0.740799069404602, 'test_accuracy': 0.8666666666666667, 'test_macro-f1': 0.8668474353283122, 'test_runtime': 2.9346, 'test_samples_per_second': 138.009, 'test_steps_per_second': 8.86} \n",
      "\n",
      "accuracy 0.8666666666666667\n",
      "macro_f1 score: 0.8668474353283122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.91      0.93      0.92        45\n",
      "          NC       0.91      0.89      0.90        45\n",
      "          PW       0.91      0.91      0.91        45\n",
      "          HC       0.81      0.78      0.80        45\n",
      "          PL       0.80      0.82      0.81        45\n",
      "          CR       0.78      0.78      0.78        45\n",
      "          CG       0.84      0.93      0.88        45\n",
      "          BE       0.84      0.84      0.84        45\n",
      "           N       1.00      0.91      0.95        45\n",
      "\n",
      "    accuracy                           0.87       405\n",
      "   macro avg       0.87      0.87      0.87       405\n",
      "weighted avg       0.87      0.87      0.87       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1140 07:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.735300</td>\n",
       "      <td>0.544558</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.825088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.626357</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.850520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.714675</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.858742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.745680</td>\n",
       "      <td>0.869136</td>\n",
       "      <td>0.869335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.778457</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.856965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1140\n",
      "Configuration saved in temp/checkpoint-1140/config.json\n",
      "Model weights saved in temp/checkpoint-1140/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-912 (score: 0.8691358024691358).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay: 0.01\n",
      " {'test_loss': 0.7456800937652588, 'test_accuracy': 0.8691358024691358, 'test_macro-f1': 0.8693350223461059, 'test_runtime': 2.9611, 'test_samples_per_second': 136.773, 'test_steps_per_second': 8.78} \n",
      "\n",
      "accuracy 0.8691358024691358\n",
      "macro_f1 score: 0.8693350223461059\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.91      0.93      0.92        45\n",
      "          NC       0.91      0.89      0.90        45\n",
      "          PW       0.91      0.91      0.91        45\n",
      "          HC       0.81      0.78      0.80        45\n",
      "          PL       0.81      0.84      0.83        45\n",
      "          CR       0.78      0.78      0.78        45\n",
      "          CG       0.84      0.93      0.88        45\n",
      "          BE       0.86      0.84      0.85        45\n",
      "           N       1.00      0.91      0.95        45\n",
      "\n",
      "    accuracy                           0.87       405\n",
      "   macro avg       0.87      0.87      0.87       405\n",
      "weighted avg       0.87      0.87      0.87       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1140 07:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.735400</td>\n",
       "      <td>0.544503</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.822471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.625975</td>\n",
       "      <td>0.854321</td>\n",
       "      <td>0.853171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.719191</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.856206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.750126</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.776785</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.859499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1140\n",
      "Configuration saved in temp/checkpoint-1140/config.json\n",
      "Model weights saved in temp/checkpoint-1140/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-912 (score: 0.8666666666666667).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay: 0.1\n",
      " {'test_loss': 0.750126302242279, 'test_accuracy': 0.8666666666666667, 'test_macro-f1': 0.8669672443839759, 'test_runtime': 2.977, 'test_samples_per_second': 136.044, 'test_steps_per_second': 8.734} \n",
      "\n",
      "accuracy 0.8666666666666667\n",
      "macro_f1 score: 0.8669672443839759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.91      0.93      0.92        45\n",
      "          NC       0.91      0.89      0.90        45\n",
      "          PW       0.91      0.91      0.91        45\n",
      "          HC       0.83      0.78      0.80        45\n",
      "          PL       0.79      0.84      0.82        45\n",
      "          CR       0.76      0.78      0.77        45\n",
      "          CG       0.84      0.93      0.88        45\n",
      "          BE       0.86      0.82      0.84        45\n",
      "           N       1.00      0.91      0.95        45\n",
      "\n",
      "    accuracy                           0.87       405\n",
      "   macro avg       0.87      0.87      0.87       405\n",
      "weighted avg       0.87      0.87      0.87       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n",
      "\n",
      "weight_decay: 0, acc: 0.8740740740740741, macro-f1: 0.873661697273395\n",
      "weight_decay: 0.0001, acc: 0.8666666666666667, macro-f1: 0.8668474353283122\n",
      "weight_decay: 0.001, acc: 0.8666666666666667, macro-f1: 0.8668474353283122\n",
      "weight_decay: 0.01, acc: 0.8691358024691358, macro-f1: 0.8693350223461059\n",
      "weight_decay: 0.1, acc: 0.8666666666666667, macro-f1: 0.8669672443839759\n",
      "\n",
      "Best Model: weight_decay = 0, acc = 0.8740740740740741, macro-f1 = 0\n"
     ]
    }
   ],
   "source": [
    "weight_decay_list = [0, 0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "best_weight_decay = 0\n",
    "best_acc = 0\n",
    "best_macro_f1 = 0\n",
    "\n",
    "acc_list = list()\n",
    "macro_f1_list = list()\n",
    "\n",
    "for weight_decay in weight_decay_list:\n",
    "  # reload pre-trained model\n",
    "  model = BertForSequenceClassification.from_pretrained(model_name,num_labels=9)\n",
    "\n",
    "  # set training arguments\n",
    "  args = TrainingArguments(\n",
    "          output_dir = 'temp/',\n",
    "          evaluation_strategy = 'epoch',\n",
    "          save_strategy = 'epoch',\n",
    "          logging_strategy='epoch',\n",
    "          learning_rate=learning_rate,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "          weight_decay=weight_decay,\n",
    "          load_best_model_at_end=True,\n",
    "          metric_for_best_model='accuracy',\n",
    "  )\n",
    "\n",
    "  # set trainer\n",
    "  trainer = Trainer(\n",
    "          model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "          args=args,                  # training arguments, defined above\n",
    "          train_dataset=dataset_train,         # training dataset\n",
    "          eval_dataset=dataset_val,            # evaluation dataset\n",
    "          compute_metrics=compute_metrics\n",
    "  )\n",
    "\n",
    "  # train model\n",
    "  trainer.train() \n",
    "\n",
    "  # generate prediction\n",
    "  predictions = trainer.predict(dataset_val)\n",
    "  print(f'weight_decay: {weight_decay}\\n', predictions.metrics, '\\n')\n",
    "  y_pred = np.array([np.argmax(pred) for pred in predictions[0]])\n",
    "\n",
    "  # evaluate model performance\n",
    "  acc = accuracy_score(y_pred, predictions[1])\n",
    "  f1 = f1_score(predictions[1], y_pred, average='macro')\n",
    "  print(f'accuracy {acc}')\n",
    "  print(f'macro_f1 score: {f1}')\n",
    "  print(classification_report(predictions[1], y_pred,target_names=labels))\n",
    "  print('------------------------------------------------------------------------------------------------------') \n",
    "  acc_list.append(acc)\n",
    "  macro_f1_list.append(f1)   \n",
    "\n",
    "  # save best model hyperparameter\n",
    "  if best_acc < acc:\n",
    "    best_acc = acc\n",
    "    best_macro_f1 = f1\n",
    "    best_weight_decay = weight_decay\n",
    "\n",
    "print()\n",
    "for i in range(len(weight_decay_list)):\n",
    "  print(f'weight_decay: {weight_decay_list[i]}, acc: {acc_list[i]}, macro-f1: {macro_f1_list[i]}')\n",
    "print()\n",
    "print(f'Best Model: weight_decay = {best_weight_decay}, acc = {best_acc}, macro-f1 = {best_macro_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31x76OUhWHcv"
   },
   "outputs": [],
   "source": [
    "weight_decay = best_weight_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz74tPw0WvPK"
   },
   "source": [
    "### num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nsG7V7GLWzxQ",
    "outputId": "0612f4ee-515f-4c61-b410-7f9261839728"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='684' max='684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [684/684 04:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.754400</td>\n",
       "      <td>0.527542</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.823767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.561514</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.851546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.615290</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.858800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-684 (score: 0.8592592592592593).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_epochs: 3\n",
      " {'test_loss': 0.6152901649475098, 'test_accuracy': 0.8592592592592593, 'test_macro-f1': 0.858799506193225, 'test_runtime': 3.1432, 'test_samples_per_second': 128.85, 'test_steps_per_second': 8.272} \n",
      "\n",
      "accuracy 0.8592592592592593\n",
      "macro_f1 score: 0.858799506193225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.91      0.91      0.91        45\n",
      "          NC       0.85      0.87      0.86        45\n",
      "          PW       0.87      0.91      0.89        45\n",
      "          HC       0.82      0.80      0.81        45\n",
      "          PL       0.80      0.78      0.79        45\n",
      "          CR       0.81      0.76      0.78        45\n",
      "          CG       0.89      0.93      0.91        45\n",
      "          BE       0.80      0.82      0.81        45\n",
      "           N       0.98      0.96      0.97        45\n",
      "\n",
      "    accuracy                           0.86       405\n",
      "   macro avg       0.86      0.86      0.86       405\n",
      "weighted avg       0.86      0.86      0.86       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='912' max='912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [912/912 05:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.730800</td>\n",
       "      <td>0.520627</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.828703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.551044</td>\n",
       "      <td>0.861728</td>\n",
       "      <td>0.861146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.695991</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.856060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.693469</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.849242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-456 (score: 0.8617283950617284).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_epochs: 4\n",
      " {'test_loss': 0.5510438084602356, 'test_accuracy': 0.8617283950617284, 'test_macro-f1': 0.8611460258390083, 'test_runtime': 3.0145, 'test_samples_per_second': 134.352, 'test_steps_per_second': 8.625} \n",
      "\n",
      "accuracy 0.8617283950617284\n",
      "macro_f1 score: 0.8611460258390083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.91      0.91      0.91        45\n",
      "          NC       0.91      0.87      0.89        45\n",
      "          PW       0.86      0.96      0.91        45\n",
      "          HC       0.76      0.87      0.81        45\n",
      "          PL       0.87      0.76      0.81        45\n",
      "          CR       0.80      0.80      0.80        45\n",
      "          CG       0.84      0.93      0.88        45\n",
      "          BE       0.87      0.76      0.81        45\n",
      "           N       0.95      0.91      0.93        45\n",
      "\n",
      "    accuracy                           0.86       405\n",
      "   macro avg       0.86      0.86      0.86       405\n",
      "weighted avg       0.86      0.86      0.86       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1140 07:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.732200</td>\n",
       "      <td>0.551671</td>\n",
       "      <td>0.819753</td>\n",
       "      <td>0.819106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.229800</td>\n",
       "      <td>0.643182</td>\n",
       "      <td>0.854321</td>\n",
       "      <td>0.853126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.745165</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.863617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.796835</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.855783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.822451</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.856269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1140\n",
      "Configuration saved in temp/checkpoint-1140/config.json\n",
      "Model weights saved in temp/checkpoint-1140/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-684 (score: 0.8641975308641975).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_epochs: 5\n",
      " {'test_loss': 0.7451650500297546, 'test_accuracy': 0.8641975308641975, 'test_macro-f1': 0.8636166878617147, 'test_runtime': 2.9977, 'test_samples_per_second': 135.104, 'test_steps_per_second': 8.673} \n",
      "\n",
      "accuracy 0.8641975308641975\n",
      "macro_f1 score: 0.8636166878617147\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.89      0.91      0.90        45\n",
      "          NC       0.86      0.84      0.85        45\n",
      "          PW       0.89      0.89      0.89        45\n",
      "          HC       0.79      0.82      0.80        45\n",
      "          PL       0.82      0.82      0.82        45\n",
      "          CR       0.81      0.78      0.80        45\n",
      "          CG       0.88      0.93      0.90        45\n",
      "          BE       0.92      0.80      0.86        45\n",
      "           N       0.92      0.98      0.95        45\n",
      "\n",
      "    accuracy                           0.86       405\n",
      "   macro avg       0.86      0.86      0.86       405\n",
      "weighted avg       0.86      0.86      0.86       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1368\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1368' max='1368' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1368/1368 08:49, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.735600</td>\n",
       "      <td>0.549178</td>\n",
       "      <td>0.817284</td>\n",
       "      <td>0.817363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.238800</td>\n",
       "      <td>0.625005</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.851040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.077400</td>\n",
       "      <td>0.693817</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.863955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.787160</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.864440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.828920</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.823678</td>\n",
       "      <td>0.871605</td>\n",
       "      <td>0.871687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1140\n",
      "Configuration saved in temp/checkpoint-1140/config.json\n",
      "Model weights saved in temp/checkpoint-1140/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1368\n",
      "Configuration saved in temp/checkpoint-1368/config.json\n",
      "Model weights saved in temp/checkpoint-1368/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-1368 (score: 0.8716049382716049).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_epochs: 6\n",
      " {'test_loss': 0.8236783146858215, 'test_accuracy': 0.8716049382716049, 'test_macro-f1': 0.8716868261275657, 'test_runtime': 3.0004, 'test_samples_per_second': 134.981, 'test_steps_per_second': 8.665} \n",
      "\n",
      "accuracy 0.8716049382716049\n",
      "macro_f1 score: 0.8716868261275657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.89      0.93      0.91        45\n",
      "          NC       0.87      0.91      0.89        45\n",
      "          PW       0.91      0.91      0.91        45\n",
      "          HC       0.80      0.80      0.80        45\n",
      "          PL       0.76      0.84      0.80        45\n",
      "          CR       0.85      0.76      0.80        45\n",
      "          CG       0.86      0.93      0.89        45\n",
      "          BE       0.93      0.82      0.87        45\n",
      "           N       1.00      0.93      0.97        45\n",
      "\n",
      "    accuracy                           0.87       405\n",
      "   macro avg       0.87      0.87      0.87       405\n",
      "weighted avg       0.87      0.87      0.87       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1596\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1596' max='1596' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1596/1596 10:19, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.738600</td>\n",
       "      <td>0.531363</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.844713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.547874</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.850993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.711663</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.863051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.824649</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.856920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.843759</td>\n",
       "      <td>0.861728</td>\n",
       "      <td>0.861053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.844743</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.856403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.846708</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.851500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1140\n",
      "Configuration saved in temp/checkpoint-1140/config.json\n",
      "Model weights saved in temp/checkpoint-1140/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1368\n",
      "Configuration saved in temp/checkpoint-1368/config.json\n",
      "Model weights saved in temp/checkpoint-1368/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1596\n",
      "Configuration saved in temp/checkpoint-1596/config.json\n",
      "Model weights saved in temp/checkpoint-1596/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-684 (score: 0.8641975308641975).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_epochs: 7\n",
      " {'test_loss': 0.7116634249687195, 'test_accuracy': 0.8641975308641975, 'test_macro-f1': 0.863050898296694, 'test_runtime': 3.0015, 'test_samples_per_second': 134.932, 'test_steps_per_second': 8.662} \n",
      "\n",
      "accuracy 0.8641975308641975\n",
      "macro_f1 score: 0.863050898296694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.88      0.93      0.90        45\n",
      "          NC       0.93      0.87      0.90        45\n",
      "          PW       0.89      0.89      0.89        45\n",
      "          HC       0.76      0.82      0.79        45\n",
      "          PL       0.84      0.82      0.83        45\n",
      "          CR       0.86      0.69      0.77        45\n",
      "          CG       0.84      0.93      0.88        45\n",
      "          BE       0.86      0.84      0.85        45\n",
      "           N       0.94      0.98      0.96        45\n",
      "\n",
      "    accuracy                           0.86       405\n",
      "   macro avg       0.87      0.86      0.86       405\n",
      "weighted avg       0.87      0.86      0.86       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1824\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1824' max='1824' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1824/1824 11:48, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.514962</td>\n",
       "      <td>0.846914</td>\n",
       "      <td>0.846988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.246600</td>\n",
       "      <td>0.581957</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.859096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.729848</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.859987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.822255</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.864253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.845877</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.856031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.893967</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.848845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.891131</td>\n",
       "      <td>0.854321</td>\n",
       "      <td>0.854016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.894380</td>\n",
       "      <td>0.854321</td>\n",
       "      <td>0.854016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1140\n",
      "Configuration saved in temp/checkpoint-1140/config.json\n",
      "Model weights saved in temp/checkpoint-1140/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1368\n",
      "Configuration saved in temp/checkpoint-1368/config.json\n",
      "Model weights saved in temp/checkpoint-1368/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1596\n",
      "Configuration saved in temp/checkpoint-1596/config.json\n",
      "Model weights saved in temp/checkpoint-1596/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1824\n",
      "Configuration saved in temp/checkpoint-1824/config.json\n",
      "Model weights saved in temp/checkpoint-1824/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-912 (score: 0.8641975308641975).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_epochs: 8\n",
      " {'test_loss': 0.8222550749778748, 'test_accuracy': 0.8641975308641975, 'test_macro-f1': 0.8642534837138766, 'test_runtime': 3.0043, 'test_samples_per_second': 134.806, 'test_steps_per_second': 8.654} \n",
      "\n",
      "accuracy 0.8641975308641975\n",
      "macro_f1 score: 0.8642534837138766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.86      0.93      0.89        45\n",
      "          NC       0.93      0.87      0.90        45\n",
      "          PW       0.91      0.87      0.89        45\n",
      "          HC       0.83      0.84      0.84        45\n",
      "          PL       0.78      0.80      0.79        45\n",
      "          CR       0.81      0.76      0.78        45\n",
      "          CG       0.84      0.96      0.90        45\n",
      "          BE       0.84      0.84      0.84        45\n",
      "           N       1.00      0.91      0.95        45\n",
      "\n",
      "    accuracy                           0.86       405\n",
      "   macro avg       0.87      0.86      0.86       405\n",
      "weighted avg       0.87      0.86      0.86       405\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n",
      "\n",
      "num_train_epochs: 3, acc: 0.8592592592592593, macro-f1: 0.858799506193225\n",
      "num_train_epochs: 4, acc: 0.8617283950617284, macro-f1: 0.8611460258390083\n",
      "num_train_epochs: 5, acc: 0.8641975308641975, macro-f1: 0.8636166878617147\n",
      "num_train_epochs: 6, acc: 0.8716049382716049, macro-f1: 0.8716868261275657\n",
      "num_train_epochs: 7, acc: 0.8641975308641975, macro-f1: 0.863050898296694\n",
      "num_train_epochs: 8, acc: 0.8641975308641975, macro-f1: 0.8642534837138766\n",
      "\n",
      "Best Model: num_train_epochs = 6, acc = 0.8716049382716049, macro-f1 = 0.8716868261275657\n"
     ]
    }
   ],
   "source": [
    "num_train_epochs_list = [3, 4, 5, 6, 7, 8]\n",
    "\n",
    "best_num_train_epochs = 0\n",
    "best_acc = 0\n",
    "best_macro_f1 = 0\n",
    "\n",
    "acc_list = list()\n",
    "macro_f1_list = list()\n",
    "\n",
    "for num_train_epochs in num_train_epochs_list:\n",
    "  # reload pre-trained model\n",
    "  model = BertForSequenceClassification.from_pretrained(model_name,num_labels=9)\n",
    "\n",
    "  # set training arguments\n",
    "  args = TrainingArguments(\n",
    "          output_dir = 'temp/',\n",
    "          evaluation_strategy = 'epoch',\n",
    "          save_strategy = 'epoch',\n",
    "          logging_strategy='epoch',\n",
    "          learning_rate=learning_rate,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "          weight_decay=weight_decay,\n",
    "          load_best_model_at_end=True,\n",
    "          metric_for_best_model='accuracy',\n",
    "  )\n",
    "\n",
    "  # set trainer\n",
    "  trainer = Trainer(\n",
    "          model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "          args=args,                  # training arguments, defined above\n",
    "          train_dataset=dataset_train,         # training dataset\n",
    "          eval_dataset=dataset_val,            # evaluation dataset\n",
    "          compute_metrics=compute_metrics\n",
    "  )\n",
    "\n",
    "  # train model\n",
    "  trainer.train() \n",
    "\n",
    "  # generate prediction\n",
    "  predictions = trainer.predict(dataset_val)\n",
    "  print(f'num_train_epochs: {num_train_epochs}\\n', predictions.metrics, '\\n')\n",
    "  y_pred = np.array([np.argmax(pred) for pred in predictions[0]])\n",
    "\n",
    "  # evaluate model performance\n",
    "  acc = accuracy_score(y_pred, predictions[1])\n",
    "  f1 = f1_score(predictions[1], y_pred, average='macro')\n",
    "  print(f'accuracy {acc}')\n",
    "  print(f'macro_f1 score: {f1}')\n",
    "  print(classification_report(predictions[1], y_pred,target_names=labels))\n",
    "  print('------------------------------------------------------------------------------------------------------')\n",
    "  acc_list.append(acc)\n",
    "  macro_f1_list.append(f1)  \n",
    "\n",
    "  # save best model hyperparameter\n",
    "  if best_acc < acc:\n",
    "    best_acc = acc\n",
    "    best_macro_f1 = f1\n",
    "    best_num_train_epochs = num_train_epochs\n",
    "\n",
    "print()\n",
    "for i in range(len(num_train_epochs_list)):\n",
    "  print(f'num_train_epochs: {num_train_epochs_list[i]}, acc: {acc_list[i]}, macro-f1: {macro_f1_list[i]}')\n",
    "print()\n",
    "print(f'Best Model: num_train_epochs = {best_num_train_epochs}, acc = {best_acc}, macro-f1 = {best_macro_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "990GhUo3Wz02"
   },
   "outputs": [],
   "source": [
    "num_train_epochs = best_num_train_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVQUmYSdW1ER"
   },
   "source": [
    "## **Final Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "CRRwdDNsk9yU"
   },
   "outputs": [],
   "source": [
    "# best hyperparameters values\n",
    "learning_rate = 5e-5\n",
    "per_device_train_batch_size = per_device_eval_batch_size = 16\n",
    "num_train_epochs = 6\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v6YMtle6AnDr",
    "outputId": "da92233c-92a7-42f7-8fbf-a2a03a3885b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/52a95139f4c3367976e7045d2f955bf82242a485e8506539b49838ca3c825654.ddd3650c75e2551777ba1291415522ee7288b429a28ea24681ddf54025802027\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30873\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/yiyanghkust/finbert-pretrain/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f0f259830c52369dfd874be436f2f8106ff53d563cd04462ecf2cafe5ec8c4c7.587885ce9bf0920100db493163208c850d88adf23165c8f8824693457e6d404e\n",
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3645\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1368\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1368' max='1368' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1368/1368 08:50, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.716200</td>\n",
       "      <td>0.599196</td>\n",
       "      <td>0.817284</td>\n",
       "      <td>0.815531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>0.569624</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.858186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.700161</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.855414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.693320</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.864312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.725072</td>\n",
       "      <td>0.861728</td>\n",
       "      <td>0.861639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.755248</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.857633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-228\n",
      "Configuration saved in temp/checkpoint-228/config.json\n",
      "Model weights saved in temp/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-456\n",
      "Configuration saved in temp/checkpoint-456/config.json\n",
      "Model weights saved in temp/checkpoint-456/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-684\n",
      "Configuration saved in temp/checkpoint-684/config.json\n",
      "Model weights saved in temp/checkpoint-684/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-912\n",
      "Configuration saved in temp/checkpoint-912/config.json\n",
      "Model weights saved in temp/checkpoint-912/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1140\n",
      "Configuration saved in temp/checkpoint-1140/config.json\n",
      "Model weights saved in temp/checkpoint-1140/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to temp/checkpoint-1368\n",
      "Configuration saved in temp/checkpoint-1368/config.json\n",
      "Model weights saved in temp/checkpoint-1368/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-912 (score: 0.8641975308641975).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 405\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_epochs: 6\n",
      " {'test_loss': 0.6933199763298035, 'test_accuracy': 0.8641975308641975, 'test_macro-f1': 0.8643115933603842, 'test_runtime': 2.9855, 'test_samples_per_second': 135.655, 'test_steps_per_second': 8.709} \n",
      "\n",
      "accuracy 0.8641975308641975\n",
      "macro_f1 score: 0.8643115933603842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.89      0.91      0.90        45\n",
      "          NC       0.93      0.87      0.90        45\n",
      "          PW       0.89      0.89      0.89        45\n",
      "          HC       0.77      0.82      0.80        45\n",
      "          PL       0.85      0.87      0.86        45\n",
      "          CR       0.77      0.76      0.76        45\n",
      "          CG       0.86      0.96      0.91        45\n",
      "          BE       0.86      0.80      0.83        45\n",
      "           N       0.98      0.91      0.94        45\n",
      "\n",
      "    accuracy                           0.86       405\n",
      "   macro avg       0.87      0.86      0.86       405\n",
      "weighted avg       0.87      0.86      0.86       405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reload pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name,num_labels=9)\n",
    "\n",
    "# set training arguments\n",
    "args = TrainingArguments(\n",
    "        output_dir = 'temp/',\n",
    "        evaluation_strategy = 'epoch',\n",
    "        save_strategy = 'epoch',\n",
    "        logging_strategy='epoch',\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    ")\n",
    "\n",
    "# set trainer\n",
    "trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=args,                  # training arguments, defined above\n",
    "        train_dataset=dataset_train,         # training dataset\n",
    "        eval_dataset=dataset_val,            # evaluation dataset\n",
    "        compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.train() \n",
    "\n",
    "# generate validation set prediction\n",
    "predictions = trainer.predict(dataset_val)\n",
    "print(f'num_train_epochs: {num_train_epochs}\\n', predictions.metrics, '\\n')\n",
    "y_pred = np.array([np.argmax(pred) for pred in predictions[0]])\n",
    "\n",
    "# evaluate model performance on validation set\n",
    "acc = accuracy_score(y_pred, predictions[1])\n",
    "f1 = f1_score(predictions[1], y_pred, average='macro')\n",
    "print(f'accuracy {acc}')\n",
    "print(f'macro_f1 score: {f1}')\n",
    "print(classification_report(predictions[1], y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xACsnCLNQH6K"
   },
   "source": [
    "## **Prediction on Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "0PRjMoe8N_iL",
    "outputId": "8763cb9f-dd89-4a2d-d437-92fbc83b21fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__. If sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 450\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8977777777777778\n",
      "macro_f1 score: 0.8980669013686238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CC       0.89      0.98      0.93        50\n",
      "          NC       0.90      0.94      0.92        50\n",
      "          PW       0.94      0.90      0.92        50\n",
      "          HC       0.90      0.86      0.88        50\n",
      "          PL       0.84      0.86      0.85        50\n",
      "          CR       0.93      0.86      0.90        50\n",
      "          CG       0.98      0.88      0.93        50\n",
      "          BE       0.80      0.90      0.85        50\n",
      "           N       0.92      0.90      0.91        50\n",
      "\n",
      "    accuracy                           0.90       450\n",
      "   macro avg       0.90      0.90      0.90       450\n",
      "weighted avg       0.90      0.90      0.90       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate prediction\n",
    "predictions = trainer.predict(dataset_test)\n",
    "y_pred = np.array([np.argmax(pred) for pred in predictions[0]])\n",
    "\n",
    "# evaluate model performance\n",
    "f1 = f1_score(predictions[1], y_pred, average='macro')\n",
    "print('accuracy %s' % accuracy_score(y_pred, predictions[1]))\n",
    "print(f'macro_f1 score: {f1}')\n",
    "print(classification_report(predictions[1], y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "3Js9jZFHOHVr",
    "outputId": "43df9fd7-6b03-444c-cb78-6e27173d6fef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ff9454c98d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot confusion matrix\n",
    "cm = confusion_matrix(predictions[1], y_pred , normalize='pred')\n",
    "cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cmp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3jYSNQe-imW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "UROP_finbert_finetune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "047369990f1a4fc9844718486460a200": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b117cdba02cf44409a9b401f21c3a336",
       "IPY_MODEL_b35e16297d664f728538ed769e69bb09",
       "IPY_MODEL_3e04da375d82459aba41ab7dc52ed37d"
      ],
      "layout": "IPY_MODEL_d5fc29e255f44e948e7e7963fc0d7049"
     }
    },
    "0a1acf5d901746bcbaabe2a79e719785": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "116ae829e6894877baa4c955d7e68d74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "138080dd5bd241cfad5a7b793ba35e9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "15d526c2547344e9960af67106128c74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "174b8dc9c91d4be2a14ab9855f1b94f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17b4ebdfd5b44cfba12db5d23d0f7116": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1857e6d6c98a4811a37411ec8da59012": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a0650735182482b9c85a8b0635eb0b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "241f216b860e484da93c158bb773d1cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "294ed8517bbc41469739a2a953f01a99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b18ef91d44f24a55a9b9d91c6484a578",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ddd05d64f67946549441be1cc0c6de3c",
      "value": " 1/1 [00:00&lt;00:00,  2.33ba/s]"
     }
    },
    "2be10749b03a4a74a62e83428daf5e36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3275dfc58b3643cd8af7bba5918a0e4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "339a7edf9f964f68ad470a5dd8c9c87f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a69f97661dca4ee58fe9ba87f5179562",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8049e9b7b2b444a6983d5f07d67ae182",
      "value": 1
     }
    },
    "3ab9bebdb83b41b5ab5bbcd451b6f8fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f4ab46d2e5d405a835c69dc75ee72b6",
       "IPY_MODEL_c106b02e4a8243cab51fa8b3583482a5",
       "IPY_MODEL_5bad19119a98492daf93ae2dc1f7a76d"
      ],
      "layout": "IPY_MODEL_9058a75c97ee40ac9e030e9e663fcc3a"
     }
    },
    "3b00b8b52a844a41ace99f163a4a88d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c191fd3223346d89cbd883027a18c5f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4b80ce4d44214e8389ea517c75eba7d0",
      "value": " 1/1 [00:00&lt;00:00,  2.60ba/s]"
     }
    },
    "3df9905d42bd45a18eb292ce1a6c3409": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1857e6d6c98a4811a37411ec8da59012",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2be10749b03a4a74a62e83428daf5e36",
      "value": " 4/4 [00:03&lt;00:00,  1.36ba/s]"
     }
    },
    "3e04da375d82459aba41ab7dc52ed37d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40b01628efb04b34bf94c9f54cfc1822",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f9c1b9b34f0a40139f4f5e2c3bff77b4",
      "value": " 221k/221k [00:00&lt;00:00, 255kB/s]"
     }
    },
    "40b01628efb04b34bf94c9f54cfc1822": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4539e98f920f4870bd783e8f94ec0032": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dc88c00f01c34b65a0e5f1e78ad436a2",
       "IPY_MODEL_d7d86bede479489db6d90a5e7c954413",
       "IPY_MODEL_3df9905d42bd45a18eb292ce1a6c3409"
      ],
      "layout": "IPY_MODEL_3275dfc58b3643cd8af7bba5918a0e4d"
     }
    },
    "4b80ce4d44214e8389ea517c75eba7d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52a82004a3d343c299fa6bc9061f076b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75861f5053fc4703a6cee058848daa74",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_dab0b987427a49278b7e3878c39965f0",
      "value": " 359/359 [00:00&lt;00:00, 3.35kB/s]"
     }
    },
    "5bad19119a98492daf93ae2dc1f7a76d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de194c8ba40d459197f1374a24dfaa3b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f6b220e1bbfe48d78a93551151b6b1f2",
      "value": " 421M/421M [00:11&lt;00:00, 28.0MB/s]"
     }
    },
    "5c191fd3223346d89cbd883027a18c5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60b800f860674dd89d7c0fe9e5abac52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62d9547882fb469c80b95642c14da5fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6877534a9d6f4137981e9344ce34e2f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c79b7e30e934869aefe34d2e20decd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62d9547882fb469c80b95642c14da5fa",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b568bb1e17d04f3daca49c1b4205756e",
      "value": "Downloading config.json: 100%"
     }
    },
    "6ce1471fcc4f48b49f32a4df740fdc33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72a8b0d08c8e4498997ddfa908d9643f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75861f5053fc4703a6cee058848daa74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8049e9b7b2b444a6983d5f07d67ae182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8f4ab46d2e5d405a835c69dc75ee72b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_116ae829e6894877baa4c955d7e68d74",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_241f216b860e484da93c158bb773d1cb",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "9058a75c97ee40ac9e030e9e663fcc3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a48d76d3284c4c1a90db79f846afc40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6c79b7e30e934869aefe34d2e20decd5",
       "IPY_MODEL_b9c4197e248f4a3f94a7b7bba758ef80",
       "IPY_MODEL_52a82004a3d343c299fa6bc9061f076b"
      ],
      "layout": "IPY_MODEL_0a1acf5d901746bcbaabe2a79e719785"
     }
    },
    "a69f97661dca4ee58fe9ba87f5179562": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8aa7c7824a8422086d2ea59696cf465": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab49c6c16dd34fe0b0e6c977f0cbf10c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6877534a9d6f4137981e9344ce34e2f0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f5a8a95652ec4819bcadba10f60ad514",
      "value": "100%"
     }
    },
    "b117cdba02cf44409a9b401f21c3a336": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f45a9ecff5344c5a94a07f98e880fb0f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ff631c22406c4ebd92d0b63f2b94ce84",
      "value": "Downloading vocab.txt: 100%"
     }
    },
    "b18ef91d44f24a55a9b9d91c6484a578": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b29de32cc0e54d32a15dcb2b937cf5d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b35e16297d664f728538ed769e69bb09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72a8b0d08c8e4498997ddfa908d9643f",
      "max": 226122,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3bb504ae1904a0699a5b1aa62d4879a",
      "value": 226122
     }
    },
    "b4a2f52f379240fc9c278ddf18db4038": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b568bb1e17d04f3daca49c1b4205756e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9797530666c4d13894784774b2c9cb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b9c4197e248f4a3f94a7b7bba758ef80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bae6c12dc8274fd290756de5d1461f24",
      "max": 359,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_15d526c2547344e9960af67106128c74",
      "value": 359
     }
    },
    "bae6c12dc8274fd290756de5d1461f24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c106b02e4a8243cab51fa8b3583482a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ce1471fcc4f48b49f32a4df740fdc33",
      "max": 441551705,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dbe3b5b35c8a4f0eacfa780641406d66",
      "value": 441551705
     }
    },
    "cd831b6b92234a76b30e112127d8d4dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ab49c6c16dd34fe0b0e6c977f0cbf10c",
       "IPY_MODEL_f9310230e50b459db761b8920d2ccfa7",
       "IPY_MODEL_294ed8517bbc41469739a2a953f01a99"
      ],
      "layout": "IPY_MODEL_b29de32cc0e54d32a15dcb2b937cf5d6"
     }
    },
    "d1064fc7ad6641a792cd03899f0e1b25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d5acb9e6dc804dcc937c7243600588cb",
       "IPY_MODEL_339a7edf9f964f68ad470a5dd8c9c87f",
       "IPY_MODEL_3b00b8b52a844a41ace99f163a4a88d4"
      ],
      "layout": "IPY_MODEL_174b8dc9c91d4be2a14ab9855f1b94f2"
     }
    },
    "d5acb9e6dc804dcc937c7243600588cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60b800f860674dd89d7c0fe9e5abac52",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b4a2f52f379240fc9c278ddf18db4038",
      "value": "100%"
     }
    },
    "d5fc29e255f44e948e7e7963fc0d7049": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7d86bede479489db6d90a5e7c954413": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a0650735182482b9c85a8b0635eb0b3",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8b9d274a98049e79b1f1a3ca7e6eb0c",
      "value": 4
     }
    },
    "dab0b987427a49278b7e3878c39965f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dbe3b5b35c8a4f0eacfa780641406d66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc88c00f01c34b65a0e5f1e78ad436a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8aa7c7824a8422086d2ea59696cf465",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_138080dd5bd241cfad5a7b793ba35e9c",
      "value": "100%"
     }
    },
    "ddd05d64f67946549441be1cc0c6de3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de194c8ba40d459197f1374a24dfaa3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3bb504ae1904a0699a5b1aa62d4879a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f45a9ecff5344c5a94a07f98e880fb0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5a8a95652ec4819bcadba10f60ad514": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f6b220e1bbfe48d78a93551151b6b1f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8b9d274a98049e79b1f1a3ca7e6eb0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f9310230e50b459db761b8920d2ccfa7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17b4ebdfd5b44cfba12db5d23d0f7116",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b9797530666c4d13894784774b2c9cb3",
      "value": 1
     }
    },
    "f9c1b9b34f0a40139f4f5e2c3bff77b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff631c22406c4ebd92d0b63f2b94ce84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
